{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mroq/opt/miniconda3/envs/exp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from core.tiny import GPTSimple\n",
    "from core.config import GPT1Config\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration huggingface-course--codeparrot-ds-train-7e9fc5dfe436a81a\n",
      "Using custom data configuration huggingface-course--codeparrot-ds-valid-65557c3279496c87\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mroq/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:106: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "\n",
      "  | Name          | Type                    | Params\n",
      "----------------------------------------------------------\n",
      "0 | embedding     | Embedding               | 38.6 M\n",
      "1 | encoder_layer | TransformerEncoderLayer | 5.5 M \n",
      "2 | encoder       | TransformerEncoder      | 66.2 M\n",
      "3 | ff            | Linear                  | 590 K \n",
      "4 | output        | Linear                  | 38.6 M\n",
      "----------------------------------------------------------\n",
      "149 M     Trainable params\n",
      "0         Non-trainable params\n",
      "149 M     Total params\n",
      "598.070   Total estimated model params size (MB)\n",
      "/Users/mroq/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 0it [00:00, ?it/s]{'repo_name': ['kmike/scikit-learn', 'mne-tools/mne-tools.github.io', 'bijanfallah/OI_CCLM', 'hsu/chrono', 'lancezlin/ml_template_py', 'wavelets/zipline', 'ChanChiChoi/scikit-learn', 'rcolasanti/CompaniesHouseScraper', 'mclaughlin6464/pasta', 'nicholaschris/landsatpy', 'Monika319/EWEF-1', 'nddsg/TreeDecomps', 'apache/spark', 'Charence/stk-code', 'belltailjp/scikit-learn', 'tosolveit/scikit-learn', 'caseyclements/bokeh', 'harlowja/networkx', 'jblackburne/scikit-learn', 'abonil91/ncanda-data-integration', 'jorik041/scikit-learn', 'jingxiang-li/kaggle-yelp', 'jreback/pandas', 'modelkayak/python_signal_examples', 'FrankTsui/robust_rescaled_svm', 'JeanKossaifi/scikit-learn', 'srinathv/vispy', 'eljost/pysisyphus', 'AtsushiSakai/PythonRobotics', 'kaczla/PJN', 'danforthcenter/plantcv', 'linearregression/mpld3'], 'path': ['sklearn/utils/__init__.py', '0.20/_downloads/76822bb92a8465181ec2a7ee96ca8cf4/plot_decoding_csp_timefreq.py', 'src/RMSE_MAPS_INGO.py', 'src/demos/trackVehicle/validationPlots_test_M113.py', 'lib/python2.7/site-packages/sklearn/metrics/tests/test_score_objects.py', 'zipline/examples/dual_ema_talib.py', 'examples/model_selection/plot_roc.py', 'DVLACompanyNmeMatchCoHoAPIFindMissing.py', 'pasta/ising.py', 'stuff.py', 'Cw2Rezonans/Karolina/Oscyloskop/OscyloskopZ5W2.py', 'xplodnTree/tdec/b2CliqueTreeRules.py', 'python/pyspark/sql/functions.py', 'tools/batch.py', 'sklearn/decomposition/base.py', 'sklearn/ensemble/tests/test_partial_dependence.py', 'bokeh/compat/mplexporter/exporter.py', 'examples/drawing/knuth_miles.py', 'sklearn/neural_network/rbm.py', 'scripts/redcap/scoring/ctq/__init__.py', 'sklearn/linear_model/randomized_l1.py', 'model/level3_model_rf.py', 'pandas/io/formats/html.py', 'energy_fft.py', 'common.py', 'sklearn/tree/tests/test_tree.py', 'vispy/visuals/isocurve.py', 'deprecated/tests/test_dynamics/test_dynamics.py', 'PathPlanning/Eta3SplinePath/eta3_spline_path.py', 'src/Przecinki/scikit.py', 'plantcv/plantcv/photosynthesis/analyze_fvfm.py', 'mpld3/__init__.py'], 'copies': ['3', '1', '1', '5', '15', '2', '146', '1', '1', '1', '1', '1', '14', '16', '313', '365', '32', '50', '46', '1', '95', '1', '2', '1', '1', '48', '18', '1', '1', '1', '2', '20'], 'size': ['10094', '6457', '2007', '4229', '17443', '3230', '3697', '5174', '5474', '1864', '1312', '3569', '161861', '3189', '5647', '6996', '12403', '2994', '12291', '3092', '23365', '5669', '23192', '2685', '1636', '47506', '7809', '2531', '13649', '1048', '5529', '1109'], 'content': ['\"\"\"\\nThe :mod:`sklearn.utils` module includes various utilites.\\n\"\"\"\\n\\nfrom collections import Sequence\\n\\nimport numpy as np\\nfrom scipy.sparse import issparse\\nimport warnings\\n\\nfrom .murmurhash import murmurhash3_32\\nfrom .validation import (as_float_array, check_arrays, safe_asarray,\\n                         assert_all_finite, array2d, atleast2d_or_csc,\\n                         atleast2d_or_csr, warn_if_not_float,\\n                         check_random_state)\\nfrom .class_weight import compute_class_weight\\n\\n__all__ = [\"murmurhash3_32\", \"as_float_array\", \"check_arrays\", \"safe_asarray\",\\n           \"assert_all_finite\", \"array2d\", \"atleast2d_or_csc\",\\n           \"atleast2d_or_csr\", \"warn_if_not_float\", \"check_random_state\",\\n           \"compute_class_weight\"]\\n\\n# Make sure that DeprecationWarning get printed\\nwarnings.simplefilter(\"always\", DeprecationWarning)\\n\\n\\nclass deprecated(object):\\n    \"\"\"Decorator to mark a function or class as deprecated.\\n\\n    Issue a warning when the function is called/the class is instantiated and\\n    adds a warning to the docstring.\\n\\n    The optional extra argument will be appended to the deprecation message\\n    and the docstring. Note: to use this with the default value for extra, put\\n    in an empty of parentheses:\\n\\n    >>> from sklearn.utils import deprecated\\n    >>> deprecated() # doctest: +ELLIPSIS\\n    <sklearn.utils.deprecated object at ...>\\n\\n    >>> @deprecated()\\n    ... def some_function(): pass\\n    \"\"\"\\n\\n    # Adapted from http://wiki.python.org/moin/PythonDecoratorLibrary,\\n    # but with many changes.\\n\\n    def __init__(self, extra=\\'\\'):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        extra: string\\n          to be added to the deprecation messages\\n\\n        \"\"\"\\n        self.extra = extra\\n\\n    def __call__(self, obj):\\n        if isinstance(obj, type):\\n            return self._decorate_class(obj)\\n        else:\\n            return self._decorate_fun(obj)\\n\\n    def _decorate_class(self, cls):\\n        msg = \"Class %s is deprecated\" % cls.__name__\\n        if self.extra:\\n            msg += \"; %s\" % self.extra\\n\\n        # FIXME: we should probably reset __new__ for full generality\\n        init = cls.__init__\\n\\n        def wrapped(*args, **kwargs):\\n            warnings.warn(msg, category=DeprecationWarning)\\n            return init(*args, **kwargs)\\n        cls.__init__ = wrapped\\n\\n        wrapped.__name__ = \\'__init__\\'\\n        wrapped.__doc__ = self._update_doc(init.__doc__)\\n        wrapped.deprecated_original = init\\n\\n        return cls\\n\\n    def _decorate_fun(self, fun):\\n        \"\"\"Decorate function fun\"\"\"\\n\\n        msg = \"Function %s is deprecated\" % fun.__name__\\n        if self.extra:\\n            msg += \"; %s\" % self.extra\\n\\n        def wrapped(*args, **kwargs):\\n            warnings.warn(msg, category=DeprecationWarning)\\n            return fun(*args, **kwargs)\\n\\n        wrapped.__name__ = fun.__name__\\n        wrapped.__dict__ = fun.__dict__\\n        wrapped.__doc__ = self._update_doc(fun.__doc__)\\n\\n        return wrapped\\n\\n    def _update_doc(self, olddoc):\\n        newdoc = \"DEPRECATED\"\\n        if self.extra:\\n            newdoc = \"%s: %s\" % (newdoc, self.extra)\\n        if olddoc:\\n            newdoc = \"%s\\\\n\\\\n%s\" % (newdoc, olddoc)\\n        return newdoc\\n\\n\\ndef safe_mask(X, mask):\\n    \"\"\"Return a mask which is safe to use on X.\\n\\n    Parameters\\n    ----------\\n    X : {array-like, sparse matrix}\\n        Data on which to apply mask.\\n\\n    mask: array\\n        Mask to be used on X.\\n\\n    Returns\\n    -------\\n        mask\\n    \"\"\"\\n    mask = np.asanyarray(mask)\\n    if np.issubdtype(mask.dtype, np.int):\\n        return mask\\n\\n    if hasattr(X, \"toarray\"):\\n        ind = np.arange(mask.shape[0])\\n        mask = ind[mask]\\n    return mask\\n\\n\\ndef resample(*arrays, **options):\\n    \"\"\"Resample arrays or sparse matrices in a consistent way\\n\\n    The default strategy implements one step of the bootstrapping\\n    procedure.\\n\\n    Parameters\\n    ----------\\n    `*arrays` : sequence of arrays or scipy.sparse matrices with same shape[0]\\n\\n    replace : boolean, True by default\\n        Implements resampling with replacement. If False, this will implement\\n        (sliced) random permutations.\\n\\n    n_samples : int, None by default\\n        Number of samples to generate. If left to None this is\\n        automatically set to the first dimension of the arrays.\\n\\n    random_state : int or RandomState instance\\n        Control the shuffling for reproducible behavior.\\n\\n    Returns\\n    -------\\n    Sequence of resampled views of the collections. The original arrays are\\n    not impacted.\\n\\n    Examples\\n    --------\\n    It is possible to mix sparse and dense arrays in the same run::\\n\\n      >>> X = [[1., 0.], [2., 1.], [0., 0.]]\\n      >>> y = np.array([0, 1, 2])\\n\\n      >>> from scipy.sparse import coo_matrix\\n      >>> X_sparse = coo_matrix(X)\\n\\n      >>> from sklearn.utils import resample\\n      >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\\n      >>> X\\n      array([[ 1.,  0.],\\n             [ 2.,  1.],\\n             [ 1.,  0.]])\\n\\n      >>> X_sparse                   # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\\n      <3x2 sparse matrix of type \\'<... \\'numpy.float64\\'>\\'\\n          with 4 stored elements in Compressed Sparse Row format>\\n\\n      >>> X_sparse.toarray()\\n      array([[ 1.,  0.],\\n             [ 2.,  1.],\\n             [ 1.,  0.]])\\n\\n      >>> y\\n      array([0, 1, 0])\\n\\n      >>> resample(y, n_samples=2, random_state=0)\\n      array([0, 1])\\n\\n\\n    See also\\n    --------\\n    :class:`sklearn.cross_validation.Bootstrap`\\n    :func:`sklearn.utils.shuffle`\\n    \"\"\"\\n    random_state = check_random_state(options.pop(\\'random_state\\', None))\\n    replace = options.pop(\\'replace\\', True)\\n    max_n_samples = options.pop(\\'n_samples\\', None)\\n    if options:\\n        raise ValueError(\"Unexpected kw arguments: %r\" % options.keys())\\n\\n    if len(arrays) == 0:\\n        return None\\n\\n    first = arrays[0]\\n    n_samples = first.shape[0] if hasattr(first, \\'shape\\') else len(first)\\n\\n    if max_n_samples is None:\\n        max_n_samples = n_samples\\n\\n    if max_n_samples > n_samples:\\n        raise ValueError(\"Cannot sample %d out of arrays with dim %d\" % (\\n            max_n_samples, n_samples))\\n\\n    arrays = check_arrays(*arrays, sparse_format=\\'csr\\')\\n\\n    if replace:\\n        indices = random_state.randint(0, n_samples, size=(max_n_samples,))\\n    else:\\n        indices = np.arange(n_samples)\\n        random_state.shuffle(indices)\\n        indices = indices[:max_n_samples]\\n\\n    resampled_arrays = []\\n\\n    for array in arrays:\\n        array = array[indices]\\n        resampled_arrays.append(array)\\n\\n    if len(resampled_arrays) == 1:\\n        # syntactic sugar for the unit argument case\\n        return resampled_arrays[0]\\n    else:\\n        return resampled_arrays\\n\\n\\ndef shuffle(*arrays, **options):\\n    \"\"\"Shuffle arrays or sparse matrices in a consistent way\\n\\n    This is a convenience alias to ``resample(*arrays, replace=False)`` to do\\n    random permutations of the collections.\\n\\n    Parameters\\n    ----------\\n    `*arrays` : sequence of arrays or scipy.sparse matrices with same shape[0]\\n\\n    random_state : int or RandomState instance\\n        Control the shuffling for reproducible behavior.\\n\\n    n_samples : int, None by default\\n        Number of samples to generate. If left to None this is\\n        automatically set to the first dimension of the arrays.\\n\\n    Returns\\n    -------\\n    Sequence of shuffled views of the collections. The original arrays are\\n    not impacted.\\n\\n    Examples\\n    --------\\n    It is possible to mix sparse and dense arrays in the same run::\\n\\n      >>> X = [[1., 0.], [2., 1.], [0., 0.]]\\n      >>> y = np.array([0, 1, 2])\\n\\n      >>> from scipy.sparse import coo_matrix\\n      >>> X_sparse = coo_matrix(X)\\n\\n      >>> from sklearn.utils import shuffle\\n      >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\\n      >>> X\\n      array([[ 0.,  0.],\\n             [ 2.,  1.],\\n             [ 1.,  0.]])\\n\\n      >>> X_sparse                   # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\\n      <3x2 sparse matrix of type \\'<... \\'numpy.float64\\'>\\'\\n          with 3 stored elements in Compressed Sparse Row format>\\n\\n      >>> X_sparse.toarray()\\n      array([[ 0.,  0.],\\n             [ 2.,  1.],\\n             [ 1.,  0.]])\\n\\n      >>> y\\n      array([2, 1, 0])\\n\\n      >>> shuffle(y, n_samples=2, random_state=0)\\n      array([0, 1])\\n\\n    See also\\n    --------\\n    :func:`sklearn.utils.resample`\\n    \"\"\"\\n    options[\\'replace\\'] = False\\n    return resample(*arrays, **options)\\n\\n\\ndef safe_sqr(X, copy=True):\\n    \"\"\"Element wise squaring of array-likes and sparse matrices.\\n\\n    Parameters\\n    ----------\\n    X : array like, matrix, sparse matrix\\n\\n    Returns\\n    -------\\n    X ** 2 : element wise square\\n    \"\"\"\\n    X = safe_asarray(X)\\n    if issparse(X):\\n        if copy:\\n            X = X.copy()\\n        X.data **= 2\\n    else:\\n        if copy:\\n            X = X ** 2\\n        else:\\n            X **= 2\\n    return X\\n\\n\\ndef gen_even_slices(n, n_packs):\\n    \"\"\"Generator to create n_packs slices going up to n.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.utils import gen_even_slices\\n    >>> list(gen_even_slices(10, 1))\\n    [slice(0, 10, None)]\\n    >>> list(gen_even_slices(10, 10))                     #doctest: +ELLIPSIS\\n    [slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)]\\n    >>> list(gen_even_slices(10, 5))                      #doctest: +ELLIPSIS\\n    [slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)]\\n    >>> list(gen_even_slices(10, 3))\\n    [slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)]\\n    \"\"\"\\n    start = 0\\n    for pack_num in range(n_packs):\\n        this_n = n // n_packs\\n        if pack_num < n % n_packs:\\n            this_n += 1\\n        if this_n > 0:\\n            end = start + this_n\\n            yield slice(start, end, None)\\n            start = end\\n\\n\\ndef tosequence(x):\\n    \"\"\"Cast iterable x to a Sequence, avoiding a copy if possible.\"\"\"\\n    if isinstance(x, np.ndarray):\\n        return np.asarray(x)\\n    elif isinstance(x, Sequence):\\n        return x\\n    else:\\n        return list(x)\\n\\n\\nclass ConvergenceWarning(Warning):\\n    \"Custom warning to capture convergence problems\"\\n', '\"\"\"\\n============================================================================\\nDecoding in time-frequency space data using the Common Spatial Pattern (CSP)\\n============================================================================\\n\\nThe time-frequency decomposition is estimated by iterating over raw data that\\nhas been band-passed at different frequencies. This is used to compute a\\ncovariance matrix over each epoch or a rolling time-window and extract the CSP\\nfiltered signals. A linear discriminant classifier is then applied to these\\nsignals.\\n\"\"\"\\n# Authors: Laura Gwilliams <laura.gwilliams@nyu.edu>\\n#          Jean-Remi King <jeanremi.king@gmail.com>\\n#          Alex Barachant <alexandre.barachant@gmail.com>\\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\\n#\\n# License: BSD (3-clause)\\n\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nfrom mne import Epochs, create_info, events_from_annotations\\nfrom mne.io import concatenate_raws, read_raw_edf\\nfrom mne.datasets import eegbci\\nfrom mne.decoding import CSP\\nfrom mne.time_frequency import AverageTFR\\n\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n###############################################################################\\n# Set parameters and read data\\nevent_id = dict(hands=2, feet=3)  # motor imagery: hands vs feet\\nsubject = 1\\nruns = [6, 10, 14]\\nraw_fnames = eegbci.load_data(subject, runs)\\nraw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\\n\\n# Extract information from the raw file\\nsfreq = raw.info[\\'sfreq\\']\\nevents, _ = events_from_annotations(raw, event_id=dict(T1=2, T2=3))\\nraw.pick_types(meg=False, eeg=True, stim=False, eog=False, exclude=\\'bads\\')\\n\\n# Assemble the classifier using scikit-learn pipeline\\nclf = make_pipeline(CSP(n_components=4, reg=None, log=True, norm_trace=False),\\n                    LinearDiscriminantAnalysis())\\nn_splits = 5  # how many folds to use for cross-validation\\ncv = StratifiedKFold(n_splits=n_splits, shuffle=True)\\n\\n# Classification & Time-frequency parameters\\ntmin, tmax = -.200, 2.000\\nn_cycles = 10.  # how many complete cycles: used to define window size\\nmin_freq = 5.\\nmax_freq = 25.\\nn_freqs = 8  # how many frequency bins to use\\n\\n# Assemble list of frequency range tuples\\nfreqs = np.linspace(min_freq, max_freq, n_freqs)  # assemble frequencies\\nfreq_ranges = list(zip(freqs[:-1], freqs[1:]))  # make freqs list of tuples\\n\\n# Infer window spacing from the max freq and number of cycles to avoid gaps\\nwindow_spacing = (n_cycles / np.max(freqs) / 2.)\\ncentered_w_times = np.arange(tmin, tmax, window_spacing)[1:]\\nn_windows = len(centered_w_times)\\n\\n# Instantiate label encoder\\nle = LabelEncoder()\\n\\n###############################################################################\\n# Loop through frequencies, apply classifier and save scores\\n\\n# init scores\\nfreq_scores = np.zeros((n_freqs - 1,))\\n\\n# Loop through each frequency range of interest\\nfor freq, (fmin, fmax) in enumerate(freq_ranges):\\n\\n    # Infer window size based on the frequency being used\\n    w_size = n_cycles / ((fmax + fmin) / 2.)  # in seconds\\n\\n    # Apply band-pass filter to isolate the specified frequencies\\n    raw_filter = raw.copy().filter(fmin, fmax, n_jobs=1, fir_design=\\'firwin\\',\\n                                   skip_by_annotation=\\'edge\\')\\n\\n    # Extract epochs from filtered data, padded by window size\\n    epochs = Epochs(raw_filter, events, event_id, tmin - w_size, tmax + w_size,\\n                    proj=False, baseline=None, preload=True)\\n    epochs.drop_bad()\\n    y = le.fit_transform(epochs.events[:, 2])\\n\\n    X = epochs.get_data()\\n\\n    # Save mean scores over folds for each frequency and time window\\n    freq_scores[freq] = np.mean(cross_val_score(estimator=clf, X=X, y=y,\\n                                                scoring=\\'roc_auc\\', cv=cv,\\n                                                n_jobs=1), axis=0)\\n\\n###############################################################################\\n# Plot frequency results\\n\\nplt.bar(freqs[:-1], freq_scores, width=np.diff(freqs)[0],\\n        align=\\'edge\\', edgecolor=\\'black\\')\\nplt.xticks(freqs)\\nplt.ylim([0, 1])\\nplt.axhline(len(epochs[\\'feet\\']) / len(epochs), color=\\'k\\', linestyle=\\'--\\',\\n            label=\\'chance level\\')\\nplt.legend()\\nplt.xlabel(\\'Frequency (Hz)\\')\\nplt.ylabel(\\'Decoding Scores\\')\\nplt.title(\\'Frequency Decoding Scores\\')\\n\\n###############################################################################\\n# Loop through frequencies and time, apply classifier and save scores\\n\\n# init scores\\ntf_scores = np.zeros((n_freqs - 1, n_windows))\\n\\n# Loop through each frequency range of interest\\nfor freq, (fmin, fmax) in enumerate(freq_ranges):\\n\\n    # Infer window size based on the frequency being used\\n    w_size = n_cycles / ((fmax + fmin) / 2.)  # in seconds\\n\\n    # Apply band-pass filter to isolate the specified frequencies\\n    raw_filter = raw.copy().filter(fmin, fmax, n_jobs=1, fir_design=\\'firwin\\',\\n                                   skip_by_annotation=\\'edge\\')\\n\\n    # Extract epochs from filtered data, padded by window size\\n    epochs = Epochs(raw_filter, events, event_id, tmin - w_size, tmax + w_size,\\n                    proj=False, baseline=None, preload=True)\\n    epochs.drop_bad()\\n    y = le.fit_transform(epochs.events[:, 2])\\n\\n    # Roll covariance, csp and lda over time\\n    for t, w_time in enumerate(centered_w_times):\\n\\n        # Center the min and max of the window\\n        w_tmin = w_time - w_size / 2.\\n        w_tmax = w_time + w_size / 2.\\n\\n        # Crop data into time-window of interest\\n        X = epochs.copy().crop(w_tmin, w_tmax).get_data()\\n\\n        # Save mean scores over folds for each frequency and time window\\n        tf_scores[freq, t] = np.mean(cross_val_score(estimator=clf, X=X, y=y,\\n                                                     scoring=\\'roc_auc\\', cv=cv,\\n                                                     n_jobs=1), axis=0)\\n\\n###############################################################################\\n# Plot time-frequency results\\n\\n# Set up time frequency object\\nav_tfr = AverageTFR(create_info([\\'freq\\'], sfreq), tf_scores[np.newaxis, :],\\n                    centered_w_times, freqs[1:], 1)\\n\\nchance = np.mean(y)  # set chance level to white in the plot\\nav_tfr.plot([0], vmin=chance, title=\"Time-Frequency Decoding Scores\",\\n            cmap=plt.cm.Reds)\\n', '# Program to show the maps of RMSE averaged over time\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import mean_squared_error\\nimport os\\nfrom netCDF4 import Dataset as NetCDFFile\\nimport numpy as np\\nfrom CCLM_OUTS import Plot_CCLM\\n# option == 1 ->  shift 4 with default cclm domain and nboundlines = 3\\n# option == 2 ->  shift 4 with smaller cclm domain and nboundlines = 3\\n# option == 3 ->  shift 4 with smaller cclm domain and nboundlines = 6\\n# option == 4 ->  shift 4 with corrected smaller cclm domain and nboundlines = 3\\n# option == 5 ->  shift 4 with corrected smaller cclm domain and nboundlines = 4\\n# option == 6 ->  shift 4 with corrected smaller cclm domain and nboundlines = 6\\n# option == 7 ->  shift 4 with corrected smaller cclm domain and nboundlines = 9\\n# option == 8 ->  shift 4 with corrected bigger cclm domain and nboundlines = 3\\nfrom CCLM_OUTS import Plot_CCLM\\n#def f(x):\\n#   if x==-9999:\\n#      return float(\\'NaN\\')\\n#   else:\\n#      return x\\ndef read_data_from_mistral(dir=\\'/work/bb1029/b324045/work1/work/member/post/\\',name=\\'member_T_2M_ts_seasmean.nc\\',var=\\'T_2M\\'):\\n    # type: (object, object, object) -> object\\n    #a function to read the data from mistral work\\n\\n    \"\"\"\\n\\n    :rtype: object\\n    \"\"\"\\n    #CMD = \\'scp $mistral:\\' + dir + name + \\' ./\\'\\n    CMD = \\'wget users.met.fu-berlin.de/~BijanFallah/\\' + dir + name\\n    os.system(CMD)\\n    nc = NetCDFFile(name)\\n#    for name2, variable in nc.variables.items():\\n#        for attrname in variable.ncattrs():\\n#                    print(name2, variable, \\'-----------------\\',attrname)\\n#                    #print(\"{} -- {}\".format(attrname, getattr(variable, attrname)))\\n    os.remove(name)\\n    lats = nc.variables[\\'lat\\'][:]\\n    lons = nc.variables[\\'lon\\'][:]\\n    t = nc.variables[var][:].squeeze()\\n    rlats = nc.variables[\\'rlat\\'][:]  # extract/copy the data\\n    rlons = nc.variables[\\'rlon\\'][:]\\n    #f2 = np.vectorize(f)\\n    #t= f2(t)\\n    #t=t.data\\n    t=t.squeeze()\\n    #print()\\n    nc.close()\\n\\n    return(t, lats, lons, rlats, rlons)\\n\\n', '# -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Wed May 06 11:00:53 2015\\n\\n@author: newJustin\\n\"\"\"\\n\\nimport ChronoTrack_pandas as CT\\nimport pylab as py\\n      \\nif __name__ == \\'__main__\\':\\n    \\n    # logger\\n    import logging as lg\\n    \\n    lg.basicConfig(fileName = \\'logFile.log\\', level=lg.WARN, format=\\'%(message)s\\')\\n    # default font size\\n\\n    import matplotlib\\n    font = {\\'size\\' : 14}\\n    matplotlib.rc(\\'font\\', **font)       \\n    \\n    #  **********************************************************************    \\n    #  ===============   USER INPUT   =======================================\\n    # data dir, end w/ \\'/\\'\\n    # data_dir = \\'D:/Chrono_github_Build/bin/outdata_M113/\\'\\n    data_dir = \\'E:/Chrono_github_Build/bin/outdata_M113/\\'    \\n    \\n    \\'\\'\\'    \\n    # list of data files to plot\\n    chassis = \\'M113_chassis.csv\\'\\n    gearSubsys = \\'M113_Side0_gear.csv\\'\\n    idlerSubsys = \\'M113_Side0_idler.csv\\'\\n    # ptrainSubsys = \\'test_driveChain_ptrain.csv\\'\\n    shoe0 = \\'M113_Side0_shoe0.csv\\'\\n    \\'\\'\\'\\n    chassis = \\'M113_400_200__chassis.csv\\'\\n    gearSubsys = \\'M113_400_200__Side0_gear.csv\\'\\n    idlerSubsys = \\'M113_400_200__Side0_idler.csv\\'\\n    # ptrainSubsys = \\'test_driveChain_ptrain.csv\\'\\n    shoe0 = \\'M113_400_200__Side0_shoe0.csv\\'    \\n\\n    data_files = [data_dir + chassis, data_dir + gearSubsys, data_dir + idlerSubsys, data_dir + shoe0]\\n    handle_list = [\\'chassis\\',\\'gear\\',\\'idler\\',\\'shoe0\\']\\n    # handle_list = [\\'Gear\\',\\'idler\\',\\'ptrain\\',\\'shoe0\\',\\'gearCV\\',\\'idlerCV\\',\\'rollerCV\\',\\'gearContact\\',\\'shoeGearContact\\']\\n\\n    \\n    \\n    \\'\\'\\'\\n    gearCV = \\'test_driveChain_GearCV.csv\\'\\n    idlerCV = \\'test_driveChain_idler0CV.csv\\'\\n    rollerCV = \\'test_driveChain_roller0CV.csv\\'\\n    gearContact = \\'test_driveChain_gearContact.csv\\'\\n    shoeGearContact = \\'test_driveChain_shoe0GearContact.csv\\'\\n    \\'\\'\\'\\n    \\n    # data_files = [data_dir + gearSubsys, data_dir + idlerSubsys, data_dir + ptrainSubsys, data_dir + shoe0, data_dir + gearCV, data_dir + idlerCV, data_dir + rollerCV, data_dir + gearContact, data_dir+shoeGearContact]\\n    # handle_list = [\\'Gear\\',\\'idler\\',\\'ptrain\\',\\'shoe0\\',\\'gearCV\\',\\'idlerCV\\',\\'rollerCV\\',\\'gearContact\\',\\'shoeGearContact\\']\\n\\n    \\n    # list of data files for gear/pin comparison plots    \\n    #  Primitive gear geometry\\n    \\'\\'\\'\\n    gear = \\'driveChain_P_gear.csv\\'\\n    gearContact = \\'driveChain_P_gearContact.csv\\'\\n    shoe = \\'driveChain_P_shoe0.csv\\'\\n    shoeContact = \\'driveChain_P_shoe0GearContact.csv\\'\\n    ptrain = \\'driveChain_P_ptrain.csv\\'    \\n    \\n    \\n    #  Collision Callback gear geometry     \\n    gear = \\'driveChain_CC_gear.csv\\'\\n    gearContact = \\'driveChain_CC_gearContact.csv\\'\\n    shoe = \\'driveChain_CC_shoe0.csv\\'\\n    shoeContact = \\'driveChain_CC_shoe0GearContact.csv\\'\\n    ptrain = \\'driveChain_CC_ptrain.csv\\'    \\n    \\n    \\n    data_files = [data_dir+gear, data_dir+gearContact, data_dir+shoe, data_dir+shoeContact, data_dir+ptrain]\\n   \\n    handle_list = [\\'Gear\\',\\'gearContact\\',\\'shoe0\\',\\'shoeGearContact\\',\\'ptrain\\']\\n    \\'\\'\\'\\n \\n \\n    # construct the panda class for the DriveChain, file list and list of legend\\n    M113_Chain0 = CT.ChronoTrack_pandas(data_files, handle_list)\\n    \\n    # set the time limits. tmin = -1 will plot the entire time range\\n    tmin = 1.0\\n    tmax = 8.0\\n    \\n    \\n    #0) plot the chassis\\n    M113_Chain0.plot_chassis(tmin, tmax)    \\n    \\n    # 1) plot the gear body info\\n    M113_Chain0.plot_gear(tmin, tmax)\\n    \\n    \\n    # 2) plot idler body info, tensioner force\\n    M113_Chain0.plot_idler(tmin,tmax)\\n\\n    \\'\\'\\'\\n    # 3) plot powertrain info\\n    M113_Chain0.plot_ptrain()    \\n    \\'\\'\\'\\n    \\n    # 4) plot shoe 0 body info, and pin 0 force/torque\\n    M113_Chain0.plot_shoe(tmin,tmax)\\n    \\n    \\'\\'\\'\\n    # 5) plot gear Constraint Violations\\n    M113_Chain0.plot_gearCV(tmin,tmax)\\n    \\n    # 6) plot idler Constraint Violations\\n    M113_Chain0.plot_idlerCV(tmin,tmax)\\n    \\n    # 7) plot roller Constraint Violations\\n    M113_Chain0.plot_rollerCV(tmin,tmax)\\n    \\n    # 8) from the contact report callback function, gear contact info\\n    M113_Chain0.plot_gearContactInfo(tmin,tmax)\\n\\n    # 9)  from shoe-gear report callback function, contact info\\n    M113_Chain0.plot_shoeGearContactInfo(tmin,tmax)\\n    \\'\\'\\'\\n    \\n    # 10) track shoe trajectory: rel-X vs. rel-Y\\n    M113_Chain0.plot_trajectory(tmin,tmax)\\n\\n    py.show()', 'import pickle\\nimport tempfile\\nimport shutil\\nimport os\\nimport numbers\\n\\nimport numpy as np\\n\\nfrom sklearn.utils.testing import assert_almost_equal\\nfrom sklearn.utils.testing import assert_array_equal\\nfrom sklearn.utils.testing import assert_raises\\nfrom sklearn.utils.testing import assert_raises_regexp\\nfrom sklearn.utils.testing import assert_true\\nfrom sklearn.utils.testing import ignore_warnings\\nfrom sklearn.utils.testing import assert_not_equal\\nfrom sklearn.utils.testing import assert_warns_message\\n\\nfrom sklearn.base import BaseEstimator\\nfrom sklearn.metrics import (f1_score, r2_score, roc_auc_score, fbeta_score,\\n                             log_loss, precision_score, recall_score)\\nfrom sklearn.metrics.cluster import adjusted_rand_score\\nfrom sklearn.metrics.scorer import (check_scoring, _PredictScorer,\\n                                    _passthrough_scorer)\\nfrom sklearn.metrics import make_scorer, get_scorer, SCORERS\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.dummy import DummyRegressor\\nfrom sklearn.linear_model import Ridge, LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.datasets import make_multilabel_classification\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split, cross_val_score\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.multiclass import OneVsRestClassifier\\nfrom sklearn.externals import joblib\\n\\n\\nREGRESSION_SCORERS = [\\'r2\\', \\'neg_mean_absolute_error\\',\\n                      \\'neg_mean_squared_error\\', \\'neg_median_absolute_error\\',\\n                      \\'mean_absolute_error\\',\\n                      \\'mean_squared_error\\', \\'median_absolute_error\\']\\n\\nCLF_SCORERS = [\\'accuracy\\', \\'f1\\', \\'f1_weighted\\', \\'f1_macro\\', \\'f1_micro\\',\\n               \\'roc_auc\\', \\'average_precision\\', \\'precision\\',\\n               \\'precision_weighted\\', \\'precision_macro\\', \\'precision_micro\\',\\n               \\'recall\\', \\'recall_weighted\\', \\'recall_macro\\', \\'recall_micro\\',\\n               \\'neg_log_loss\\', \\'log_loss\\',\\n               \\'adjusted_rand_score\\'  # not really, but works\\n               ]\\n\\nMULTILABEL_ONLY_SCORERS = [\\'precision_samples\\', \\'recall_samples\\', \\'f1_samples\\']\\n\\n\\ndef _make_estimators(X_train, y_train, y_ml_train):\\n    # Make estimators that make sense to test various scoring methods\\n    sensible_regr = DummyRegressor(strategy=\\'median\\')\\n    sensible_regr.fit(X_train, y_train)\\n    sensible_clf = DecisionTreeClassifier(random_state=0)\\n    sensible_clf.fit(X_train, y_train)\\n    sensible_ml_clf = DecisionTreeClassifier(random_state=0)\\n    sensible_ml_clf.fit(X_train, y_ml_train)\\n    return dict(\\n        [(name, sensible_regr) for name in REGRESSION_SCORERS] +\\n        [(name, sensible_clf) for name in CLF_SCORERS] +\\n        [(name, sensible_ml_clf) for name in MULTILABEL_ONLY_SCORERS]\\n    )\\n\\n\\nX_mm, y_mm, y_ml_mm = None, None, None\\nESTIMATORS = None\\nTEMP_FOLDER = None\\n\\n\\ndef setup_module():\\n    # Create some memory mapped data\\n    global X_mm, y_mm, y_ml_mm, TEMP_FOLDER, ESTIMATORS\\n    TEMP_FOLDER = tempfile.mkdtemp(prefix=\\'sklearn_test_score_objects_\\')\\n    X, y = make_classification(n_samples=30, n_features=5, random_state=0)\\n    _, y_ml = make_multilabel_classification(n_samples=X.shape[0],\\n                                             random_state=0)\\n    filename = os.path.join(TEMP_FOLDER, \\'test_data.pkl\\')\\n    joblib.dump((X, y, y_ml), filename)\\n    X_mm, y_mm, y_ml_mm = joblib.load(filename, mmap_mode=\\'r\\')\\n    ESTIMATORS = _make_estimators(X_mm, y_mm, y_ml_mm)\\n\\n\\ndef teardown_module():\\n    global X_mm, y_mm, y_ml_mm, TEMP_FOLDER, ESTIMATORS\\n    # GC closes the mmap file descriptors\\n    X_mm, y_mm, y_ml_mm, ESTIMATORS = None, None, None, None\\n    shutil.rmtree(TEMP_FOLDER)\\n\\n\\nclass EstimatorWithoutFit(object):\\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\\n    pass\\n\\n\\nclass EstimatorWithFit(BaseEstimator):\\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\\n    def fit(self, X, y):\\n        return self\\n\\n\\nclass EstimatorWithFitAndScore(object):\\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\\n    def fit(self, X, y):\\n        return self\\n\\n    def score(self, X, y):\\n        return 1.0\\n\\n\\nclass EstimatorWithFitAndPredict(object):\\n    \"\"\"Dummy estimator to test check_scoring\"\"\"\\n    def fit(self, X, y):\\n        self.y = y\\n        return self\\n\\n    def predict(self, X):\\n        return self.y\\n\\n\\nclass DummyScorer(object):\\n    \"\"\"Dummy scorer that always returns 1.\"\"\"\\n    def __call__(self, est, X, y):\\n        return 1\\n\\n\\ndef test_all_scorers_repr():\\n    # Test that all scorers have a working repr\\n    for name, scorer in SCORERS.items():\\n        repr(scorer)\\n\\n\\ndef test_check_scoring():\\n    # Test all branches of check_scoring\\n    estimator = EstimatorWithoutFit()\\n    pattern = (r\"estimator should be an estimator implementing \\'fit\\' method,\"\\n               r\" .* was passed\")\\n    assert_raises_regexp(TypeError, pattern, check_scoring, estimator)\\n\\n    estimator = EstimatorWithFitAndScore()\\n    estimator.fit([[1]], [1])\\n    scorer = check_scoring(estimator)\\n    assert_true(scorer is _passthrough_scorer)\\n    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)\\n\\n    estimator = EstimatorWithFitAndPredict()\\n    estimator.fit([[1]], [1])\\n    pattern = (r\"If no scoring is specified, the estimator passed should have\"\\n               r\" a \\'score\\' method\\\\. The estimator .* does not\\\\.\")\\n    assert_raises_regexp(TypeError, pattern, check_scoring, estimator)\\n\\n    scorer = check_scoring(estimator, \"accuracy\")\\n    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)\\n\\n    estimator = EstimatorWithFit()\\n    scorer = check_scoring(estimator, \"accuracy\")\\n    assert_true(isinstance(scorer, _PredictScorer))\\n\\n    estimator = EstimatorWithFit()\\n    scorer = check_scoring(estimator, allow_none=True)\\n    assert_true(scorer is None)\\n\\n\\ndef test_check_scoring_gridsearchcv():\\n    # test that check_scoring works on GridSearchCV and pipeline.\\n    # slightly redundant non-regression test.\\n\\n    grid = GridSearchCV(LinearSVC(), param_grid={\\'C\\': [.1, 1]})\\n    scorer = check_scoring(grid, \"f1\")\\n    assert_true(isinstance(scorer, _PredictScorer))\\n\\n    pipe = make_pipeline(LinearSVC())\\n    scorer = check_scoring(pipe, \"f1\")\\n    assert_true(isinstance(scorer, _PredictScorer))\\n\\n    # check that cross_val_score definitely calls the scorer\\n    # and doesn\\'t make any assumptions about the estimator apart from having a\\n    # fit.\\n    scores = cross_val_score(EstimatorWithFit(), [[1], [2], [3]], [1, 0, 1],\\n                             scoring=DummyScorer())\\n    assert_array_equal(scores, 1)\\n\\n\\ndef test_make_scorer():\\n    # Sanity check on the make_scorer factory function.\\n    f = lambda *args: 0\\n    assert_raises(ValueError, make_scorer, f, needs_threshold=True,\\n                  needs_proba=True)\\n\\n\\ndef test_classification_scores():\\n    # Test classification scorers.\\n    X, y = make_blobs(random_state=0, centers=2)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf = LinearSVC(random_state=0)\\n    clf.fit(X_train, y_train)\\n\\n    for prefix, metric in [(\\'f1\\', f1_score), (\\'precision\\', precision_score),\\n                           (\\'recall\\', recall_score)]:\\n\\n        score1 = get_scorer(\\'%s_weighted\\' % prefix)(clf, X_test, y_test)\\n        score2 = metric(y_test, clf.predict(X_test), pos_label=None,\\n                        average=\\'weighted\\')\\n        assert_almost_equal(score1, score2)\\n\\n        score1 = get_scorer(\\'%s_macro\\' % prefix)(clf, X_test, y_test)\\n        score2 = metric(y_test, clf.predict(X_test), pos_label=None,\\n                        average=\\'macro\\')\\n        assert_almost_equal(score1, score2)\\n\\n        score1 = get_scorer(\\'%s_micro\\' % prefix)(clf, X_test, y_test)\\n        score2 = metric(y_test, clf.predict(X_test), pos_label=None,\\n                        average=\\'micro\\')\\n        assert_almost_equal(score1, score2)\\n\\n        score1 = get_scorer(\\'%s\\' % prefix)(clf, X_test, y_test)\\n        score2 = metric(y_test, clf.predict(X_test), pos_label=1)\\n        assert_almost_equal(score1, score2)\\n\\n    # test fbeta score that takes an argument\\n    scorer = make_scorer(fbeta_score, beta=2)\\n    score1 = scorer(clf, X_test, y_test)\\n    score2 = fbeta_score(y_test, clf.predict(X_test), beta=2)\\n    assert_almost_equal(score1, score2)\\n\\n    # test that custom scorer can be pickled\\n    unpickled_scorer = pickle.loads(pickle.dumps(scorer))\\n    score3 = unpickled_scorer(clf, X_test, y_test)\\n    assert_almost_equal(score1, score3)\\n\\n    # smoke test the repr:\\n    repr(fbeta_score)\\n\\n\\ndef test_regression_scorers():\\n    # Test regression scorers.\\n    diabetes = load_diabetes()\\n    X, y = diabetes.data, diabetes.target\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf = Ridge()\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'r2\\')(clf, X_test, y_test)\\n    score2 = r2_score(y_test, clf.predict(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n\\ndef test_thresholded_scorers():\\n    # Test scorers that take thresholds.\\n    X, y = make_blobs(random_state=0, centers=2)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf = LogisticRegression(random_state=0)\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\\n    score3 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\\n    assert_almost_equal(score1, score2)\\n    assert_almost_equal(score1, score3)\\n\\n    logscore = get_scorer(\\'neg_log_loss\\')(clf, X_test, y_test)\\n    logloss = log_loss(y_test, clf.predict_proba(X_test))\\n    assert_almost_equal(-logscore, logloss)\\n\\n    # same for an estimator without decision_function\\n    clf = DecisionTreeClassifier()\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\\n    assert_almost_equal(score1, score2)\\n\\n    # test with a regressor (no decision_function)\\n    reg = DecisionTreeRegressor()\\n    reg.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(reg, X_test, y_test)\\n    score2 = roc_auc_score(y_test, reg.predict(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n    # Test that an exception is raised on more than two classes\\n    X, y = make_blobs(random_state=0, centers=3)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf.fit(X_train, y_train)\\n    assert_raises(ValueError, get_scorer(\\'roc_auc\\'), clf, X_test, y_test)\\n\\n\\ndef test_thresholded_scorers_multilabel_indicator_data():\\n    # Test that the scorer work with multilabel-indicator format\\n    # for multilabel and multi-output multi-class classifier\\n    X, y = make_multilabel_classification(allow_unlabeled=False,\\n                                          random_state=0)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\n    # Multi-output multi-class predict_proba\\n    clf = DecisionTreeClassifier()\\n    clf.fit(X_train, y_train)\\n    y_proba = clf.predict_proba(X_test)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, np.vstack(p[:, -1] for p in y_proba).T)\\n    assert_almost_equal(score1, score2)\\n\\n    # Multi-output multi-class decision_function\\n    # TODO Is there any yet?\\n    clf = DecisionTreeClassifier()\\n    clf.fit(X_train, y_train)\\n    clf._predict_proba = clf.predict_proba\\n    clf.predict_proba = None\\n    clf.decision_function = lambda X: [p[:, 1] for p in clf._predict_proba(X)]\\n\\n    y_proba = clf.decision_function(X_test)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, np.vstack(p for p in y_proba).T)\\n    assert_almost_equal(score1, score2)\\n\\n    # Multilabel predict_proba\\n    clf = OneVsRestClassifier(DecisionTreeClassifier())\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, clf.predict_proba(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n    # Multilabel decision function\\n    clf = OneVsRestClassifier(LinearSVC(random_state=0))\\n    clf.fit(X_train, y_train)\\n    score1 = get_scorer(\\'roc_auc\\')(clf, X_test, y_test)\\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n\\ndef test_unsupervised_scorers():\\n    # Test clustering scorers against gold standard labeling.\\n    # We don\\'t have any real unsupervised Scorers yet.\\n    X, y = make_blobs(random_state=0, centers=2)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    km = KMeans(n_clusters=3)\\n    km.fit(X_train)\\n    score1 = get_scorer(\\'adjusted_rand_score\\')(km, X_test, y_test)\\n    score2 = adjusted_rand_score(y_test, km.predict(X_test))\\n    assert_almost_equal(score1, score2)\\n\\n\\n@ignore_warnings\\ndef test_raises_on_score_list():\\n    # Test that when a list of scores is returned, we raise proper errors.\\n    X, y = make_blobs(random_state=0)\\n    f1_scorer_no_average = make_scorer(f1_score, average=None)\\n    clf = DecisionTreeClassifier()\\n    assert_raises(ValueError, cross_val_score, clf, X, y,\\n                  scoring=f1_scorer_no_average)\\n    grid_search = GridSearchCV(clf, scoring=f1_scorer_no_average,\\n                               param_grid={\\'max_depth\\': [1, 2]})\\n    assert_raises(ValueError, grid_search.fit, X, y)\\n\\n\\n@ignore_warnings\\ndef test_scorer_sample_weight():\\n    # Test that scorers support sample_weight or raise sensible errors\\n\\n    # Unlike the metrics invariance test, in the scorer case it\\'s harder\\n    # to ensure that, on the classifier output, weighted and unweighted\\n    # scores really should be unequal.\\n    X, y = make_classification(random_state=0)\\n    _, y_ml = make_multilabel_classification(n_samples=X.shape[0],\\n                                             random_state=0)\\n    split = train_test_split(X, y, y_ml, random_state=0)\\n    X_train, X_test, y_train, y_test, y_ml_train, y_ml_test = split\\n\\n    sample_weight = np.ones_like(y_test)\\n    sample_weight[:10] = 0\\n\\n    # get sensible estimators for each metric\\n    estimator = _make_estimators(X_train, y_train, y_ml_train)\\n\\n    for name, scorer in SCORERS.items():\\n        if name in MULTILABEL_ONLY_SCORERS:\\n            target = y_ml_test\\n        else:\\n            target = y_test\\n        try:\\n            weighted = scorer(estimator[name], X_test, target,\\n                              sample_weight=sample_weight)\\n            ignored = scorer(estimator[name], X_test[10:], target[10:])\\n            unweighted = scorer(estimator[name], X_test, target)\\n            assert_not_equal(weighted, unweighted,\\n                             msg=\"scorer {0} behaves identically when \"\\n                             \"called with sample weights: {1} vs \"\\n                             \"{2}\".format(name, weighted, unweighted))\\n            assert_almost_equal(weighted, ignored,\\n                                err_msg=\"scorer {0} behaves differently when \"\\n                                \"ignoring samples and setting sample_weight to\"\\n                                \" 0: {1} vs {2}\".format(name, weighted,\\n                                                        ignored))\\n\\n        except TypeError as e:\\n            assert_true(\"sample_weight\" in str(e),\\n                        \"scorer {0} raises unhelpful exception when called \"\\n                        \"with sample weights: {1}\".format(name, str(e)))\\n\\n\\n@ignore_warnings  # UndefinedMetricWarning for P / R scores\\ndef check_scorer_memmap(scorer_name):\\n    scorer, estimator = SCORERS[scorer_name], ESTIMATORS[scorer_name]\\n    if scorer_name in MULTILABEL_ONLY_SCORERS:\\n        score = scorer(estimator, X_mm, y_ml_mm)\\n    else:\\n        score = scorer(estimator, X_mm, y_mm)\\n    assert isinstance(score, numbers.Number), scorer_name\\n\\n\\ndef test_scorer_memmap_input():\\n    # Non-regression test for #6147: some score functions would\\n    # return singleton memmap when computed on memmap data instead of scalar\\n    # float values.\\n    for name in SCORERS.keys():\\n        yield check_scorer_memmap, name\\n\\n\\ndef test_deprecated_names():\\n    X, y = make_blobs(random_state=0, centers=2)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n    clf = LogisticRegression(random_state=0)\\n    clf.fit(X_train, y_train)\\n\\n    for name in (\\'mean_absolute_error\\', \\'mean_squared_error\\',\\n                 \\'median_absolute_error\\', \\'log_loss\\'):\\n        warning_msg = \"Scoring method %s was renamed to\" % name\\n        for scorer in (get_scorer(name), SCORERS[name]):\\n            assert_warns_message(DeprecationWarning,\\n                                 warning_msg,\\n                                 scorer, clf, X, y)\\n\\n        assert_warns_message(DeprecationWarning,\\n                             warning_msg,\\n                             cross_val_score, clf, X, y, scoring=name)\\n\\n\\ndef test_scoring_is_not_metric():\\n    assert_raises_regexp(ValueError, \\'make_scorer\\', check_scoring,\\n                         LogisticRegression(), f1_score)\\n    assert_raises_regexp(ValueError, \\'make_scorer\\', check_scoring,\\n                         LogisticRegression(), roc_auc_score)\\n    assert_raises_regexp(ValueError, \\'make_scorer\\', check_scoring,\\n                         Ridge(), r2_score)\\n    assert_raises_regexp(ValueError, \\'make_scorer\\', check_scoring,\\n                         KMeans(), adjusted_rand_score)\\n', '#!/usr/bin/env python\\n#\\n# Copyright 2013 Quantopian, Inc.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nimport matplotlib.pyplot as plt\\n\\nfrom zipline.algorithm import TradingAlgorithm\\nfrom zipline.utils.factory import load_from_yahoo\\n\\n# Import exponential moving average from talib wrapper\\nfrom zipline.transforms.ta import EMA\\n\\nfrom datetime import datetime\\nimport pytz\\n\\n\\nclass DualEMATaLib(TradingAlgorithm):\\n    \"\"\"Dual Moving Average Crossover algorithm.\\n\\n    This algorithm buys apple once its short moving average crosses\\n    its long moving average (indicating upwards momentum) and sells\\n    its shares once the averages cross again (indicating downwards\\n    momentum).\\n\\n    \"\"\"\\n    def initialize(self, short_window=20, long_window=40):\\n        # Add 2 mavg transforms, one with a long window, one\\n        # with a short window.\\n        self.short_ema_trans = EMA(timeperiod=short_window)\\n        self.long_ema_trans = EMA(timeperiod=long_window)\\n\\n        # To keep track of whether we invested in the stock or not\\n        self.invested = False\\n\\n    def handle_data(self, data):\\n        self.short_ema = self.short_ema_trans.handle_data(data)\\n        self.long_ema = self.long_ema_trans.handle_data(data)\\n        if self.short_ema is None or self.long_ema is None:\\n            return\\n\\n        self.buy = False\\n        self.sell = False\\n\\n        if (self.short_ema > self.long_ema).all() and not self.invested:\\n            self.order(\\'AAPL\\', 100)\\n            self.invested = True\\n            self.buy = True\\n        elif (self.short_ema < self.long_ema).all() and self.invested:\\n            self.order(\\'AAPL\\', -100)\\n            self.invested = False\\n            self.sell = True\\n\\n        self.record(AAPL=data[\\'AAPL\\'].price,\\n                    short_ema=self.short_ema[\\'AAPL\\'],\\n                    long_ema=self.long_ema[\\'AAPL\\'],\\n                    buy=self.buy,\\n                    sell=self.sell)\\n\\nif __name__ == \\'__main__\\':\\n    start = datetime(1990, 1, 1, 0, 0, 0, 0, pytz.utc)\\n    end = datetime(1991, 1, 1, 0, 0, 0, 0, pytz.utc)\\n    data = load_from_yahoo(stocks=[\\'AAPL\\'], indexes={}, start=start,\\n                           end=end)\\n\\n    dma = DualEMATaLib()\\n    results = dma.run(data).dropna()\\n\\n    fig = plt.figure()\\n    ax1 = fig.add_subplot(211, ylabel=\\'portfolio value\\')\\n    results.portfolio_value.plot(ax=ax1)\\n\\n    ax2 = fig.add_subplot(212)\\n    results[[\\'AAPL\\', \\'short_ema\\', \\'long_ema\\']].plot(ax=ax2)\\n\\n    ax2.plot(results.ix[results.buy].index, results.short_ema[results.buy],\\n             \\'^\\', markersize=10, color=\\'m\\')\\n    ax2.plot(results.ix[results.sell].index, results.short_ema[results.sell],\\n             \\'v\\', markersize=10, color=\\'k\\')\\n    plt.legend(loc=0)\\n    plt.gcf().set_size_inches(18, 8)\\n', '\"\"\"\\n=======================================\\nReceiver Operating Characteristic (ROC)\\n=======================================\\n\\nExample of Receiver Operating Characteristic (ROC) metric to evaluate\\nclassifier output quality.\\n\\nROC curves typically feature true positive rate on the Y axis, and false\\npositive rate on the X axis. This means that the top left corner of the plot is\\nthe \"ideal\" point - a false positive rate of zero, and a true positive rate of\\none. This is not very realistic, but it does mean that a larger area under the\\ncurve (AUC) is usually better.\\n\\nThe \"steepness\" of ROC curves is also important, since it is ideal to maximize\\nthe true positive rate while minimizing the false positive rate.\\n\\nROC curves are typically used in binary classification to study the output of\\na classifier. In order to extend ROC curve and ROC area to multi-class\\nor multi-label classification, it is necessary to binarize the output. One ROC\\ncurve can be drawn per label, but one can also draw a ROC curve by considering\\neach element of the label indicator matrix as a binary prediction\\n(micro-averaging).\\n\\n.. note::\\n\\n    See also :func:`sklearn.metrics.roc_auc_score`,\\n             :ref:`example_model_selection_plot_roc_crossval.py`.\\n\\n\"\"\"\\nprint(__doc__)\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn import svm, datasets\\nfrom sklearn.metrics import roc_curve, auc\\nfrom sklearn.cross_validation import train_test_split\\nfrom sklearn.preprocessing import label_binarize\\nfrom sklearn.multiclass import OneVsRestClassifier\\n\\n# Import some data to play with\\niris = datasets.load_iris()\\nX = iris.data\\ny = iris.target\\n\\n# Binarize the output\\ny = label_binarize(y, classes=[0, 1, 2])\\nn_classes = y.shape[1]\\n\\n# Add noisy features to make the problem harder\\nrandom_state = np.random.RandomState(0)\\nn_samples, n_features = X.shape\\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\\n\\n# shuffle and split training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\\n                                                    random_state=0)\\n\\n# Learn to predict each class against the other\\nclassifier = OneVsRestClassifier(svm.SVC(kernel=\\'linear\\', probability=True,\\n                                 random_state=random_state))\\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\\n\\n# Compute ROC curve and ROC area for each class\\nfpr = dict()\\ntpr = dict()\\nroc_auc = dict()\\nfor i in range(n_classes):\\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\\n    roc_auc[i] = auc(fpr[i], tpr[i])\\n\\n# Compute micro-average ROC curve and ROC area\\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\\n\\n# Plot of a ROC curve for a specific class\\nplt.figure()\\nplt.plot(fpr[2], tpr[2], label=\\'ROC curve (area = %0.2f)\\' % roc_auc[2])\\nplt.plot([0, 1], [0, 1], \\'k--\\')\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\'False Positive Rate\\')\\nplt.ylabel(\\'True Positive Rate\\')\\nplt.title(\\'Receiver operating characteristic example\\')\\nplt.legend(loc=\"lower right\")\\nplt.show()\\n\\n# Plot ROC curve\\nplt.figure()\\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\\n         label=\\'micro-average ROC curve (area = {0:0.2f})\\'\\n               \\'\\'.format(roc_auc[\"micro\"]))\\nfor i in range(n_classes):\\n    plt.plot(fpr[i], tpr[i], label=\\'ROC curve of class {0} (area = {1:0.2f})\\'\\n                                   \\'\\'.format(i, roc_auc[i]))\\n\\nplt.plot([0, 1], [0, 1], \\'k--\\')\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\'False Positive Rate\\')\\nplt.ylabel(\\'True Positive Rate\\')\\nplt.title(\\'Some extension of Receiver operating characteristic to multi-class\\')\\nplt.legend(loc=\"lower right\")\\nplt.show()\\n', '\\n\\nimport requests\\nimport json\\nimport numpy as np\\nimport pandas as pd\\nimport CoHouseToken\\nfrom difflib import SequenceMatcher\\n\\n\\n\\n# In[3]:\\n\\n\\ndef exactMatch(line1, line2):\\n    line1=line1.upper().rstrip()    \\n    line2=line2.upper().rstrip()\\n    #print(\"|\"+line1+\"|\"+line2+\"|\",line1==line2)\\n    return line1==line2\\n\\n\\n\\n\\ndef aStopWord(word):\\n    return word.upper().replace(\"COMPANY\",\"CO\").replace(\"LIMITED\",\"LTD\").replace(\"&\",\"AND\").rstrip() \\n\\n\\n\\n\\n\\ndef spaces(word):\\n    w = word.upper().replace(\"/\",\" \")\\n    w = w.replace(\".\",\" \").replace(\",\",\" \").replace(\"-\",\" \").rstrip() \\n    return w\\n\\n\\n\\n\\n\\ndef removeAStopWord(word):\\n    w = word.upper().replace(\"LTD\",\" \").replace(\"CO\",\" \").replace(\"AND\",\" \").replace(\"(\",\" \").replace(\"/\",\" \")\\n    w = w.replace(\")\",\" \").replace(\".\",\" \").replace(\",\",\" \").replace(\"-\",\" \").rstrip() \\n    return w\\n\\n\\n\\n\\n\\ndef removeABlank(word):\\n    w = word.replace(\" \",\"\")\\n    return w\\n\\n\\n\\n\\n\\ndef removeABracket (line):\\n    flag = False\\n    word=\"\"\\n    for a in line:\\n        if a==\"(\":\\n            flag = True\\n            a=\"\"\\n        if a==\")\":\\n            a=\"\"\\n            flag = False\\n        if flag:\\n            a=\"\"\\n        word+=a\\n    return word\\n    \\n\\n\\n\\n\\n\\ndef stopWord(line1, line2):\\n    line1=aStopWord(line1)  \\n    line2=aStopWord(line2)\\n    #print(\"|\"+line1+\"|\"+line2+\"|\",line1==line2)\\n    return line1==line2\\n\\n\\n\\n\\n\\ndef removeStopWord(line1, line2):\\n    line1=spaces(line1)  \\n    line2=spaces(line2)\\n    line1=aStopWord(line1)  \\n    line2=aStopWord(line2)\\n    line1=removeAStopWord(line1)  \\n    line2=removeAStopWord(line2)\\n    #print(\"|\"+line1+\"|\"+line2+\"|\",line1==line2)\\n    return line1==line2\\n\\n\\n\\n\\n\\ndef removeBlanks(line1, line2):\\n    line1=spaces(line1)  \\n    line2=spaces(line2)\\n    line1=aStopWord(line1)  \\n    line2=aStopWord(line2)\\n    line1=removeAStopWord(line1)  \\n    line2=removeAStopWord(line2)\\n    line1=removeABlank(line1)  \\n    line2=removeABlank(line2)\\n    return line1==line2\\n\\n\\n\\n\\n\\ndef removeBrackets(line1, line2):\\n    line1=removeABracket(line1)  \\n    line2=removeABracket(line2)\\n    line1=spaces(line1)  \\n    line2=spaces(line2)\\n    line1=aStopWord(line1)  \\n    line2=aStopWord(line2)\\n    line1=removeAStopWord(line1)  \\n    line2=removeAStopWord(line2)\\n    line1=removeABlank(line1)  \\n    line2=removeABlank(line2)\\n   #print(\"|\"+line1+\"|\"+line2+\"|\",line1==line2)\\n    \\n    return line1==line2\\n\\n\\n\\n\\n\\ndef strip(line1, line2):\\n    line1=removeABracket(line1)  \\n    line2=removeABracket(line2)\\n    line1=spaces(line1)  \\n    line2=spaces(line2)\\n    line1=aStopWord(line1)  \\n    line2=aStopWord(line2)\\n    line1=removeAStopWord(line1)  \\n    line2=removeAStopWord(line2)\\n    line1=removeABlank(line1)  \\n    line2=removeABlank(line2)\\n    \\n    return line1,line2\\n\\n\\n\\n\\n\\ndef match(company,results):\\n    for i in results[\\'items\\']:\\n        line = i[\\'title\\']\\n        number = i[\\'company_number\\']\\n        if(exactMatch(company,line)):\\n            return True,line,number\\n            \\n    for i in results[\\'items\\']:\\n        line = i[\\'title\\']\\n        number = i[\\'company_number\\']\\n        if(stopWord(company,line)):\\n            return True,line,number\\n            \\n    for i in results[\\'items\\']:\\n        line = i[\\'title\\']\\n        number = i[\\'company_number\\']\\n        if(removeStopWord(company,line)):\\n            return True,line,number\\n            \\n    for i in results[\\'items\\']:\\n        line = i[\\'title\\']\\n        number = i[\\'company_number\\']\\n        if(removeBlanks(company,line)):\\n            return True,line,number\\n            \\n    for i in results[\\'items\\']:\\n        line = i[\\'title\\']\\n        number = i[\\'company_number\\']\\n        if(removeBrackets(company,line)):\\n            return True,line,number\\n        \\n        #old_match(company,results)\\n    return False,\"\",\"\"\\n\\n\\n\\n\\ndef main(args):\\n    print(args[0])\\n    search_url =\"https://api.companieshouse.gov.uk/search/companies?q=\"\\n    token = CoHouseToken.getToken()\\n    pw = \\'\\'\\n    base_url = \\'https://api.companieshouse.gov.uk\\'\\n    file = args[1]\\n    print(file)\\n    df = pd.read_csv(file,names=[\\'Organisation\\'])\\n    companies = df.Organisation\\n    count=0\\n    found = open(\"found2.csv\",\\'w\\')\\n    missing = open(\"missing2.csv\",\\'w\\')\\n\\n    for c in companies:\\n        c =c.upper().replace(\"&\",\"AND\")\\n        c = c.split(\" T/A \")[0]\\n        c = c.split(\"WAS \")[0]\\n        c= spaces(c)\\n        url=search_url+c\\n        results = json.loads(requests.get(url, auth=(token,pw)).text)\\n        for i , key  in enumerate(results[\\'items\\']):\\n            a,b = strip(c, key[\\'title\\'])\\n            r = SequenceMatcher(None, a, b).ratio()\\n            print(\"%s \\\\t %s\\\\t %.2f \\\\t %s \\\\t %s\"%(i,c,r,key[\\'company_number\\'],key[\\'title\\']))\\n        \\n        v = input(\\'type number or return to reject: \\')\\n        if v ==\"\":\\n            print(\"reject\")\\n            missing.write(\"%s\\\\n\"%(c))\\n        else:\\n            key = results[\\'items\\'][int(v)]\\n            print(\"%s \\\\t %s\\\\t %.2f \\\\t %s \\\\t %s\"%(v,c,r,key[\\'company_number\\'],key[\\'title\\']))\\n            print(\"*************************\")\\n            found.write(\"%s,%s,%s,\\\\n\"%(c,key[\\'title\\'],key[\\'company_number\\']))\\n        \\n            \\n    print()\\n    #print(count/len(companies))\\n\\n\\n\\n\\n    return 0\\n\\nif __name__ == \\'__main__\\':\\n    import sys\\n    sys.exit(main(sys.argv))\\n\\n\\n\\n\\n\\n', '\\'\\'\\'\\nThis is a dummy file for me to get started making an Ising model. I\\'ll get this 2-D Ising running, then generalize.\\n\\'\\'\\'\\n\\nimport argparse\\nfrom itertools import izip\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef run_ising(N, d, K, J,h, n_steps, plot = False):\\n    \\'\\'\\'\\n\\n    :param N:\\n    :param d:\\n    :param K:\\n    :param J:\\n    :param h:\\n    :param n_steps:\\n    :param plot:\\n    :return:\\n    \\'\\'\\'\\n\\n    if plot:\\n        try:\\n            assert d <= 2\\n        except AssertionError:\\n            raise AssertionError(\"Can only plot in one or two dimensions.\")\\n\\n    #TODO wrap these better\\n    assert N >0 and N < 1000\\n    assert d > 0\\n    assert n_steps > 0\\n\\n    np.random.seed(0)\\n\\n    size = tuple(N for i in xrange(d))\\n    lattice = np.ones(size)\\n    #make a random initial state\\n    lattice-= np.random.randint(0,2, size =size)*2\\n\\n    # do different initialization\\n    E_0 = energy(lattice, potential, K, h)\\n    if plot:\\n        plt.ion()\\n    for step in xrange(n_steps):\\n        if step%1000 == 0:\\n            print step\\n        site = tuple(np.random.randint(0, N, size=d))\\n        # consider flipping this site\\n        lattice[site] *= -1\\n        E_f = energy(lattice, potential, K, h)\\n\\n        # if E_F < E_0, keep\\n        # if E_F > E_0, keep randomly given change of energies\\n        if E_f >= E_0:\\n            keep = np.random.uniform() < np.exp(K / J * (E_0 - E_f))\\n        else:\\n            keep = True\\n\\n        if keep:\\n            E_0 = E_f\\n        else:\\n            lattice[site] *= -1\\n\\n        # fig = plt.figure()\\n        if plot and step % 100 == 0:\\n            if d == 1:\\n                plt.imshow(lattice.reshape((1, -1)),interpolation=\\'none\\')\\n            else:\\n                plt.imshow(lattice, interpolation=\\'none\\')\\n            plt.title(correlation(lattice, N/2))\\n            plt.pause(0.01)\\n            plt.clf()\\n\\n    return np.array([correlation(lattice, r) for r in xrange(1, N/2+1)])\\n\\ndef get_NN(site, N, d, r= 1):\\n    \\'\\'\\'\\n    The NN of the site. Will only return those UP in index (east, south, and down) to avoid double counting.\\n    Accounts for PBC\\n    :param site:\\n        (d,) array of coordinates in the lattice\\n    :param N:\\n        Size of one side of the lattice\\n    :param d:\\n        dimension of the lattice\\n    :return:\\n        dxd numpy array where each row corresponds to the nearest neighbors.\\n    \\'\\'\\'\\n    mult_sites = np.r_[ [site for i in xrange(d)]]\\n    adjustment = np.eye(d)*r\\n    return ((mult_sites+adjustment)%N).astype(int)\\n\\n\\ndef potential(s1, s2, K, h):\\n    \\'\\'\\'\\n    Basic Ising potential\\n    :param s1:\\n        First spin (-1 or 1)\\n    :param s2:\\n        Second spin\\n    :param K:\\n        Coupling constant\\n    :return:\\n        Energy of this particular bond\\n    \\'\\'\\'\\n    return -1*K*s1*s2 - h/2*(s1+s2)#should this be abstracted to call the NN function?\\n\\ndef energy(lattice, potential, K, h = 0):\\n    \\'\\'\\'\\n    Calculate the energy of a lattice\\n    :param lattice:\\n        Lattice to calculate the energy on\\n    :param potential:\\n        Function defining the potential of a given site.\\n    :return:\\n        Energy of the lattice\\n    \\'\\'\\'\\n    N = lattice.shape[0]\\n    d = len(lattice.shape)\\n\\n    dim_slices = np.meshgrid(*(xrange(N) for i in xrange(d)), indexing = \\'ij\\')\\n    all_sites = izip(*[slice.flatten() for slice in dim_slices])\\n\\n    E = 0\\n    for site in all_sites:\\n        nn = get_NN(site, N, d)\\n        for neighbor in nn:\\n            E+=potential(lattice[site], lattice[tuple(neighbor)],K = K, h = h)\\n\\n    return E\\n\\ndef magnetization(lattice):\\n    return lattice.mean()\\n\\ndef correlation(lattice, r):\\n    \\'\\'\\'\\n    The average spin correlation at distance r.\\n    :param lattice:\\n        The lattice to calculate the statistic on.\\n    :param r:\\n        Distance to measure correlation\\n    :return:\\n    \\'\\'\\'\\n    N = lattice.shape[0]\\n    d = len(lattice.shape)\\n\\n    dim_slices = np.meshgrid(*(xrange(N) for i in xrange(d)), indexing=\\'ij\\')\\n    all_sites = izip(*[slice.flatten() for slice in dim_slices])\\n\\n    xi = 0\\n    for site in all_sites:\\n        nn = get_NN(site, N, d, r)\\n        for neighbor in nn:\\n            xi += lattice[site]*lattice[tuple(neighbor)]\\n\\n    return xi/((N**d)*d)\\n\\nif __name__  == \\'__main__\\':\\n    parser = argparse.ArgumentParser(description=\\'Simulate an ising model\\')\\n    parser.add_argument(\\'N\\', type = int, help = \\'Length of one side of the cube.\\')\\n    parser.add_argument(\\'d\\', type = int, help = \\'Number of dimensions of the cube.\\')\\n    #parser.add_argument(\\'K\\', type = float, help =\\'Bond coupling strength.\\')\\n\\n    parser.add_argument(\\'J\\', type = float, default = 1.0, nargs = \\'?\\',\\\\\\n                        help = \\'Energy of bond strength. Optional, default is 1.\\')\\n    parser.add_argument(\\'h\\', type = float, default=0.0, nargs = \\'?\\',\\\\\\n                        help = \\'Magnetic field strength. Optional, default is 0.\\')\\n    parser.add_argument(\\'n_steps\\', type = int, default = 1000, nargs = \\'?\\',\\\\\\n                        help = \\'Number of steps to simulate. Default is 1e5\\')\\n    parser.add_argument(\\'--plot\\', action = \\'store_true\\',\\\\\\n                        help = \\'Whether or not to plot results. Only allowed with d = 1 or 2.\\')\\n\\n    args = parser.parse_args()\\n    spins = []\\n    Ks = [ 0.5,0.6,0.65, 0.7,0.8, 0.9]\\n    for K in Ks:\\n        print K\\n        spins.append(run_ising(K = K, **vars(args)))\\n\\n    for K, spin in izip(Ks, spins):\\n        plt.plot(spin, label = K )\\n    plt.legend(loc = \\'best\\')\\n    plt.ylim([-0.1, 1.1])\\n    plt.show()', 'import cloud_detection_new as cloud_detection\\nfrom matplotlib import pyplot as plt\\nimport views\\nfrom skimage import exposure\\n\\nnir = cloud_detection.get_nir()[0:600,2000:2600]\\nred = cloud_detection.get_red()[0:600,2000:2600]\\ngreen = cloud_detection.get_green()[0:600,2000:2600]\\nblue = cloud_detection.get_blue()[0:600,2000:2600] # or use coastal\\ncoastal = cloud_detection.get_coastal()[0:600,2000:2600]\\nmarine_shadow_index = (green-blue)/(green+blue)\\n\\nimg = views.create_composite(red, green, blue)\\nimg_rescale = exposure.rescale_intensity(img, in_range=(0, 90))\\n\\nplt.rcParams[\\'savefig.facecolor\\'] = \"0.8\"\\nvmin, vmax=0.0,0.1\\ndef example_plot(ax, data, fontsize=12):\\n     ax.imshow(data, vmin=vmin, vmax=vmax)\\n     ax.locator_params(nbins=3)\\n     ax.set_xlabel(\\'x-label\\', fontsize=fontsize)\\n     ax.set_ylabel(\\'y-label\\', fontsize=fontsize)\\n     ax.set_title(\\'Title\\', fontsize=fontsize)\\n\\nplt.close(\\'all\\')\\nfig = plt.figure\\n\\n\\nax1=plt.subplot(243)\\nax2=plt.subplot(244)\\nax3=plt.subplot(247)\\nax4=plt.subplot(248)\\nax5=plt.subplot(121)\\n\\na_coastal = coastal[500:600, 500:600]\\na_blue = blue[500:600, 500:600]\\na_green = green[500:600, 500:600]\\na_red = red[500:600, 500:600]\\na_nir = nir[500:600, 500:600]\\na_img = img[500:600, 500:600]\\nspec1 = [a_coastal[60, 60], a_blue[60, 60], a_green[60, 60], a_red[60, 60], a_nir[60, 60]]\\n\\nb_coastal = coastal[200:300, 100:200]\\nb_blue = blue[200:300, 100:200]\\nb_green = green[200:300, 100:200]\\nb_red = red[200:300, 100:200]\\nb_nir = nir[200:300, 100:200]\\nb_img = img[200:300, 100:200]\\n\\nexample_plot(ax1, coastal)\\nexample_plot(ax2, blue)\\nexample_plot(ax3, green)\\nexample_plot(ax4, red)\\nax5.imshow(img)\\n\\n# plt.tight_layout()\\nplt.close(\\'all\\')\\nspec = [b_coastal[60, 60], b_blue[60, 60], b_green[60, 60], b_red[60, 60], b_nir[60, 60]]\\nplt.plot(spec, \\'k*-\\')\\nplt.plot(spec1, \\'k.-\\')\\n\\nplt.close(\\'all\\')\\ncbg = (coastal+blue+green)/3\\n\\nplt.imshow(cbg/red)', '# -*- coding: utf-8 -*-\\n\"\"\"\\nPlot oscilloscope files from MultiSim\\n\"\"\"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport sys\\nimport os\\nfrom matplotlib import rc\\n\\nrc(\\'font\\',family=\"Consolas\")\\nfiles=[\"real_zad5_05f_p2.txt\"]\\nfor NazwaPliku in files:\\n    print NazwaPliku\\n    Plik=open(NazwaPliku)\\n    #print DeltaT\\n    Dane=Plik.readlines()#[4:]\\n    DeltaT=float(Dane[2].split()[3].replace(\",\",\".\"))\\n    #M=len(Dane[4].split())/2\\n    M=2\\n    Dane=Dane[5:]\\n    Plik.close()\\n\\n    print M\\n    Ys=[np.zeros(len(Dane)) for i in range(M)]\\n\\n    for m in range(M):\\n        for i in range(len(Dane)):\\n            try:\\n                Ys[m][i]=float(Dane[i].split()[2+3*m].replace(\",\",\".\"))\\n            except:\\n                print m, i, 2+3*m, len(Dane[i].split()), Dane[i].split()\\n        #print i, Y[i]\\n    X=np.zeros_like(Ys[0])\\n    for i in range(len(X)):\\n        X[i]=i*DeltaT\\n\\n    for y in Ys:\\n        print max(y)-min(y)\\n    Opis=u\"Ukad szeregowy\\\\nPoowa czstotliwoci rezonansowej\"\\n    Nazwa=u\"Z5W2\"\\n    plt.title(u\"Przebieg napiciowy\\\\n\"+Opis)\\n    plt.xlabel(u\"Czas t [s]\")\\n    plt.ylabel(u\"Napicie [V]\")\\n    plt.plot(X,Ys[0],label=u\"Wejcie\")\\n    plt.plot(X,Ys[1],label=u\"Wyjcie\")\\n    plt.grid()\\n    plt.legend(loc=\"best\")\\n    plt.savefig(Nazwa + \".png\", bbox_inches=\\'tight\\')\\n    plt.show()\\n\\n\\n\\n', '#!/usr/bin/env python\\n__author__ = \\'saguinag\\' + \\'@\\' + \\'nd.edu\\'\\n__version__ = \"0.1.0\"\\n\\n##\\n## fname \"b2CliqueTreeRules.py\"\\n##\\n\\n## TODO: some todo list\\n\\n## VersionLog:\\n\\nimport net_metrics as metrics\\nimport pandas as pd\\nimport argparse, traceback\\nimport os, sys\\nimport networkx as nx\\nimport re\\nfrom collections import deque, defaultdict, Counter\\nimport tree_decomposition as td\\nimport PHRG as phrg\\nimport probabilistic_cfg as pcfg\\nimport exact_phrg as xphrg\\nimport a1_hrg_cliq_tree as nfld\\nfrom a1_hrg_cliq_tree import load_edgelist\\n\\nDEBUG = False\\n\\n\\ndef get_parser ():\\n  parser = argparse.ArgumentParser(description=\\'b2CliqueTreeRules.py: given a tree derive grammar rules\\')\\n  parser.add_argument(\\'-t\\', \\'--treedecomp\\', required=True, help=\\'input tree decomposition (dimacs file format)\\')\\n  parser.add_argument(\\'--version\\', action=\\'version\\', version=__version__)\\n  return parser\\n\\ndef dimacs_td_ct (tdfname):\\n  \"\"\" tree decomp to clique-tree \"\"\"\\n\\n  print \\'... input file:\\', tdfname\\n  fname = tdfname\\n  graph_name = os.path.basename(fname)\\n  gname = graph_name.split(\\'.\\')[0]\\n  gfname = \"datasets/out.\" + gname\\n  tdh = os.path.basename(fname).split(\\'.\\')[1] # tree decomp heuristic\\n  tfname = gname+\".\"+tdh\\n\\n  G = load_edgelist(gfname)\\n\\n  if DEBUG: print nx.info(G)\\n  print\\n  with open(fname, \\'r\\') as f:  # read tree decomp from inddgo\\n    lines = f.readlines()\\n    lines = [x.rstrip(\\'\\\\r\\\\n\\') for x in lines]\\n\\n  cbags = {}\\n  bags = [x.split() for x in lines if x.startswith(\\'B\\')]\\n\\n  for b in bags:\\n    cbags[int(b[1])] = [int(x) for x in b[3:]]  # what to do with bag size?\\n\\n  edges = [x.split()[1:] for x in lines if x.startswith(\\'e\\')]\\n  edges = [[int(k) for k in x] for x in edges]\\n\\n  tree = defaultdict(set)\\n  for s, t in edges:\\n    tree[frozenset(cbags[s])].add(frozenset(cbags[t]))\\n    if DEBUG: print \\'.. # of keys in `tree`:\\', len(tree.keys())\\n  if DEBUG: print tree.keys()\\n  root = list(tree)[0]\\n  if DEBUG: print \\'.. Root:\\', root\\n  root = frozenset(cbags[1])\\n  if DEBUG: print \\'.. Root:\\', root\\n  T = td.make_rooted(tree, root)\\n  if DEBUG: print \\'.. T rooted:\\', len(T)\\n  # nfld.unfold_2wide_tuple(T) # lets me display the tree\\'s frozen sets\\n\\n  T = phrg.binarize(T)\\n\\n  prod_rules = {}\\n  td.new_visit(T, G, prod_rules)\\n\\n  if DEBUG: print \"--------------------\"\\n  if DEBUG: print \"- Production Rules -\"\\n  if DEBUG: print \"--------------------\"\\n\\n  for k in prod_rules.iterkeys():\\n    if DEBUG: print k\\n    s = 0\\n    for d in prod_rules[k]:\\n      s += prod_rules[k][d]\\n    for d in prod_rules[k]:\\n      prod_rules[k][d] = float(prod_rules[k][d]) / float(s)  # normailization step to create probs not counts.\\n      if DEBUG: print \\'\\\\t -> \\', d, prod_rules[k][d]\\n\\n  rules = []\\n  id = 0\\n  for k, v in prod_rules.iteritems():\\n    sid = 0\\n    for x in prod_rules[k]:\\n      rhs = re.findall(\"[^()]+\", x)\\n      rules.append((\"r%d.%d\" % (id, sid), \"%s\" % re.findall(\"[^()]+\", k)[0], rhs, prod_rules[k][x]))\\n      if DEBUG: print (\"r%d.%d\" % (id, sid), \"%s\" % re.findall(\"[^()]+\", k)[0], rhs, prod_rules[k][x])\\n\\n      sid += 1\\n    id += 1\\n\\n  df = pd.DataFrame(rules)\\n\\n  outdf_fname = \"./ProdRules/\"+tfname+\".prules\"\\n  if not os.path.isfile(outdf_fname+\".bz2\"):\\n    print \\'...\\',outdf_fname, \"written\"\\n    df.to_csv(outdf_fname+\".bz2\", compression=\"bz2\")\\n  else:\\n    print \\'...\\', outdf_fname, \"file exists\"\\n  return\\n\\n\\ndef main ():\\n  parser = get_parser()\\n  args = vars(parser.parse_args())\\n\\n  dimacs_td_ct(args[\\'treedecomp\\'])  # gen synth graph\\n\\n\\nif __name__ == \\'__main__\\':\\n  try:\\n    main()\\n  except Exception, e:\\n    print str(e)\\n    traceback.print_exc()\\n    sys.exit(1)\\n  sys.exit(0)\\n', '#\\n# Licensed to the Apache Software Foundation (ASF) under one or more\\n# contributor license agreements.  See the NOTICE file distributed with\\n# this work for additional information regarding copyright ownership.\\n# The ASF licenses this file to You under the Apache License, Version 2.0\\n# (the \"License\"); you may not use this file except in compliance with\\n# the License.  You may obtain a copy of the License at\\n#\\n#    http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n\\n\"\"\"\\nA collections of builtin functions\\n\"\"\"\\nimport sys\\nimport functools\\nimport warnings\\n\\nfrom pyspark import since, SparkContext\\nfrom pyspark.rdd import PythonEvalType\\nfrom pyspark.sql.column import Column, _to_java_column, _to_seq, _create_column_from_literal\\nfrom pyspark.sql.dataframe import DataFrame\\nfrom pyspark.sql.types import StringType, DataType\\n# Keep UserDefinedFunction import for backwards compatible import; moved in SPARK-22409\\nfrom pyspark.sql.udf import UserDefinedFunction, _create_udf  # noqa: F401\\nfrom pyspark.sql.udf import _create_udf\\n# Keep pandas_udf and PandasUDFType import for backwards compatible import; moved in SPARK-28264\\nfrom pyspark.sql.pandas.functions import pandas_udf, PandasUDFType  # noqa: F401\\nfrom pyspark.sql.utils import to_str\\n\\n# Note to developers: all of PySpark functions here take string as column names whenever possible.\\n# Namely, if columns are referred as arguments, they can be always both Column or string,\\n# even though there might be few exceptions for legacy or inevitable reasons.\\n# If you are fixing other language APIs together, also please note that Scala side is not the case\\n# since it requires to make every single overridden definition.\\n\\n\\ndef _get_get_jvm_function(name, sc):\\n    \"\"\"\\n    Retrieves JVM function identified by name from\\n    Java gateway associated with sc.\\n    \"\"\"\\n    return getattr(sc._jvm.functions, name)\\n\\n\\ndef _invoke_function(name, *args):\\n    \"\"\"\\n    Invokes JVM function identified by name with args\\n    and wraps the result with :class:`~pyspark.sql.Column`.\\n    \"\"\"\\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\\n    return Column(jf(*args))\\n\\n\\ndef _invoke_function_over_column(name, col):\\n    \"\"\"\\n    Invokes unary JVM function identified by name\\n    and wraps the result with :class:`~pyspark.sql.Column`.\\n    \"\"\"\\n    return _invoke_function(name, _to_java_column(col))\\n\\n\\ndef _invoke_binary_math_function(name, col1, col2):\\n    \"\"\"\\n    Invokes binary JVM math function identified by name\\n    and wraps the result with :class:`~pyspark.sql.Column`.\\n    \"\"\"\\n    return _invoke_function(\\n        name,\\n        # For legacy reasons, the arguments here can be implicitly converted into floats,\\n        # if they are not columns or strings.\\n        _to_java_column(col1) if isinstance(col1, (str, Column)) else float(col1),\\n        _to_java_column(col2) if isinstance(col2, (str, Column)) else float(col2)\\n    )\\n\\n\\ndef _options_to_str(options=None):\\n    if options:\\n        return {key: to_str(value) for (key, value) in options.items()}\\n    return {}\\n\\n\\ndef lit(col):\\n    \"\"\"\\n    Creates a :class:`~pyspark.sql.Column` of literal value.\\n\\n    .. versionadded:: 1.3.0\\n\\n    Examples\\n    --------\\n    >>> df.select(lit(5).alias(\\'height\\')).withColumn(\\'spark_user\\', lit(True)).take(1)\\n    [Row(height=5, spark_user=True)]\\n    \"\"\"\\n    return col if isinstance(col, Column) else _invoke_function(\"lit\", col)\\n\\n\\n@since(1.3)\\ndef col(col):\\n    \"\"\"\\n    Returns a :class:`~pyspark.sql.Column` based on the given column name.\\'\\n    Examples\\n    --------\\n    >>> col(\\'x\\')\\n    Column<\\'x\\'>\\n    >>> column(\\'x\\')\\n    Column<\\'x\\'>\\n    \"\"\"\\n    return _invoke_function(\"col\", col)\\n\\n\\ncolumn = col\\n\\n\\n@since(1.3)\\ndef asc(col):\\n    \"\"\"\\n    Returns a sort expression based on the ascending order of the given column name.\\n    \"\"\"\\n    return (\\n        col.asc() if isinstance(col, Column)\\n        else _invoke_function(\"asc\", col)\\n    )\\n\\n\\n@since(1.3)\\ndef desc(col):\\n    \"\"\"\\n    Returns a sort expression based on the descending order of the given column name.\\n    \"\"\"\\n    return (\\n        col.desc() if isinstance(col, Column)\\n        else _invoke_function(\"desc\", col)\\n    )\\n\\n\\n@since(1.3)\\ndef sqrt(col):\\n    \"\"\"\\n    Computes the square root of the specified float value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"sqrt\", col)\\n\\n\\n@since(1.3)\\ndef abs(col):\\n    \"\"\"\\n    Computes the absolute value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"abs\", col)\\n\\n\\n@since(1.3)\\ndef max(col):\\n    \"\"\"\\n    Aggregate function: returns the maximum value of the expression in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"max\", col)\\n\\n\\n@since(1.3)\\ndef min(col):\\n    \"\"\"\\n    Aggregate function: returns the minimum value of the expression in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"min\", col)\\n\\n\\n@since(1.3)\\ndef count(col):\\n    \"\"\"\\n    Aggregate function: returns the number of items in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"count\", col)\\n\\n\\n@since(1.3)\\ndef sum(col):\\n    \"\"\"\\n    Aggregate function: returns the sum of all values in the expression.\\n    \"\"\"\\n    return _invoke_function_over_column(\"sum\", col)\\n\\n\\n@since(1.3)\\ndef avg(col):\\n    \"\"\"\\n    Aggregate function: returns the average of the values in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"avg\", col)\\n\\n\\n@since(1.3)\\ndef mean(col):\\n    \"\"\"\\n    Aggregate function: returns the average of the values in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"mean\", col)\\n\\n\\n@since(1.3)\\ndef sumDistinct(col):\\n    \"\"\"\\n    Aggregate function: returns the sum of distinct values in the expression.\\n\\n    .. deprecated:: 3.2.0\\n        Use :func:`sum_distinct` instead.\\n    \"\"\"\\n    warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\\n    return sum_distinct(col)\\n\\n\\n@since(3.2)\\ndef sum_distinct(col):\\n    \"\"\"\\n    Aggregate function: returns the sum of distinct values in the expression.\\n    \"\"\"\\n    return _invoke_function_over_column(\"sum_distinct\", col)\\n\\n\\ndef product(col):\\n    \"\"\"\\n    Aggregate function: returns the product of the values in a group.\\n\\n    .. versionadded:: 3.2.0\\n\\n    Parameters\\n    ----------\\n    col : str, :class:`Column`\\n        column containing values to be multiplied together\\n\\n    Examples\\n    --------\\n    >>> df = spark.range(1, 10).toDF(\\'x\\').withColumn(\\'mod3\\', col(\\'x\\') % 3)\\n    >>> prods = df.groupBy(\\'mod3\\').agg(product(\\'x\\').alias(\\'product\\'))\\n    >>> prods.orderBy(\\'mod3\\').show()\\n    +----+-------+\\n    |mod3|product|\\n    +----+-------+\\n    |   0|  162.0|\\n    |   1|   28.0|\\n    |   2|   80.0|\\n    +----+-------+\\n\\n    \"\"\"\\n    return _invoke_function_over_column(\"product\", col)\\n\\n\\ndef acos(col):\\n    \"\"\"\\n    .. versionadded:: 1.4.0\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"acos\", col)\\n\\n\\ndef acosh(col):\\n    \"\"\"\\n    Computes inverse hyperbolic cosine of the input column.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n    \"\"\"\\n    return _invoke_function_over_column(\"acosh\", col)\\n\\n\\ndef asin(col):\\n    \"\"\"\\n    .. versionadded:: 1.3.0\\n\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        inverse sine of `col`, as if computed by `java.lang.Math.asin()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"asin\", col)\\n\\n\\ndef asinh(col):\\n    \"\"\"\\n    Computes inverse hyperbolic sine of the input column.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n    \"\"\"\\n    return _invoke_function_over_column(\"asinh\", col)\\n\\n\\ndef atan(col):\\n    \"\"\"\\n    .. versionadded:: 1.4.0\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"atan\", col)\\n\\n\\ndef atanh(col):\\n    \"\"\"\\n    Computes inverse hyperbolic tangent of the input column.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n    \"\"\"\\n    return _invoke_function_over_column(\"atanh\", col)\\n\\n\\n@since(1.4)\\ndef cbrt(col):\\n    \"\"\"\\n    Computes the cube-root of the given value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"cbrt\", col)\\n\\n\\n@since(1.4)\\ndef ceil(col):\\n    \"\"\"\\n    Computes the ceiling of the given value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"ceil\", col)\\n\\n\\ndef cos(col):\\n    \"\"\"\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        angle in radians\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        cosine of the angle, as if computed by `java.lang.Math.cos()`.\\n    \"\"\"\\n    return _invoke_function_over_column(\"cos\", col)\\n\\n\\ndef cosh(col):\\n    \"\"\"\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        hyperbolic angle\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"cosh\", col)\\n\\n\\n@since(1.4)\\ndef exp(col):\\n    \"\"\"\\n    Computes the exponential of the given value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"exp\", col)\\n\\n\\n@since(1.4)\\ndef expm1(col):\\n    \"\"\"\\n    Computes the exponential of the given value minus one.\\n    \"\"\"\\n    return _invoke_function_over_column(\"expm1\", col)\\n\\n\\n@since(1.4)\\ndef floor(col):\\n    \"\"\"\\n    Computes the floor of the given value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"floor\", col)\\n\\n\\n@since(1.4)\\ndef log(col):\\n    \"\"\"\\n    Computes the natural logarithm of the given value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"log\", col)\\n\\n\\n@since(1.4)\\ndef log10(col):\\n    \"\"\"\\n    Computes the logarithm of the given value in Base 10.\\n    \"\"\"\\n    return _invoke_function_over_column(\"log10\", col)\\n\\n\\n@since(1.4)\\ndef log1p(col):\\n    \"\"\"\\n    Computes the natural logarithm of the given value plus one.\\n    \"\"\"\\n    return _invoke_function_over_column(\"log1p\", col)\\n\\n\\n@since(1.4)\\ndef rint(col):\\n    \"\"\"\\n    Returns the double value that is closest in value to the argument and\\n    is equal to a mathematical integer.\\n    \"\"\"\\n    return _invoke_function_over_column(\"rint\", col)\\n\\n\\n@since(1.4)\\ndef signum(col):\\n    \"\"\"\\n    Computes the signum of the given value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"signum\", col)\\n\\n\\ndef sin(col):\\n    \"\"\"\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        sine of the angle, as if computed by `java.lang.Math.sin()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"sin\", col)\\n\\n\\ndef sinh(col):\\n    \"\"\"\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        hyperbolic angle\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        hyperbolic sine of the given value,\\n        as if computed by `java.lang.Math.sinh()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"sinh\", col)\\n\\n\\ndef tan(col):\\n    \"\"\"\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        angle in radians\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        tangent of the given value, as if computed by `java.lang.Math.tan()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"tan\", col)\\n\\n\\ndef tanh(col):\\n    \"\"\"\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        hyperbolic angle\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        hyperbolic tangent of the given value\\n        as if computed by `java.lang.Math.tanh()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"tanh\", col)\\n\\n\\n@since(1.4)\\ndef toDegrees(col):\\n    \"\"\"\\n    .. deprecated:: 2.1.0\\n        Use :func:`degrees` instead.\\n    \"\"\"\\n    warnings.warn(\"Deprecated in 2.1, use degrees instead.\", FutureWarning)\\n    return degrees(col)\\n\\n\\n@since(1.4)\\ndef toRadians(col):\\n    \"\"\"\\n    .. deprecated:: 2.1.0\\n        Use :func:`radians` instead.\\n    \"\"\"\\n    warnings.warn(\"Deprecated in 2.1, use radians instead.\", FutureWarning)\\n    return radians(col)\\n\\n\\n@since(1.4)\\ndef bitwiseNOT(col):\\n    \"\"\"\\n    Computes bitwise not.\\n\\n    .. deprecated:: 3.2.0\\n        Use :func:`bitwise_not` instead.\\n    \"\"\"\\n    warnings.warn(\"Deprecated in 3.2, use bitwise_not instead.\", FutureWarning)\\n    return bitwise_not(col)\\n\\n\\n@since(3.2)\\ndef bitwise_not(col):\\n    \"\"\"\\n    Computes bitwise not.\\n    \"\"\"\\n    return _invoke_function_over_column(\"bitwise_not\", col)\\n\\n\\n@since(2.4)\\ndef asc_nulls_first(col):\\n    \"\"\"\\n    Returns a sort expression based on the ascending order of the given\\n    column name, and null values return before non-null values.\\n    \"\"\"\\n    return (\\n        col.asc_nulls_first() if isinstance(col, Column)\\n        else _invoke_function(\"asc_nulls_first\", col)\\n    )\\n\\n\\n@since(2.4)\\ndef asc_nulls_last(col):\\n    \"\"\"\\n    Returns a sort expression based on the ascending order of the given\\n    column name, and null values appear after non-null values.\\n    \"\"\"\\n    return (\\n        col.asc_nulls_last() if isinstance(col, Column)\\n        else _invoke_function(\"asc_nulls_last\", col)\\n    )\\n\\n\\n@since(2.4)\\ndef desc_nulls_first(col):\\n    \"\"\"\\n    Returns a sort expression based on the descending order of the given\\n    column name, and null values appear before non-null values.\\n    \"\"\"\\n    return (\\n        col.desc_nulls_first() if isinstance(col, Column)\\n        else _invoke_function(\"desc_nulls_first\", col)\\n    )\\n\\n\\n@since(2.4)\\ndef desc_nulls_last(col):\\n    \"\"\"\\n    Returns a sort expression based on the descending order of the given\\n    column name, and null values appear after non-null values.\\n    \"\"\"\\n    return (\\n        col.desc_nulls_last() if isinstance(col, Column)\\n        else _invoke_function(\"desc_nulls_last\", col)\\n    )\\n\\n\\n@since(1.6)\\ndef stddev(col):\\n    \"\"\"\\n    Aggregate function: alias for stddev_samp.\\n    \"\"\"\\n    return _invoke_function_over_column(\"stddev\", col)\\n\\n\\n@since(1.6)\\ndef stddev_samp(col):\\n    \"\"\"\\n    Aggregate function: returns the unbiased sample standard deviation of\\n    the expression in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"stddev_samp\", col)\\n\\n\\n@since(1.6)\\ndef stddev_pop(col):\\n    \"\"\"\\n    Aggregate function: returns population standard deviation of\\n    the expression in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"stddev_pop\", col)\\n\\n\\n@since(1.6)\\ndef variance(col):\\n    \"\"\"\\n    Aggregate function: alias for var_samp\\n    \"\"\"\\n    return _invoke_function_over_column(\"variance\", col)\\n\\n\\n@since(1.6)\\ndef var_samp(col):\\n    \"\"\"\\n    Aggregate function: returns the unbiased sample variance of\\n    the values in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"var_samp\", col)\\n\\n\\n@since(1.6)\\ndef var_pop(col):\\n    \"\"\"\\n    Aggregate function: returns the population variance of the values in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"var_pop\", col)\\n\\n\\n@since(1.6)\\ndef skewness(col):\\n    \"\"\"\\n    Aggregate function: returns the skewness of the values in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"skewness\", col)\\n\\n\\n@since(1.6)\\ndef kurtosis(col):\\n    \"\"\"\\n    Aggregate function: returns the kurtosis of the values in a group.\\n    \"\"\"\\n    return _invoke_function_over_column(\"kurtosis\", col)\\n\\n\\ndef collect_list(col):\\n    \"\"\"\\n    Aggregate function: returns a list of objects with duplicates.\\n\\n    .. versionadded:: 1.6.0\\n\\n    Notes\\n    -----\\n    The function is non-deterministic because the order of collected results depends\\n    on the order of the rows which may be non-deterministic after a shuffle.\\n\\n    Examples\\n    --------\\n    >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], (\\'age\\',))\\n    >>> df2.agg(collect_list(\\'age\\')).collect()\\n    [Row(collect_list(age)=[2, 5, 5])]\\n    \"\"\"\\n    return _invoke_function_over_column(\"collect_list\", col)\\n\\n\\ndef collect_set(col):\\n    \"\"\"\\n    Aggregate function: returns a set of objects with duplicate elements eliminated.\\n\\n    .. versionadded:: 1.6.0\\n\\n    Notes\\n    -----\\n    The function is non-deterministic because the order of collected results depends\\n    on the order of the rows which may be non-deterministic after a shuffle.\\n\\n    Examples\\n    --------\\n    >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], (\\'age\\',))\\n    >>> df2.agg(collect_set(\\'age\\')).collect()\\n    [Row(collect_set(age)=[5, 2])]\\n    \"\"\"\\n    return _invoke_function_over_column(\"collect_set\", col)\\n\\n\\ndef degrees(col):\\n    \"\"\"\\n    Converts an angle measured in radians to an approximately equivalent angle\\n    measured in degrees.\\n\\n    .. versionadded:: 2.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        angle in radians\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        angle in degrees, as if computed by `java.lang.Math.toDegrees()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"degrees\", col)\\n\\n\\ndef radians(col):\\n    \"\"\"\\n    Converts an angle measured in degrees to an approximately equivalent angle\\n    measured in radians.\\n\\n    .. versionadded:: 2.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        angle in degrees\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        angle in radians, as if computed by `java.lang.Math.toRadians()`\\n    \"\"\"\\n    return _invoke_function_over_column(\"radians\", col)\\n\\n\\ndef atan2(col1, col2):\\n    \"\"\"\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    col1 : str, :class:`~pyspark.sql.Column` or float\\n        coordinate on y-axis\\n    col2 : str, :class:`~pyspark.sql.Column` or float\\n        coordinate on x-axis\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n        the `theta` component of the point\\n        (`r`, `theta`)\\n        in polar coordinates that corresponds to the point\\n        (`x`, `y`) in Cartesian coordinates,\\n        as if computed by `java.lang.Math.atan2()`\\n    \"\"\"\\n    return _invoke_binary_math_function(\"atan2\", col1, col2)\\n\\n\\n@since(1.4)\\ndef hypot(col1, col2):\\n    \"\"\"\\n    Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\\n    \"\"\"\\n    return _invoke_binary_math_function(\"hypot\", col1, col2)\\n\\n\\n@since(1.4)\\ndef pow(col1, col2):\\n    \"\"\"\\n    Returns the value of the first argument raised to the power of the second argument.\\n    \"\"\"\\n    return _invoke_binary_math_function(\"pow\", col1, col2)\\n\\n\\n@since(1.6)\\ndef row_number():\\n    \"\"\"\\n    Window function: returns a sequential number starting at 1 within a window partition.\\n    \"\"\"\\n    return _invoke_function(\"row_number\")\\n\\n\\n@since(1.6)\\ndef dense_rank():\\n    \"\"\"\\n    Window function: returns the rank of rows within a window partition, without any gaps.\\n\\n    The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\\n    sequence when there are ties. That is, if you were ranking a competition using dense_rank\\n    and had three people tie for second place, you would say that all three were in second\\n    place and that the next person came in third. Rank would give me sequential numbers, making\\n    the person that came in third place (after the ties) would register as coming in fifth.\\n\\n    This is equivalent to the DENSE_RANK function in SQL.\\n    \"\"\"\\n    return _invoke_function(\"dense_rank\")\\n\\n\\n@since(1.6)\\ndef rank():\\n    \"\"\"\\n    Window function: returns the rank of rows within a window partition.\\n\\n    The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\\n    sequence when there are ties. That is, if you were ranking a competition using dense_rank\\n    and had three people tie for second place, you would say that all three were in second\\n    place and that the next person came in third. Rank would give me sequential numbers, making\\n    the person that came in third place (after the ties) would register as coming in fifth.\\n\\n    This is equivalent to the RANK function in SQL.\\n    \"\"\"\\n    return _invoke_function(\"rank\")\\n\\n\\n@since(1.6)\\ndef cume_dist():\\n    \"\"\"\\n    Window function: returns the cumulative distribution of values within a window partition,\\n    i.e. the fraction of rows that are below the current row.\\n    \"\"\"\\n    return _invoke_function(\"cume_dist\")\\n\\n\\n@since(1.6)\\ndef percent_rank():\\n    \"\"\"\\n    Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\\n    \"\"\"\\n    return _invoke_function(\"percent_rank\")\\n\\n\\n@since(1.3)\\ndef approxCountDistinct(col, rsd=None):\\n    \"\"\"\\n    .. deprecated:: 2.1.0\\n        Use :func:`approx_count_distinct` instead.\\n    \"\"\"\\n    warnings.warn(\"Deprecated in 2.1, use approx_count_distinct instead.\", FutureWarning)\\n    return approx_count_distinct(col, rsd)\\n\\n\\ndef approx_count_distinct(col, rsd=None):\\n    \"\"\"Aggregate function: returns a new :class:`~pyspark.sql.Column` for approximate distinct count\\n    of column `col`.\\n\\n    .. versionadded:: 2.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n    rsd : float, optional\\n        maximum relative standard deviation allowed (default = 0.05).\\n        For rsd < 0.01, it is more efficient to use :func:`count_distinct`\\n\\n    Examples\\n    --------\\n    >>> df.agg(approx_count_distinct(df.age).alias(\\'distinct_ages\\')).collect()\\n    [Row(distinct_ages=2)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if rsd is None:\\n        jc = sc._jvm.functions.approx_count_distinct(_to_java_column(col))\\n    else:\\n        jc = sc._jvm.functions.approx_count_distinct(_to_java_column(col), rsd)\\n    return Column(jc)\\n\\n\\n@since(1.6)\\ndef broadcast(df):\\n    \"\"\"Marks a DataFrame as small enough for use in broadcast joins.\"\"\"\\n\\n    sc = SparkContext._active_spark_context\\n    return DataFrame(sc._jvm.functions.broadcast(df._jdf), df.sql_ctx)\\n\\n\\ndef coalesce(*cols):\\n    \"\"\"Returns the first column that is not null.\\n\\n    .. versionadded:: 1.4.0\\n\\n    Examples\\n    --------\\n    >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\\n    >>> cDf.show()\\n    +----+----+\\n    |   a|   b|\\n    +----+----+\\n    |null|null|\\n    |   1|null|\\n    |null|   2|\\n    +----+----+\\n\\n    >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\\n    +--------------+\\n    |coalesce(a, b)|\\n    +--------------+\\n    |          null|\\n    |             1|\\n    |             2|\\n    +--------------+\\n\\n    >>> cDf.select(\\'*\\', coalesce(cDf[\"a\"], lit(0.0))).show()\\n    +----+----+----------------+\\n    |   a|   b|coalesce(a, 0.0)|\\n    +----+----+----------------+\\n    |null|null|             0.0|\\n    |   1|null|             1.0|\\n    |null|   2|             0.0|\\n    +----+----+----------------+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.coalesce(_to_seq(sc, cols, _to_java_column))\\n    return Column(jc)\\n\\n\\ndef corr(col1, col2):\\n    \"\"\"Returns a new :class:`~pyspark.sql.Column` for the Pearson Correlation Coefficient for\\n    ``col1`` and ``col2``.\\n\\n    .. versionadded:: 1.6.0\\n\\n    Examples\\n    --------\\n    >>> a = range(20)\\n    >>> b = [2 * x for x in range(20)]\\n    >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\\n    >>> df.agg(corr(\"a\", \"b\").alias(\\'c\\')).collect()\\n    [Row(c=1.0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.corr(_to_java_column(col1), _to_java_column(col2)))\\n\\n\\ndef covar_pop(col1, col2):\\n    \"\"\"Returns a new :class:`~pyspark.sql.Column` for the population covariance of ``col1`` and\\n    ``col2``.\\n\\n    .. versionadded:: 2.0.0\\n\\n    Examples\\n    --------\\n    >>> a = [1] * 10\\n    >>> b = [1] * 10\\n    >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\\n    >>> df.agg(covar_pop(\"a\", \"b\").alias(\\'c\\')).collect()\\n    [Row(c=0.0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.covar_pop(_to_java_column(col1), _to_java_column(col2)))\\n\\n\\ndef covar_samp(col1, col2):\\n    \"\"\"Returns a new :class:`~pyspark.sql.Column` for the sample covariance of ``col1`` and\\n    ``col2``.\\n\\n    .. versionadded:: 2.0.0\\n\\n    Examples\\n    --------\\n    >>> a = [1] * 10\\n    >>> b = [1] * 10\\n    >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\\n    >>> df.agg(covar_samp(\"a\", \"b\").alias(\\'c\\')).collect()\\n    [Row(c=0.0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.covar_samp(_to_java_column(col1), _to_java_column(col2)))\\n\\n\\ndef countDistinct(col, *cols):\\n    \"\"\"Returns a new :class:`~pyspark.sql.Column` for distinct count of ``col`` or ``cols``.\\n\\n    An alias of :func:`count_distinct`, and it is encouraged to use :func:`count_distinct`\\n    directly.\\n\\n    .. versionadded:: 1.3.0\\n    \"\"\"\\n    return count_distinct(col, *cols)\\n\\n\\ndef count_distinct(col, *cols):\\n    \"\"\"Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\\n\\n    .. versionadded:: 3.2.0\\n\\n    Examples\\n    --------\\n    >>> df.agg(count_distinct(df.age, df.name).alias(\\'c\\')).collect()\\n    [Row(c=2)]\\n\\n    >>> df.agg(count_distinct(\"age\", \"name\").alias(\\'c\\')).collect()\\n    [Row(c=2)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.count_distinct(_to_java_column(col), _to_seq(sc, cols, _to_java_column))\\n    return Column(jc)\\n\\n\\ndef first(col, ignorenulls=False):\\n    \"\"\"Aggregate function: returns the first value in a group.\\n\\n    The function by default returns the first values it sees. It will return the first non-null\\n    value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\\n\\n    .. versionadded:: 1.3.0\\n\\n    Notes\\n    -----\\n    The function is non-deterministic because its results depends on the order of the\\n    rows which may be non-deterministic after a shuffle.\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.first(_to_java_column(col), ignorenulls)\\n    return Column(jc)\\n\\n\\ndef grouping(col):\\n    \"\"\"\\n    Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\\n    or not, returns 1 for aggregated or 0 for not aggregated in the result set.\\n\\n    .. versionadded:: 2.0.0\\n\\n    Examples\\n    --------\\n    >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\\n    +-----+--------------+--------+\\n    | name|grouping(name)|sum(age)|\\n    +-----+--------------+--------+\\n    | null|             1|       7|\\n    |Alice|             0|       2|\\n    |  Bob|             0|       5|\\n    +-----+--------------+--------+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.grouping(_to_java_column(col))\\n    return Column(jc)\\n\\n\\ndef grouping_id(*cols):\\n    \"\"\"\\n    Aggregate function: returns the level of grouping, equals to\\n\\n       (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\\n\\n    .. versionadded:: 2.0.0\\n\\n    Notes\\n    -----\\n    The list of columns should match with grouping columns exactly, or empty (means all\\n    the grouping columns).\\n\\n    Examples\\n    --------\\n    >>> df.cube(\"name\").agg(grouping_id(), sum(\"age\")).orderBy(\"name\").show()\\n    +-----+-------------+--------+\\n    | name|grouping_id()|sum(age)|\\n    +-----+-------------+--------+\\n    | null|            1|       7|\\n    |Alice|            0|       2|\\n    |  Bob|            0|       5|\\n    +-----+-------------+--------+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.grouping_id(_to_seq(sc, cols, _to_java_column))\\n    return Column(jc)\\n\\n\\n@since(1.6)\\ndef input_file_name():\\n    \"\"\"Creates a string column for the file name of the current Spark task.\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.input_file_name())\\n\\n\\ndef isnan(col):\\n    \"\"\"An expression that returns true iff the column is NaN.\\n\\n    .. versionadded:: 1.6.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1.0, float(\\'nan\\')), (float(\\'nan\\'), 2.0)], (\"a\", \"b\"))\\n    >>> df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()\\n    [Row(r1=False, r2=False), Row(r1=True, r2=True)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.isnan(_to_java_column(col)))\\n\\n\\ndef isnull(col):\\n    \"\"\"An expression that returns true iff the column is null.\\n\\n    .. versionadded:: 1.6.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\\n    >>> df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()\\n    [Row(r1=False, r2=False), Row(r1=True, r2=True)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.isnull(_to_java_column(col)))\\n\\n\\ndef last(col, ignorenulls=False):\\n    \"\"\"Aggregate function: returns the last value in a group.\\n\\n    The function by default returns the last values it sees. It will return the last non-null\\n    value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\\n\\n    .. versionadded:: 1.3.0\\n\\n    Notes\\n    -----\\n    The function is non-deterministic because its results depends on the order of the\\n    rows which may be non-deterministic after a shuffle.\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.last(_to_java_column(col), ignorenulls)\\n    return Column(jc)\\n\\n\\ndef monotonically_increasing_id():\\n    \"\"\"A column that generates monotonically increasing 64-bit integers.\\n\\n    The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\\n    The current implementation puts the partition ID in the upper 31 bits, and the record number\\n    within each partition in the lower 33 bits. The assumption is that the data frame has\\n    less than 1 billion partitions, and each partition has less than 8 billion records.\\n\\n    .. versionadded:: 1.6.0\\n\\n    Notes\\n    -----\\n    The function is non-deterministic because its result depends on partition IDs.\\n\\n    As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\\n    This expression would return the following IDs:\\n    0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\\n\\n    >>> df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF([\\'col1\\'])\\n    >>> df0.select(monotonically_increasing_id().alias(\\'id\\')).collect()\\n    [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.monotonically_increasing_id())\\n\\n\\ndef nanvl(col1, col2):\\n    \"\"\"Returns col1 if it is not NaN, or col2 if col1 is NaN.\\n\\n    Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\\n\\n    .. versionadded:: 1.6.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1.0, float(\\'nan\\')), (float(\\'nan\\'), 2.0)], (\"a\", \"b\"))\\n    >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\\n    [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.nanvl(_to_java_column(col1), _to_java_column(col2)))\\n\\n\\ndef percentile_approx(col, percentage, accuracy=10000):\\n    \"\"\"Returns the approximate `percentile` of the numeric column `col` which is the smallest value\\n    in the ordered `col` values (sorted from least to greatest) such that no more than `percentage`\\n    of `col` values is less than the value or equal to that value.\\n    The value of percentage must be between 0.0 and 1.0.\\n\\n    The accuracy parameter (default: 10000)\\n    is a positive numeric literal which controls approximation accuracy at the cost of memory.\\n    Higher value of accuracy yields better accuracy, 1.0/accuracy is the relative error\\n    of the approximation.\\n\\n    When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\\n    In this case, returns the approximate percentile array of column col\\n    at the given percentage array.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Examples\\n    --------\\n    >>> key = (col(\"id\") % 3).alias(\"key\")\\n    >>> value = (randn(42) + key * 10).alias(\"value\")\\n    >>> df = spark.range(0, 1000, 1, 1).select(key, value)\\n    >>> df.select(\\n    ...     percentile_approx(\"value\", [0.25, 0.5, 0.75], 1000000).alias(\"quantiles\")\\n    ... ).printSchema()\\n    root\\n     |-- quantiles: array (nullable = true)\\n     |    |-- element: double (containsNull = false)\\n\\n    >>> df.groupBy(\"key\").agg(\\n    ...     percentile_approx(\"value\", 0.5, lit(1000000)).alias(\"median\")\\n    ... ).printSchema()\\n    root\\n     |-- key: long (nullable = true)\\n     |-- median: double (nullable = true)\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n\\n    if isinstance(percentage, (list, tuple)):\\n        # A local list\\n        percentage = sc._jvm.functions.array(_to_seq(sc, [\\n            _create_column_from_literal(x) for x in percentage\\n        ]))\\n    elif isinstance(percentage, Column):\\n        # Already a Column\\n        percentage = _to_java_column(percentage)\\n    else:\\n        # Probably scalar\\n        percentage = _create_column_from_literal(percentage)\\n\\n    accuracy = (\\n        _to_java_column(accuracy) if isinstance(accuracy, Column)\\n        else _create_column_from_literal(accuracy)\\n    )\\n\\n    return Column(sc._jvm.functions.percentile_approx(_to_java_column(col), percentage, accuracy))\\n\\n\\ndef rand(seed=None):\\n    \"\"\"Generates a random column with independent and identically distributed (i.i.d.) samples\\n    uniformly distributed in [0.0, 1.0).\\n\\n    .. versionadded:: 1.4.0\\n\\n    Notes\\n    -----\\n    The function is non-deterministic in general case.\\n\\n    Examples\\n    --------\\n    >>> df.withColumn(\\'rand\\', rand(seed=42) * 3).collect()\\n    [Row(age=2, name=\\'Alice\\', rand=2.4052597283576684),\\n     Row(age=5, name=\\'Bob\\', rand=2.3913904055683974)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if seed is not None:\\n        jc = sc._jvm.functions.rand(seed)\\n    else:\\n        jc = sc._jvm.functions.rand()\\n    return Column(jc)\\n\\n\\ndef randn(seed=None):\\n    \"\"\"Generates a column with independent and identically distributed (i.i.d.) samples from\\n    the standard normal distribution.\\n\\n    .. versionadded:: 1.4.0\\n\\n    Notes\\n    -----\\n    The function is non-deterministic in general case.\\n\\n    Examples\\n    --------\\n    >>> df.withColumn(\\'randn\\', randn(seed=42)).collect()\\n    [Row(age=2, name=\\'Alice\\', randn=1.1027054481455365),\\n    Row(age=5, name=\\'Bob\\', randn=0.7400395449950132)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if seed is not None:\\n        jc = sc._jvm.functions.randn(seed)\\n    else:\\n        jc = sc._jvm.functions.randn()\\n    return Column(jc)\\n\\n\\ndef round(col, scale=0):\\n    \"\"\"\\n    Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\\n    or at integral part when `scale` < 0.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(2.5,)], [\\'a\\']).select(round(\\'a\\', 0).alias(\\'r\\')).collect()\\n    [Row(r=3.0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.round(_to_java_column(col), scale))\\n\\n\\ndef bround(col, scale=0):\\n    \"\"\"\\n    Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\\n    or at integral part when `scale` < 0.\\n\\n    .. versionadded:: 2.0.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(2.5,)], [\\'a\\']).select(bround(\\'a\\', 0).alias(\\'r\\')).collect()\\n    [Row(r=2.0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.bround(_to_java_column(col), scale))\\n\\n\\ndef shiftLeft(col, numBits):\\n    \"\"\"Shift the given value numBits left.\\n\\n    .. versionadded:: 1.5.0\\n\\n    .. deprecated:: 3.2.0\\n        Use :func:`shiftleft` instead.\\n    \"\"\"\\n    warnings.warn(\"Deprecated in 3.2, use shiftleft instead.\", FutureWarning)\\n    return shiftleft(col, numBits)\\n\\n\\ndef shiftleft(col, numBits):\\n    \"\"\"Shift the given value numBits left.\\n\\n    .. versionadded:: 3.2.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(21,)], [\\'a\\']).select(shiftleft(\\'a\\', 1).alias(\\'r\\')).collect()\\n    [Row(r=42)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.shiftleft(_to_java_column(col), numBits))\\n\\n\\ndef shiftRight(col, numBits):\\n    \"\"\"(Signed) shift the given value numBits right.\\n\\n    .. versionadded:: 1.5.0\\n\\n    .. deprecated:: 3.2.0\\n        Use :func:`shiftright` instead.\\n    \"\"\"\\n    warnings.warn(\"Deprecated in 3.2, use shiftright instead.\", FutureWarning)\\n    return shiftright(col, numBits)\\n\\n\\ndef shiftright(col, numBits):\\n    \"\"\"(Signed) shift the given value numBits right.\\n\\n    .. versionadded:: 3.2.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(42,)], [\\'a\\']).select(shiftright(\\'a\\', 1).alias(\\'r\\')).collect()\\n    [Row(r=21)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.shiftRight(_to_java_column(col), numBits)\\n    return Column(jc)\\n\\n\\ndef shiftRightUnsigned(col, numBits):\\n    \"\"\"Unsigned shift the given value numBits right.\\n\\n    .. versionadded:: 1.5.0\\n\\n    .. deprecated:: 3.2.0\\n        Use :func:`shiftrightunsigned` instead.\\n    \"\"\"\\n    warnings.warn(\"Deprecated in 3.2, use shiftrightunsigned instead.\", FutureWarning)\\n    return shiftrightunsigned(col, numBits)\\n\\n\\ndef shiftrightunsigned(col, numBits):\\n    \"\"\"Unsigned shift the given value numBits right.\\n\\n    .. versionadded:: 3.2.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(-42,)], [\\'a\\'])\\n    >>> df.select(shiftrightunsigned(\\'a\\', 1).alias(\\'r\\')).collect()\\n    [Row(r=9223372036854775787)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.shiftRightUnsigned(_to_java_column(col), numBits)\\n    return Column(jc)\\n\\n\\ndef spark_partition_id():\\n    \"\"\"A column for partition ID.\\n\\n    .. versionadded:: 1.6.0\\n\\n    Notes\\n    -----\\n    This is non deterministic because it depends on data partitioning and task scheduling.\\n\\n    Examples\\n    --------\\n    >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\\n    [Row(pid=0), Row(pid=0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.spark_partition_id())\\n\\n\\ndef expr(str):\\n    \"\"\"Parses the expression string into the column that it represents\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df.select(expr(\"length(name)\")).collect()\\n    [Row(length(name)=5), Row(length(name)=3)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.expr(str))\\n\\n\\ndef struct(*cols):\\n    \"\"\"Creates a new struct column.\\n\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    cols : list, set, str or :class:`~pyspark.sql.Column`\\n        column names or :class:`~pyspark.sql.Column`\\\\\\\\s to contain in the output struct.\\n\\n    Examples\\n    --------\\n    >>> df.select(struct(\\'age\\', \\'name\\').alias(\"struct\")).collect()\\n    [Row(struct=Row(age=2, name=\\'Alice\\')), Row(struct=Row(age=5, name=\\'Bob\\'))]\\n    >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\\n    [Row(struct=Row(age=2, name=\\'Alice\\')), Row(struct=Row(age=5, name=\\'Bob\\'))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if len(cols) == 1 and isinstance(cols[0], (list, set)):\\n        cols = cols[0]\\n    jc = sc._jvm.functions.struct(_to_seq(sc, cols, _to_java_column))\\n    return Column(jc)\\n\\n\\ndef greatest(*cols):\\n    \"\"\"\\n    Returns the greatest value of the list of column names, skipping null values.\\n    This function takes at least 2 parameters. It will return null iff all parameters are null.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, 4, 3)], [\\'a\\', \\'b\\', \\'c\\'])\\n    >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\\n    [Row(greatest=4)]\\n    \"\"\"\\n    if len(cols) < 2:\\n        raise ValueError(\"greatest should take at least two columns\")\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.greatest(_to_seq(sc, cols, _to_java_column)))\\n\\n\\ndef least(*cols):\\n    \"\"\"\\n    Returns the least value of the list of column names, skipping null values.\\n    This function takes at least 2 parameters. It will return null iff all parameters are null.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, 4, 3)], [\\'a\\', \\'b\\', \\'c\\'])\\n    >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\\n    [Row(least=1)]\\n    \"\"\"\\n    if len(cols) < 2:\\n        raise ValueError(\"least should take at least two columns\")\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.least(_to_seq(sc, cols, _to_java_column)))\\n\\n\\ndef when(condition, value):\\n    \"\"\"Evaluates a list of conditions and returns one of multiple possible result expressions.\\n    If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\\n    conditions.\\n\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    condition : :class:`~pyspark.sql.Column`\\n        a boolean :class:`~pyspark.sql.Column` expression.\\n    value :\\n        a literal value, or a :class:`~pyspark.sql.Column` expression.\\n\\n    >>> df.select(when(df[\\'age\\'] == 2, 3).otherwise(4).alias(\"age\")).collect()\\n    [Row(age=3), Row(age=4)]\\n\\n    >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\\n    [Row(age=3), Row(age=None)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if not isinstance(condition, Column):\\n        raise TypeError(\"condition should be a Column\")\\n    v = value._jc if isinstance(value, Column) else value\\n    jc = sc._jvm.functions.when(condition._jc, v)\\n    return Column(jc)\\n\\n\\ndef log(arg1, arg2=None):\\n    \"\"\"Returns the first argument-based logarithm of the second argument.\\n\\n    If there is only one argument, then this takes the natural logarithm of the argument.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df.select(log(10.0, df.age).alias(\\'ten\\')).rdd.map(lambda l: str(l.ten)[:7]).collect()\\n    [\\'0.30102\\', \\'0.69897\\']\\n\\n    >>> df.select(log(df.age).alias(\\'e\\')).rdd.map(lambda l: str(l.e)[:7]).collect()\\n    [\\'0.69314\\', \\'1.60943\\']\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if arg2 is None:\\n        jc = sc._jvm.functions.log(_to_java_column(arg1))\\n    else:\\n        jc = sc._jvm.functions.log(arg1, _to_java_column(arg2))\\n    return Column(jc)\\n\\n\\ndef log2(col):\\n    \"\"\"Returns the base-2 logarithm of the argument.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(4,)], [\\'a\\']).select(log2(\\'a\\').alias(\\'log2\\')).collect()\\n    [Row(log2=2.0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.log2(_to_java_column(col)))\\n\\n\\ndef conv(col, fromBase, toBase):\\n    \"\"\"\\n    Convert a number in a string column from one base to another.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\"010101\",)], [\\'n\\'])\\n    >>> df.select(conv(df.n, 2, 16).alias(\\'hex\\')).collect()\\n    [Row(hex=\\'15\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.conv(_to_java_column(col), fromBase, toBase))\\n\\n\\ndef factorial(col):\\n    \"\"\"\\n    Computes the factorial of the given value.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(5,)], [\\'n\\'])\\n    >>> df.select(factorial(df.n).alias(\\'f\\')).collect()\\n    [Row(f=120)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.factorial(_to_java_column(col)))\\n\\n\\n# ---------------  Window functions ------------------------\\n\\ndef lag(col, offset=1, default=None):\\n    \"\"\"\\n    Window function: returns the value that is `offset` rows before the current row, and\\n    `default` if there is less than `offset` rows before the current row. For example,\\n    an `offset` of one will return the previous row at any given point in the window partition.\\n\\n    This is equivalent to the LAG function in SQL.\\n\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    offset : int, optional\\n        number of row to extend\\n    default : optional\\n        default value\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.lag(_to_java_column(col), offset, default))\\n\\n\\ndef lead(col, offset=1, default=None):\\n    \"\"\"\\n    Window function: returns the value that is `offset` rows after the current row, and\\n    `default` if there is less than `offset` rows after the current row. For example,\\n    an `offset` of one will return the next row at any given point in the window partition.\\n\\n    This is equivalent to the LEAD function in SQL.\\n\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    offset : int, optional\\n        number of row to extend\\n    default : optional\\n        default value\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.lead(_to_java_column(col), offset, default))\\n\\n\\ndef nth_value(col, offset, ignoreNulls=False):\\n    \"\"\"\\n    Window function: returns the value that is the `offset`\\\\\\\\th row of the window frame\\n    (counting from 1), and `null` if the size of window frame is less than `offset` rows.\\n\\n    It will return the `offset`\\\\\\\\th non-null value it sees when `ignoreNulls` is set to\\n    true. If all values are null, then null is returned.\\n\\n    This is equivalent to the nth_value function in SQL.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    offset : int, optional\\n        number of row to use as the value\\n    ignoreNulls : bool, optional\\n        indicates the Nth value should skip null in the\\n        determination of which row to use\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.nth_value(_to_java_column(col), offset, ignoreNulls))\\n\\n\\ndef ntile(n):\\n    \"\"\"\\n    Window function: returns the ntile group id (from 1 to `n` inclusive)\\n    in an ordered window partition. For example, if `n` is 4, the first\\n    quarter of the rows will get value 1, the second quarter will get 2,\\n    the third quarter will get 3, and the last quarter will get 4.\\n\\n    This is equivalent to the NTILE function in SQL.\\n\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    n : int\\n        an integer\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.ntile(int(n)))\\n\\n\\n# ---------------------- Date/Timestamp functions ------------------------------\\n\\n@since(1.5)\\ndef current_date():\\n    \"\"\"\\n    Returns the current date at the start of query evaluation as a :class:`DateType` column.\\n    All calls of current_date within the same query return the same value.\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.current_date())\\n\\n\\ndef current_timestamp():\\n    \"\"\"\\n    Returns the current timestamp at the start of query evaluation as a :class:`TimestampType`\\n    column. All calls of current_timestamp within the same query return the same value.\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.current_timestamp())\\n\\n\\ndef date_format(date, format):\\n    \"\"\"\\n    Converts a date/timestamp/string to a value of string in the format specified by the date\\n    format given by the second argument.\\n\\n    A pattern could be for instance `dd.MM.yyyy` and could return a string like \\'18.03.1993\\'. All\\n    pattern letters of `datetime pattern`_. can be used.\\n\\n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\\n\\n    .. versionadded:: 1.5.0\\n\\n    Notes\\n    -----\\n    Whenever possible, use specialized functions like `year`.\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(date_format(\\'dt\\', \\'MM/dd/yyy\\').alias(\\'date\\')).collect()\\n    [Row(date=\\'04/08/2015\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.date_format(_to_java_column(date), format))\\n\\n\\ndef year(col):\\n    \"\"\"\\n    Extract the year of a given date as integer.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(year(\\'dt\\').alias(\\'year\\')).collect()\\n    [Row(year=2015)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.year(_to_java_column(col)))\\n\\n\\ndef quarter(col):\\n    \"\"\"\\n    Extract the quarter of a given date as integer.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(quarter(\\'dt\\').alias(\\'quarter\\')).collect()\\n    [Row(quarter=2)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.quarter(_to_java_column(col)))\\n\\n\\ndef month(col):\\n    \"\"\"\\n    Extract the month of a given date as integer.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(month(\\'dt\\').alias(\\'month\\')).collect()\\n    [Row(month=4)]\\n   \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.month(_to_java_column(col)))\\n\\n\\ndef dayofweek(col):\\n    \"\"\"\\n    Extract the day of the week of a given date as integer.\\n\\n    .. versionadded:: 2.3.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(dayofweek(\\'dt\\').alias(\\'day\\')).collect()\\n    [Row(day=4)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.dayofweek(_to_java_column(col)))\\n\\n\\ndef dayofmonth(col):\\n    \"\"\"\\n    Extract the day of the month of a given date as integer.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(dayofmonth(\\'dt\\').alias(\\'day\\')).collect()\\n    [Row(day=8)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.dayofmonth(_to_java_column(col)))\\n\\n\\ndef dayofyear(col):\\n    \"\"\"\\n    Extract the day of the year of a given date as integer.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(dayofyear(\\'dt\\').alias(\\'day\\')).collect()\\n    [Row(day=98)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.dayofyear(_to_java_column(col)))\\n\\n\\ndef hour(col):\\n    \"\"\"\\n    Extract the hours of a given date as integer.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08 13:08:15\\',)], [\\'ts\\'])\\n    >>> df.select(hour(\\'ts\\').alias(\\'hour\\')).collect()\\n    [Row(hour=13)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.hour(_to_java_column(col)))\\n\\n\\ndef minute(col):\\n    \"\"\"\\n    Extract the minutes of a given date as integer.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08 13:08:15\\',)], [\\'ts\\'])\\n    >>> df.select(minute(\\'ts\\').alias(\\'minute\\')).collect()\\n    [Row(minute=8)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.minute(_to_java_column(col)))\\n\\n\\ndef second(col):\\n    \"\"\"\\n    Extract the seconds of a given date as integer.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08 13:08:15\\',)], [\\'ts\\'])\\n    >>> df.select(second(\\'ts\\').alias(\\'second\\')).collect()\\n    [Row(second=15)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.second(_to_java_column(col)))\\n\\n\\ndef weekofyear(col):\\n    \"\"\"\\n    Extract the week number of a given date as integer.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(weekofyear(df.dt).alias(\\'week\\')).collect()\\n    [Row(week=15)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.weekofyear(_to_java_column(col)))\\n\\n\\ndef date_add(start, days):\\n    \"\"\"\\n    Returns the date that is `days` days after `start`\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(date_add(df.dt, 1).alias(\\'next_date\\')).collect()\\n    [Row(next_date=datetime.date(2015, 4, 9))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.date_add(_to_java_column(start), days))\\n\\n\\ndef date_sub(start, days):\\n    \"\"\"\\n    Returns the date that is `days` days before `start`\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(date_sub(df.dt, 1).alias(\\'prev_date\\')).collect()\\n    [Row(prev_date=datetime.date(2015, 4, 7))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.date_sub(_to_java_column(start), days))\\n\\n\\ndef datediff(end, start):\\n    \"\"\"\\n    Returns the number of days from `start` to `end`.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',\\'2015-05-10\\')], [\\'d1\\', \\'d2\\'])\\n    >>> df.select(datediff(df.d2, df.d1).alias(\\'diff\\')).collect()\\n    [Row(diff=32)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.datediff(_to_java_column(end), _to_java_column(start)))\\n\\n\\ndef add_months(start, months):\\n    \"\"\"\\n    Returns the date that is `months` months after `start`\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> df.select(add_months(df.dt, 1).alias(\\'next_month\\')).collect()\\n    [Row(next_month=datetime.date(2015, 5, 8))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.add_months(_to_java_column(start), months))\\n\\n\\ndef months_between(date1, date2, roundOff=True):\\n    \"\"\"\\n    Returns number of months between dates date1 and date2.\\n    If date1 is later than date2, then the result is positive.\\n    If date1 and date2 are on the same day of month, or both are the last day of month,\\n    returns an integer (time of day will be ignored).\\n    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\', \\'1996-10-30\\')], [\\'date1\\', \\'date2\\'])\\n    >>> df.select(months_between(df.date1, df.date2).alias(\\'months\\')).collect()\\n    [Row(months=3.94959677)]\\n    >>> df.select(months_between(df.date1, df.date2, False).alias(\\'months\\')).collect()\\n    [Row(months=3.9495967741935485)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.months_between(\\n        _to_java_column(date1), _to_java_column(date2), roundOff))\\n\\n\\ndef to_date(col, format=None):\\n    \"\"\"Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\\n    using the optionally specified format. Specify formats according to `datetime pattern`_.\\n    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\\n    is omitted. Equivalent to ``col.cast(\"date\")``.\\n\\n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\\n\\n    .. versionadded:: 2.2.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\',)], [\\'t\\'])\\n    >>> df.select(to_date(df.t).alias(\\'date\\')).collect()\\n    [Row(date=datetime.date(1997, 2, 28))]\\n\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\',)], [\\'t\\'])\\n    >>> df.select(to_date(df.t, \\'yyyy-MM-dd HH:mm:ss\\').alias(\\'date\\')).collect()\\n    [Row(date=datetime.date(1997, 2, 28))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if format is None:\\n        jc = sc._jvm.functions.to_date(_to_java_column(col))\\n    else:\\n        jc = sc._jvm.functions.to_date(_to_java_column(col), format)\\n    return Column(jc)\\n\\n\\ndef to_timestamp(col, format=None):\\n    \"\"\"Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\\n    using the optionally specified format. Specify formats according to `datetime pattern`_.\\n    By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\\n    is omitted. Equivalent to ``col.cast(\"timestamp\")``.\\n\\n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\\n\\n    .. versionadded:: 2.2.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\',)], [\\'t\\'])\\n    >>> df.select(to_timestamp(df.t).alias(\\'dt\\')).collect()\\n    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\\n\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\',)], [\\'t\\'])\\n    >>> df.select(to_timestamp(df.t, \\'yyyy-MM-dd HH:mm:ss\\').alias(\\'dt\\')).collect()\\n    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if format is None:\\n        jc = sc._jvm.functions.to_timestamp(_to_java_column(col))\\n    else:\\n        jc = sc._jvm.functions.to_timestamp(_to_java_column(col), format)\\n    return Column(jc)\\n\\n\\ndef trunc(date, format):\\n    \"\"\"\\n    Returns date truncated to the unit specified by the format.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    date : :class:`~pyspark.sql.Column` or str\\n    format : str\\n        \\'year\\', \\'yyyy\\', \\'yy\\' or \\'month\\', \\'mon\\', \\'mm\\'\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'1997-02-28\\',)], [\\'d\\'])\\n    >>> df.select(trunc(df.d, \\'year\\').alias(\\'year\\')).collect()\\n    [Row(year=datetime.date(1997, 1, 1))]\\n    >>> df.select(trunc(df.d, \\'mon\\').alias(\\'month\\')).collect()\\n    [Row(month=datetime.date(1997, 2, 1))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.trunc(_to_java_column(date), format))\\n\\n\\ndef date_trunc(format, timestamp):\\n    \"\"\"\\n    Returns timestamp truncated to the unit specified by the format.\\n\\n    .. versionadded:: 2.3.0\\n\\n    Parameters\\n    ----------\\n    format : str\\n        \\'year\\', \\'yyyy\\', \\'yy\\', \\'month\\', \\'mon\\', \\'mm\\',\\n        \\'day\\', \\'dd\\', \\'hour\\', \\'minute\\', \\'second\\', \\'week\\', \\'quarter\\'\\n    timestamp : :class:`~pyspark.sql.Column` or str\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 05:02:11\\',)], [\\'t\\'])\\n    >>> df.select(date_trunc(\\'year\\', df.t).alias(\\'year\\')).collect()\\n    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\\n    >>> df.select(date_trunc(\\'mon\\', df.t).alias(\\'month\\')).collect()\\n    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.date_trunc(format, _to_java_column(timestamp)))\\n\\n\\ndef next_day(date, dayOfWeek):\\n    \"\"\"\\n    Returns the first date which is later than the value of the date column.\\n\\n    Day of the week parameter is case insensitive, and accepts:\\n        \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'2015-07-27\\',)], [\\'d\\'])\\n    >>> df.select(next_day(df.d, \\'Sun\\').alias(\\'date\\')).collect()\\n    [Row(date=datetime.date(2015, 8, 2))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.next_day(_to_java_column(date), dayOfWeek))\\n\\n\\ndef last_day(date):\\n    \"\"\"\\n    Returns the last day of the month which the given date belongs to.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'1997-02-10\\',)], [\\'d\\'])\\n    >>> df.select(last_day(df.d).alias(\\'date\\')).collect()\\n    [Row(date=datetime.date(1997, 2, 28))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.last_day(_to_java_column(date)))\\n\\n\\ndef from_unixtime(timestamp, format=\"yyyy-MM-dd HH:mm:ss\"):\\n    \"\"\"\\n    Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\\n    representing the timestamp of that moment in the current system time zone in the given\\n    format.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\\n    >>> time_df = spark.createDataFrame([(1428476400,)], [\\'unix_time\\'])\\n    >>> time_df.select(from_unixtime(\\'unix_time\\').alias(\\'ts\\')).collect()\\n    [Row(ts=\\'2015-04-08 00:00:00\\')]\\n    >>> spark.conf.unset(\"spark.sql.session.timeZone\")\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.from_unixtime(_to_java_column(timestamp), format))\\n\\n\\ndef unix_timestamp(timestamp=None, format=\\'yyyy-MM-dd HH:mm:ss\\'):\\n    \"\"\"\\n    Convert time string with given pattern (\\'yyyy-MM-dd HH:mm:ss\\', by default)\\n    to Unix time stamp (in seconds), using the default timezone and the default\\n    locale, return null if fail.\\n\\n    if `timestamp` is None, then it returns current timestamp.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\\n    >>> time_df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> time_df.select(unix_timestamp(\\'dt\\', \\'yyyy-MM-dd\\').alias(\\'unix_time\\')).collect()\\n    [Row(unix_time=1428476400)]\\n    >>> spark.conf.unset(\"spark.sql.session.timeZone\")\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if timestamp is None:\\n        return Column(sc._jvm.functions.unix_timestamp())\\n    return Column(sc._jvm.functions.unix_timestamp(_to_java_column(timestamp), format))\\n\\n\\ndef from_utc_timestamp(timestamp, tz):\\n    \"\"\"\\n    This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\\n    takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\\n    renders that timestamp as a timestamp in the given time zone.\\n\\n    However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\\n    timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\\n    the given timezone.\\n\\n    This function may return confusing result if the input is a string with timezone, e.g.\\n    \\'2018-03-13T06:18:23+00:00\\'. The reason is that, Spark firstly cast the string to timestamp\\n    according to the timezone in the string, and finally display the result by converting the\\n    timestamp to string according to the session local timezone.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    timestamp : :class:`~pyspark.sql.Column` or str\\n        the column that contains timestamps\\n    tz : :class:`~pyspark.sql.Column` or str\\n        A string detailing the time zone ID that the input should be adjusted to. It should\\n        be in the format of either region-based zone IDs or zone offsets. Region IDs must\\n        have the form \\'area/city\\', such as \\'America/Los_Angeles\\'. Zone offsets must be in\\n        the format \\'(+|-)HH:mm\\', for example \\'-08:00\\' or \\'+01:00\\'. Also \\'UTC\\' and \\'Z\\' are\\n        supported as aliases of \\'+00:00\\'. Other short names are not recommended to use\\n        because they can be ambiguous.\\n\\n        .. versionchanged:: 2.4\\n           `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\', \\'JST\\')], [\\'ts\\', \\'tz\\'])\\n    >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias(\\'local_time\\')).collect()\\n    [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\\n    >>> df.select(from_utc_timestamp(df.ts, df.tz).alias(\\'local_time\\')).collect()\\n    [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if isinstance(tz, Column):\\n        tz = _to_java_column(tz)\\n    return Column(sc._jvm.functions.from_utc_timestamp(_to_java_column(timestamp), tz))\\n\\n\\ndef to_utc_timestamp(timestamp, tz):\\n    \"\"\"\\n    This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\\n    takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\\n    timezone, and renders that timestamp as a timestamp in UTC.\\n\\n    However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\\n    timezone-agnostic. So in Spark this function just shift the timestamp value from the given\\n    timezone to UTC timezone.\\n\\n    This function may return confusing result if the input is a string with timezone, e.g.\\n    \\'2018-03-13T06:18:23+00:00\\'. The reason is that, Spark firstly cast the string to timestamp\\n    according to the timezone in the string, and finally display the result by converting the\\n    timestamp to string according to the session local timezone.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    timestamp : :class:`~pyspark.sql.Column` or str\\n        the column that contains timestamps\\n    tz : :class:`~pyspark.sql.Column` or str\\n        A string detailing the time zone ID that the input should be adjusted to. It should\\n        be in the format of either region-based zone IDs or zone offsets. Region IDs must\\n        have the form \\'area/city\\', such as \\'America/Los_Angeles\\'. Zone offsets must be in\\n        the format \\'(+|-)HH:mm\\', for example \\'-08:00\\' or \\'+01:00\\'. Also \\'UTC\\' and \\'Z\\' are\\n        upported as aliases of \\'+00:00\\'. Other short names are not recommended to use\\n        because they can be ambiguous.\\n\\n        .. versionchanged:: 2.4.0\\n           `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\', \\'JST\\')], [\\'ts\\', \\'tz\\'])\\n    >>> df.select(to_utc_timestamp(df.ts, \"PST\").alias(\\'utc_time\\')).collect()\\n    [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\\n    >>> df.select(to_utc_timestamp(df.ts, df.tz).alias(\\'utc_time\\')).collect()\\n    [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if isinstance(tz, Column):\\n        tz = _to_java_column(tz)\\n    return Column(sc._jvm.functions.to_utc_timestamp(_to_java_column(timestamp), tz))\\n\\n\\ndef timestamp_seconds(col):\\n    \"\"\"\\n    .. versionadded:: 3.1.0\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql.functions import timestamp_seconds\\n    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\\n    >>> time_df = spark.createDataFrame([(1230219000,)], [\\'unix_time\\'])\\n    >>> time_df.select(timestamp_seconds(time_df.unix_time).alias(\\'ts\\')).show()\\n    +-------------------+\\n    |                 ts|\\n    +-------------------+\\n    |2008-12-25 07:30:00|\\n    +-------------------+\\n    >>> spark.conf.unset(\"spark.sql.session.timeZone\")\\n    \"\"\"\\n\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.timestamp_seconds(_to_java_column(col)))\\n\\n\\ndef window(timeColumn, windowDuration, slideDuration=None, startTime=None):\\n    \"\"\"Bucketize rows into one or more time windows given a timestamp specifying column. Window\\n    starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\\n    [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\\n    the order of months are not supported.\\n\\n    The time column must be of :class:`pyspark.sql.types.TimestampType`.\\n\\n    Durations are provided as strings, e.g. \\'1 second\\', \\'1 day 12 hours\\', \\'2 minutes\\'. Valid\\n    interval strings are \\'week\\', \\'day\\', \\'hour\\', \\'minute\\', \\'second\\', \\'millisecond\\', \\'microsecond\\'.\\n    If the ``slideDuration`` is not provided, the windows will be tumbling windows.\\n\\n    The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\\n    window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\\n    past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\\n\\n    The output column will be a struct called \\'window\\' by default with the nested columns \\'start\\'\\n    and \\'end\\', where \\'start\\' and \\'end\\' will be of :class:`pyspark.sql.types.TimestampType`.\\n\\n    .. versionadded:: 2.0.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\\n    >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\\n    >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\\n    ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\\n    [Row(start=\\'2016-03-11 09:00:05\\', end=\\'2016-03-11 09:00:10\\', sum=1)]\\n    \"\"\"\\n    def check_string_field(field, fieldName):\\n        if not field or type(field) is not str:\\n            raise TypeError(\"%s should be provided as a string\" % fieldName)\\n\\n    sc = SparkContext._active_spark_context\\n    time_col = _to_java_column(timeColumn)\\n    check_string_field(windowDuration, \"windowDuration\")\\n    if slideDuration and startTime:\\n        check_string_field(slideDuration, \"slideDuration\")\\n        check_string_field(startTime, \"startTime\")\\n        res = sc._jvm.functions.window(time_col, windowDuration, slideDuration, startTime)\\n    elif slideDuration:\\n        check_string_field(slideDuration, \"slideDuration\")\\n        res = sc._jvm.functions.window(time_col, windowDuration, slideDuration)\\n    elif startTime:\\n        check_string_field(startTime, \"startTime\")\\n        res = sc._jvm.functions.window(time_col, windowDuration, windowDuration, startTime)\\n    else:\\n        res = sc._jvm.functions.window(time_col, windowDuration)\\n    return Column(res)\\n\\n\\n# ---------------------------- misc functions ----------------------------------\\n\\ndef crc32(col):\\n    \"\"\"\\n    Calculates the cyclic redundancy check value  (CRC32) of a binary column and\\n    returns the value as a bigint.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'ABC\\',)], [\\'a\\']).select(crc32(\\'a\\').alias(\\'crc32\\')).collect()\\n    [Row(crc32=2743272264)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.crc32(_to_java_column(col)))\\n\\n\\ndef md5(col):\\n    \"\"\"Calculates the MD5 digest and returns the value as a 32 character hex string.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'ABC\\',)], [\\'a\\']).select(md5(\\'a\\').alias(\\'hash\\')).collect()\\n    [Row(hash=\\'902fbdd2b1df0c4f70b4a5d23525e932\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.md5(_to_java_column(col))\\n    return Column(jc)\\n\\n\\ndef sha1(col):\\n    \"\"\"Returns the hex string result of SHA-1.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'ABC\\',)], [\\'a\\']).select(sha1(\\'a\\').alias(\\'hash\\')).collect()\\n    [Row(hash=\\'3c01bdbb26f358bab27f267924aa2c9a03fcfdb8\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.sha1(_to_java_column(col))\\n    return Column(jc)\\n\\n\\ndef sha2(col, numBits):\\n    \"\"\"Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\\n    and SHA-512). The numBits indicates the desired bit length of the result, which must have a\\n    value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> digests = df.select(sha2(df.name, 256).alias(\\'s\\')).collect()\\n    >>> digests[0]\\n    Row(s=\\'3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043\\')\\n    >>> digests[1]\\n    Row(s=\\'cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961\\')\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.sha2(_to_java_column(col), numBits)\\n    return Column(jc)\\n\\n\\ndef hash(*cols):\\n    \"\"\"Calculates the hash code of given columns, and returns the result as an int column.\\n\\n    .. versionadded:: 2.0.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'ABC\\',)], [\\'a\\']).select(hash(\\'a\\').alias(\\'hash\\')).collect()\\n    [Row(hash=-757602832)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.hash(_to_seq(sc, cols, _to_java_column))\\n    return Column(jc)\\n\\n\\ndef xxhash64(*cols):\\n    \"\"\"Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,\\n    and returns the result as a long column.\\n\\n    .. versionadded:: 3.0.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'ABC\\',)], [\\'a\\']).select(xxhash64(\\'a\\').alias(\\'hash\\')).collect()\\n    [Row(hash=4105715581806190027)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.xxhash64(_to_seq(sc, cols, _to_java_column))\\n    return Column(jc)\\n\\n\\ndef assert_true(col, errMsg=None):\\n    \"\"\"\\n    Returns null if the input column is true; throws an exception with the provided error message\\n    otherwise.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(0,1)], [\\'a\\', \\'b\\'])\\n    >>> df.select(assert_true(df.a < df.b).alias(\\'r\\')).collect()\\n    [Row(r=None)]\\n    >>> df = spark.createDataFrame([(0,1)], [\\'a\\', \\'b\\'])\\n    >>> df.select(assert_true(df.a < df.b, df.a).alias(\\'r\\')).collect()\\n    [Row(r=None)]\\n    >>> df = spark.createDataFrame([(0,1)], [\\'a\\', \\'b\\'])\\n    >>> df.select(assert_true(df.a < df.b, \\'error\\').alias(\\'r\\')).collect()\\n    [Row(r=None)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if errMsg is None:\\n        return Column(sc._jvm.functions.assert_true(_to_java_column(col)))\\n    if not isinstance(errMsg, (str, Column)):\\n        raise TypeError(\\n            \"errMsg should be a Column or a str, got {}\".format(type(errMsg))\\n        )\\n\\n    errMsg = (\\n        _create_column_from_literal(errMsg)\\n        if isinstance(errMsg, str)\\n        else _to_java_column(errMsg)\\n    )\\n    return Column(sc._jvm.functions.assert_true(_to_java_column(col), errMsg))\\n\\n\\n@since(3.1)\\ndef raise_error(errMsg):\\n    \"\"\"\\n    Throws an exception with the provided error message.\\n    \"\"\"\\n    if not isinstance(errMsg, (str, Column)):\\n        raise TypeError(\\n            \"errMsg should be a Column or a str, got {}\".format(type(errMsg))\\n        )\\n\\n    sc = SparkContext._active_spark_context\\n    errMsg = (\\n        _create_column_from_literal(errMsg)\\n        if isinstance(errMsg, str)\\n        else _to_java_column(errMsg)\\n    )\\n    return Column(sc._jvm.functions.raise_error(errMsg))\\n\\n\\n# ---------------------- String/Binary functions ------------------------------\\n\\n@since(1.5)\\ndef upper(col):\\n    \"\"\"\\n    Converts a string expression to upper case.\\n    \"\"\"\\n    return _invoke_function_over_column(\"upper\", col)\\n\\n\\n@since(1.5)\\ndef lower(col):\\n    \"\"\"\\n    Converts a string expression to lower case.\\n    \"\"\"\\n    return _invoke_function_over_column(\"lower\", col)\\n\\n\\n@since(1.5)\\ndef ascii(col):\\n    \"\"\"\\n    Computes the numeric value of the first character of the string column.\\n    \"\"\"\\n    return _invoke_function_over_column(\"ascii\", col)\\n\\n\\n@since(1.5)\\ndef base64(col):\\n    \"\"\"\\n    Computes the BASE64 encoding of a binary column and returns it as a string column.\\n    \"\"\"\\n    return _invoke_function_over_column(\"base64\", col)\\n\\n\\n@since(1.5)\\ndef unbase64(col):\\n    \"\"\"\\n    Decodes a BASE64 encoded string column and returns it as a binary column.\\n    \"\"\"\\n    return _invoke_function_over_column(\"unbase64\", col)\\n\\n\\n@since(1.5)\\ndef ltrim(col):\\n    \"\"\"\\n    Trim the spaces from left end for the specified string value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"ltrim\", col)\\n\\n\\n@since(1.5)\\ndef rtrim(col):\\n    \"\"\"\\n    Trim the spaces from right end for the specified string value.\\n    \"\"\"\\n    return _invoke_function_over_column(\"rtrim\", col)\\n\\n\\n@since(1.5)\\ndef trim(col):\\n    \"\"\"\\n    Trim the spaces from both ends for the specified string column.\\n    \"\"\"\\n    return _invoke_function_over_column(\"trim\", col)\\n\\n\\ndef concat_ws(sep, *cols):\\n    \"\"\"\\n    Concatenates multiple input string columns together into a single string column,\\n    using the given separator.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'abcd\\',\\'123\\')], [\\'s\\', \\'d\\'])\\n    >>> df.select(concat_ws(\\'-\\', df.s, df.d).alias(\\'s\\')).collect()\\n    [Row(s=\\'abcd-123\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.concat_ws(sep, _to_seq(sc, cols, _to_java_column)))\\n\\n\\n@since(1.5)\\ndef decode(col, charset):\\n    \"\"\"\\n    Computes the first argument into a string from a binary using the provided character set\\n    (one of \\'US-ASCII\\', \\'ISO-8859-1\\', \\'UTF-8\\', \\'UTF-16BE\\', \\'UTF-16LE\\', \\'UTF-16\\').\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.decode(_to_java_column(col), charset))\\n\\n\\n@since(1.5)\\ndef encode(col, charset):\\n    \"\"\"\\n    Computes the first argument into a binary from a string using the provided character set\\n    (one of \\'US-ASCII\\', \\'ISO-8859-1\\', \\'UTF-8\\', \\'UTF-16BE\\', \\'UTF-16LE\\', \\'UTF-16\\').\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.encode(_to_java_column(col), charset))\\n\\n\\ndef format_number(col, d):\\n    \"\"\"\\n    Formats the number X to a format like \\'#,--#,--#.--\\', rounded to d decimal places\\n    with HALF_EVEN round mode, and returns the result as a string.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        the column name of the numeric value to be formatted\\n    d : int\\n        the N decimal places\\n\\n    >>> spark.createDataFrame([(5,)], [\\'a\\']).select(format_number(\\'a\\', 4).alias(\\'v\\')).collect()\\n    [Row(v=\\'5.0000\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.format_number(_to_java_column(col), d))\\n\\n\\ndef format_string(format, *cols):\\n    \"\"\"\\n    Formats the arguments in printf-style and returns the result as a string column.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    format : str\\n        string that can contain embedded format tags and used as result column\\'s value\\n    cols : :class:`~pyspark.sql.Column` or str\\n        column names or :class:`~pyspark.sql.Column`\\\\\\\\s to be used in formatting\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(5, \"hello\")], [\\'a\\', \\'b\\'])\\n    >>> df.select(format_string(\\'%d %s\\', df.a, df.b).alias(\\'v\\')).collect()\\n    [Row(v=\\'5 hello\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.format_string(format, _to_seq(sc, cols, _to_java_column)))\\n\\n\\ndef instr(str, substr):\\n    \"\"\"\\n    Locate the position of the first occurrence of substr column in the given string.\\n    Returns null if either of the arguments are null.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Notes\\n    -----\\n    The position is not zero based, but 1 based index. Returns 0 if substr\\n    could not be found in str.\\n\\n    >>> df = spark.createDataFrame([(\\'abcd\\',)], [\\'s\\',])\\n    >>> df.select(instr(df.s, \\'b\\').alias(\\'s\\')).collect()\\n    [Row(s=2)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.instr(_to_java_column(str), substr))\\n\\n\\ndef overlay(src, replace, pos, len=-1):\\n    \"\"\"\\n    Overlay the specified portion of `src` with `replace`,\\n    starting from byte position `pos` of `src` and proceeding for `len` bytes.\\n\\n    .. versionadded:: 3.0.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\\n    >>> df.select(overlay(\"x\", \"y\", 7).alias(\"overlayed\")).show()\\n    +----------+\\n    | overlayed|\\n    +----------+\\n    |SPARK_CORE|\\n    +----------+\\n    \"\"\"\\n    if not isinstance(pos, (int, str, Column)):\\n        raise TypeError(\\n            \"pos should be an integer or a Column / column name, got {}\".format(type(pos)))\\n    if len is not None and not isinstance(len, (int, str, Column)):\\n        raise TypeError(\\n            \"len should be an integer or a Column / column name, got {}\".format(type(len)))\\n\\n    pos = _create_column_from_literal(pos) if isinstance(pos, int) else _to_java_column(pos)\\n    len = _create_column_from_literal(len) if isinstance(len, int) else _to_java_column(len)\\n\\n    sc = SparkContext._active_spark_context\\n\\n    return Column(sc._jvm.functions.overlay(\\n        _to_java_column(src),\\n        _to_java_column(replace),\\n        pos,\\n        len\\n    ))\\n\\n\\ndef sentences(string, language=None, country=None):\\n    \"\"\"\\n    Splits a string into arrays of sentences, where each sentence is an array of words.\\n    The \\'language\\' and \\'country\\' arguments are optional, and if omitted, the default locale is used.\\n\\n    .. versionadded:: 3.2.0\\n\\n    Parameters\\n    ----------\\n    string : :class:`~pyspark.sql.Column` or str\\n        a string to be split\\n    language : :class:`~pyspark.sql.Column` or str, optional\\n        a language of the locale\\n    country : :class:`~pyspark.sql.Column` or str, optional\\n        a country of the locale\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([[\"This is an example sentence.\"]], [\"string\"])\\n    >>> df.select(sentences(df.string, lit(\"en\"), lit(\"US\"))).show(truncate=False)\\n    +-----------------------------------+\\n    |sentences(string, en, US)          |\\n    +-----------------------------------+\\n    |[[This, is, an, example, sentence]]|\\n    +-----------------------------------+\\n    \"\"\"\\n    if language is None:\\n        language = lit(\"\")\\n    if country is None:\\n        country = lit(\"\")\\n\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.sentences(\\n        _to_java_column(string),\\n        _to_java_column(language),\\n        _to_java_column(country)\\n    ))\\n\\n\\ndef substring(str, pos, len):\\n    \"\"\"\\n    Substring starts at `pos` and is of length `len` when str is String type or\\n    returns the slice of byte array that starts at `pos` in byte and is of length `len`\\n    when str is Binary type.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Notes\\n    -----\\n    The position is not zero based, but 1 based index.\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'abcd\\',)], [\\'s\\',])\\n    >>> df.select(substring(df.s, 1, 2).alias(\\'s\\')).collect()\\n    [Row(s=\\'ab\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.substring(_to_java_column(str), pos, len))\\n\\n\\ndef substring_index(str, delim, count):\\n    \"\"\"\\n    Returns the substring from string str before count occurrences of the delimiter delim.\\n    If count is positive, everything the left of the final delimiter (counting from left) is\\n    returned. If count is negative, every to the right of the final delimiter (counting from the\\n    right) is returned. substring_index performs a case-sensitive match when searching for delim.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'a.b.c.d\\',)], [\\'s\\'])\\n    >>> df.select(substring_index(df.s, \\'.\\', 2).alias(\\'s\\')).collect()\\n    [Row(s=\\'a.b\\')]\\n    >>> df.select(substring_index(df.s, \\'.\\', -3).alias(\\'s\\')).collect()\\n    [Row(s=\\'b.c.d\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.substring_index(_to_java_column(str), delim, count))\\n\\n\\ndef levenshtein(left, right):\\n    \"\"\"Computes the Levenshtein distance of the two given strings.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df0 = spark.createDataFrame([(\\'kitten\\', \\'sitting\\',)], [\\'l\\', \\'r\\'])\\n    >>> df0.select(levenshtein(\\'l\\', \\'r\\').alias(\\'d\\')).collect()\\n    [Row(d=3)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right))\\n    return Column(jc)\\n\\n\\ndef locate(substr, str, pos=1):\\n    \"\"\"\\n    Locate the position of the first occurrence of substr in a string column, after position pos.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    substr : str\\n        a string\\n    str : :class:`~pyspark.sql.Column` or str\\n        a Column of :class:`pyspark.sql.types.StringType`\\n    pos : int, optional\\n        start position (zero based)\\n\\n    Notes\\n    -----\\n    The position is not zero based, but 1 based index. Returns 0 if substr\\n    could not be found in str.\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'abcd\\',)], [\\'s\\',])\\n    >>> df.select(locate(\\'b\\', df.s, 1).alias(\\'s\\')).collect()\\n    [Row(s=2)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.locate(substr, _to_java_column(str), pos))\\n\\n\\ndef lpad(col, len, pad):\\n    \"\"\"\\n    Left-pad the string column to width `len` with `pad`.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'abcd\\',)], [\\'s\\',])\\n    >>> df.select(lpad(df.s, 6, \\'#\\').alias(\\'s\\')).collect()\\n    [Row(s=\\'##abcd\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.lpad(_to_java_column(col), len, pad))\\n\\n\\ndef rpad(col, len, pad):\\n    \"\"\"\\n    Right-pad the string column to width `len` with `pad`.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'abcd\\',)], [\\'s\\',])\\n    >>> df.select(rpad(df.s, 6, \\'#\\').alias(\\'s\\')).collect()\\n    [Row(s=\\'abcd##\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.rpad(_to_java_column(col), len, pad))\\n\\n\\ndef repeat(col, n):\\n    \"\"\"\\n    Repeats a string column n times, and returns it as a new string column.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'ab\\',)], [\\'s\\',])\\n    >>> df.select(repeat(df.s, 3).alias(\\'s\\')).collect()\\n    [Row(s=\\'ababab\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.repeat(_to_java_column(col), n))\\n\\n\\ndef split(str, pattern, limit=-1):\\n    \"\"\"\\n    Splits str around matches of the given pattern.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    str : :class:`~pyspark.sql.Column` or str\\n        a string expression to split\\n    pattern : str\\n        a string representing a regular expression. The regex string should be\\n        a Java regular expression.\\n    limit : int, optional\\n        an integer which controls the number of times `pattern` is applied.\\n\\n        * ``limit > 0``: The resulting array\\'s length will not be more than `limit`, and the\\n                         resulting array\\'s last entry will contain all input beyond the last\\n                         matched pattern.\\n        * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\\n                          array can be of any size.\\n\\n        .. versionchanged:: 3.0\\n           `split` now takes an optional `limit` field. If not provided, default limit value is -1.\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'oneAtwoBthreeC\\',)], [\\'s\\',])\\n    >>> df.select(split(df.s, \\'[ABC]\\', 2).alias(\\'s\\')).collect()\\n    [Row(s=[\\'one\\', \\'twoBthreeC\\'])]\\n    >>> df.select(split(df.s, \\'[ABC]\\', -1).alias(\\'s\\')).collect()\\n    [Row(s=[\\'one\\', \\'two\\', \\'three\\', \\'\\'])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.split(_to_java_column(str), pattern, limit))\\n\\n\\ndef regexp_extract(str, pattern, idx):\\n    r\"\"\"Extract a specific group matched by a Java regex, from the specified string column.\\n    If the regex did not match, or the specified group did not match, an empty string is returned.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'100-200\\',)], [\\'str\\'])\\n    >>> df.select(regexp_extract(\\'str\\', r\\'(\\\\d+)-(\\\\d+)\\', 1).alias(\\'d\\')).collect()\\n    [Row(d=\\'100\\')]\\n    >>> df = spark.createDataFrame([(\\'foo\\',)], [\\'str\\'])\\n    >>> df.select(regexp_extract(\\'str\\', r\\'(\\\\d+)\\', 1).alias(\\'d\\')).collect()\\n    [Row(d=\\'\\')]\\n    >>> df = spark.createDataFrame([(\\'aaaac\\',)], [\\'str\\'])\\n    >>> df.select(regexp_extract(\\'str\\', \\'(a+)(b)?(c)\\', 2).alias(\\'d\\')).collect()\\n    [Row(d=\\'\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.regexp_extract(_to_java_column(str), pattern, idx)\\n    return Column(jc)\\n\\n\\ndef regexp_replace(str, pattern, replacement):\\n    r\"\"\"Replace all substrings of the specified string value that match regexp with rep.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'100-200\\',)], [\\'str\\'])\\n    >>> df.select(regexp_replace(\\'str\\', r\\'(\\\\d+)\\', \\'--\\').alias(\\'d\\')).collect()\\n    [Row(d=\\'-----\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.regexp_replace(_to_java_column(str), pattern, replacement)\\n    return Column(jc)\\n\\n\\ndef initcap(col):\\n    \"\"\"Translate the first letter of each word to upper case in the sentence.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'ab cd\\',)], [\\'a\\']).select(initcap(\"a\").alias(\\'v\\')).collect()\\n    [Row(v=\\'Ab Cd\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.initcap(_to_java_column(col)))\\n\\n\\ndef soundex(col):\\n    \"\"\"\\n    Returns the SoundEx encoding for a string\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], [\\'name\\'])\\n    >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\\n    [Row(soundex=\\'P362\\'), Row(soundex=\\'U612\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.soundex(_to_java_column(col)))\\n\\n\\ndef bin(col):\\n    \"\"\"Returns the string representation of the binary value of the given column.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df.select(bin(df.age).alias(\\'c\\')).collect()\\n    [Row(c=\\'10\\'), Row(c=\\'101\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.bin(_to_java_column(col))\\n    return Column(jc)\\n\\n\\ndef hex(col):\\n    \"\"\"Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\\n    :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\\n    :class:`pyspark.sql.types.LongType`.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'ABC\\', 3)], [\\'a\\', \\'b\\']).select(hex(\\'a\\'), hex(\\'b\\')).collect()\\n    [Row(hex(a)=\\'414243\\', hex(b)=\\'3\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.hex(_to_java_column(col))\\n    return Column(jc)\\n\\n\\ndef unhex(col):\\n    \"\"\"Inverse of hex. Interprets each pair of characters as a hexadecimal number\\n    and converts to the byte representation of number.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'414243\\',)], [\\'a\\']).select(unhex(\\'a\\')).collect()\\n    [Row(unhex(a)=bytearray(b\\'ABC\\'))]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.unhex(_to_java_column(col)))\\n\\n\\ndef length(col):\\n    \"\"\"Computes the character length of string data or number of bytes of binary data.\\n    The length of character data includes the trailing spaces. The length of binary data\\n    includes binary zeros.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'ABC \\',)], [\\'a\\']).select(length(\\'a\\').alias(\\'length\\')).collect()\\n    [Row(length=4)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.length(_to_java_column(col)))\\n\\n\\ndef translate(srcCol, matching, replace):\\n    \"\"\"A function translate any character in the `srcCol` by a character in `matching`.\\n    The characters in `replace` is corresponding to the characters in `matching`.\\n    The translate will happen when any character in the string matching with the character\\n    in the `matching`.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> spark.createDataFrame([(\\'translate\\',)], [\\'a\\']).select(translate(\\'a\\', \"rnlt\", \"123\") \\\\\\\\\\n    ...     .alias(\\'r\\')).collect()\\n    [Row(r=\\'1a2s3ae\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.translate(_to_java_column(srcCol), matching, replace))\\n\\n\\n# ---------------------- Collection functions ------------------------------\\n\\ndef create_map(*cols):\\n    \"\"\"Creates a new map column.\\n\\n    .. versionadded:: 2.0.0\\n\\n    Parameters\\n    ----------\\n    cols : :class:`~pyspark.sql.Column` or str\\n        column names or :class:`~pyspark.sql.Column`\\\\\\\\s that are\\n        grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).\\n\\n    Examples\\n    --------\\n    >>> df.select(create_map(\\'name\\', \\'age\\').alias(\"map\")).collect()\\n    [Row(map={\\'Alice\\': 2}), Row(map={\\'Bob\\': 5})]\\n    >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\\n    [Row(map={\\'Alice\\': 2}), Row(map={\\'Bob\\': 5})]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if len(cols) == 1 and isinstance(cols[0], (list, set)):\\n        cols = cols[0]\\n    jc = sc._jvm.functions.map(_to_seq(sc, cols, _to_java_column))\\n    return Column(jc)\\n\\n\\ndef map_from_arrays(col1, col2):\\n    \"\"\"Creates a new map from two arrays.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col1 : :class:`~pyspark.sql.Column` or str\\n        name of column containing a set of keys. All elements should not be null\\n    col2 : :class:`~pyspark.sql.Column` or str\\n        name of column containing a set of values\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([2, 5], [\\'a\\', \\'b\\'])], [\\'k\\', \\'v\\'])\\n    >>> df.select(map_from_arrays(df.k, df.v).alias(\"map\")).show()\\n    +----------------+\\n    |             map|\\n    +----------------+\\n    |{2 -> a, 5 -> b}|\\n    +----------------+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.map_from_arrays(_to_java_column(col1), _to_java_column(col2)))\\n\\n\\ndef array(*cols):\\n    \"\"\"Creates a new array column.\\n\\n    .. versionadded:: 1.4.0\\n\\n    Parameters\\n    ----------\\n    cols : :class:`~pyspark.sql.Column` or str\\n        column names or :class:`~pyspark.sql.Column`\\\\\\\\s that have\\n        the same data type.\\n\\n    Examples\\n    --------\\n    >>> df.select(array(\\'age\\', \\'age\\').alias(\"arr\")).collect()\\n    [Row(arr=[2, 2]), Row(arr=[5, 5])]\\n    >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\\n    [Row(arr=[2, 2]), Row(arr=[5, 5])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if len(cols) == 1 and isinstance(cols[0], (list, set)):\\n        cols = cols[0]\\n    jc = sc._jvm.functions.array(_to_seq(sc, cols, _to_java_column))\\n    return Column(jc)\\n\\n\\ndef array_contains(col, value):\\n    \"\"\"\\n    Collection function: returns null if the array is null, true if the array contains the\\n    given value, and false otherwise.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column containing array\\n    value :\\n        value or column to check for in array\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], [\\'data\\'])\\n    >>> df.select(array_contains(df.data, \"a\")).collect()\\n    [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\\n    >>> df.select(array_contains(df.data, lit(\"a\"))).collect()\\n    [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    value = value._jc if isinstance(value, Column) else value\\n    return Column(sc._jvm.functions.array_contains(_to_java_column(col), value))\\n\\n\\ndef arrays_overlap(a1, a2):\\n    \"\"\"\\n    Collection function: returns true if the arrays contain any common non-null element; if not,\\n    returns null if both the arrays are non-empty and any of them contains a null element; returns\\n    false otherwise.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], [\\'x\\', \\'y\\'])\\n    >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\\n    [Row(overlap=True), Row(overlap=False)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.arrays_overlap(_to_java_column(a1), _to_java_column(a2)))\\n\\n\\ndef slice(x, start, length):\\n    \"\"\"\\n    Collection function: returns an array containing  all the elements in `x` from index `start`\\n    (array indices start at 1, or from the end if `start` is negative) with the specified `length`.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    x : :class:`~pyspark.sql.Column` or str\\n        the array to be sliced\\n    start : :class:`~pyspark.sql.Column` or int\\n        the starting index\\n    length : :class:`~pyspark.sql.Column` or int\\n        the length of the slice\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], [\\'x\\'])\\n    >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\\n    [Row(sliced=[2, 3]), Row(sliced=[5])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.slice(\\n        _to_java_column(x),\\n        start._jc if isinstance(start, Column) else start,\\n        length._jc if isinstance(length, Column) else length\\n    ))\\n\\n\\ndef array_join(col, delimiter, null_replacement=None):\\n    \"\"\"\\n    Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\\n    `null_replacement` if set, otherwise they are ignored.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], [\\'data\\'])\\n    >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\\n    [Row(joined=\\'a,b,c\\'), Row(joined=\\'a\\')]\\n    >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\\n    [Row(joined=\\'a,b,c\\'), Row(joined=\\'a,NULL\\')]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if null_replacement is None:\\n        return Column(sc._jvm.functions.array_join(_to_java_column(col), delimiter))\\n    else:\\n        return Column(sc._jvm.functions.array_join(\\n            _to_java_column(col), delimiter, null_replacement))\\n\\n\\ndef concat(*cols):\\n    \"\"\"\\n    Concatenates multiple input columns together into a single column.\\n    The function works with strings, binary and compatible array columns.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'abcd\\',\\'123\\')], [\\'s\\', \\'d\\'])\\n    >>> df.select(concat(df.s, df.d).alias(\\'s\\')).collect()\\n    [Row(s=\\'abcd123\\')]\\n\\n    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], [\\'a\\', \\'b\\', \\'c\\'])\\n    >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.concat(_to_seq(sc, cols, _to_java_column)))\\n\\n\\ndef array_position(col, value):\\n    \"\"\"\\n    Collection function: Locates the position of the first occurrence of the given value\\n    in the given array. Returns null if either of the arguments are null.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Notes\\n    -----\\n    The position is not zero based, but 1 based index. Returns 0 if the given\\n    value could not be found in the array.\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], [\\'data\\'])\\n    >>> df.select(array_position(df.data, \"a\")).collect()\\n    [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_position(_to_java_column(col), value))\\n\\n\\ndef element_at(col, extraction):\\n    \"\"\"\\n    Collection function: Returns element of array at given index in extraction if col is array.\\n    Returns value for the given key in extraction if col is map.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column containing array or map\\n    extraction :\\n        index to check for in array or key to check for in map\\n\\n    Notes\\n    -----\\n    The position is not zero based, but 1 based index.\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], [\\'data\\'])\\n    >>> df.select(element_at(df.data, 1)).collect()\\n    [Row(element_at(data, 1)=\\'a\\'), Row(element_at(data, 1)=None)]\\n\\n    >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},), ({},)], [\\'data\\'])\\n    >>> df.select(element_at(df.data, lit(\"a\"))).collect()\\n    [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.element_at(\\n        _to_java_column(col), lit(extraction)._jc))\\n\\n\\ndef array_remove(col, element):\\n    \"\"\"\\n    Collection function: Remove all elements that equal to element from the given array.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column containing array\\n    element :\\n        element to be removed from the array\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], [\\'data\\'])\\n    >>> df.select(array_remove(df.data, 1)).collect()\\n    [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_remove(_to_java_column(col), element))\\n\\n\\ndef array_distinct(col):\\n    \"\"\"\\n    Collection function: removes duplicate values from the array.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], [\\'data\\'])\\n    >>> df.select(array_distinct(df.data)).collect()\\n    [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_distinct(_to_java_column(col)))\\n\\n\\ndef array_intersect(col1, col2):\\n    \"\"\"\\n    Collection function: returns an array of the elements in the intersection of col1 and col2,\\n    without duplicates.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col1 : :class:`~pyspark.sql.Column` or str\\n        name of column containing array\\n    col2 : :class:`~pyspark.sql.Column` or str\\n        name of column containing array\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql import Row\\n    >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\\n    >>> df.select(array_intersect(df.c1, df.c2)).collect()\\n    [Row(array_intersect(c1, c2)=[\\'a\\', \\'c\\'])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_intersect(_to_java_column(col1), _to_java_column(col2)))\\n\\n\\ndef array_union(col1, col2):\\n    \"\"\"\\n    Collection function: returns an array of the elements in the union of col1 and col2,\\n    without duplicates.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col1 : :class:`~pyspark.sql.Column` or str\\n        name of column containing array\\n    col2 : :class:`~pyspark.sql.Column` or str\\n        name of column containing array\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql import Row\\n    >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\\n    >>> df.select(array_union(df.c1, df.c2)).collect()\\n    [Row(array_union(c1, c2)=[\\'b\\', \\'a\\', \\'c\\', \\'d\\', \\'f\\'])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_union(_to_java_column(col1), _to_java_column(col2)))\\n\\n\\ndef array_except(col1, col2):\\n    \"\"\"\\n    Collection function: returns an array of the elements in col1 but not in col2,\\n    without duplicates.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col1 : :class:`~pyspark.sql.Column` or str\\n        name of column containing array\\n    col2 : :class:`~pyspark.sql.Column` or str\\n        name of column containing array\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql import Row\\n    >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\\n    >>> df.select(array_except(df.c1, df.c2)).collect()\\n    [Row(array_except(c1, c2)=[\\'b\\'])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_except(_to_java_column(col1), _to_java_column(col2)))\\n\\n\\ndef explode(col):\\n    \"\"\"\\n    Returns a new row for each element in the given array or map.\\n    Uses the default column name `col` for elements in the array and\\n    `key` and `value` for elements in the map unless specified otherwise.\\n\\n    .. versionadded:: 1.4.0\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql import Row\\n    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\\n    >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\\n    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\\n\\n    >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\\n    +---+-----+\\n    |key|value|\\n    +---+-----+\\n    |  a|    b|\\n    +---+-----+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.explode(_to_java_column(col))\\n    return Column(jc)\\n\\n\\ndef posexplode(col):\\n    \"\"\"\\n    Returns a new row for each element with position in the given array or map.\\n    Uses the default column name `pos` for position, and `col` for elements in the\\n    array and `key` and `value` for elements in the map unless specified otherwise.\\n\\n    .. versionadded:: 2.1.0\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql import Row\\n    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\\n    >>> eDF.select(posexplode(eDF.intlist)).collect()\\n    [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\\n\\n    >>> eDF.select(posexplode(eDF.mapfield)).show()\\n    +---+---+-----+\\n    |pos|key|value|\\n    +---+---+-----+\\n    |  0|  a|    b|\\n    +---+---+-----+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.posexplode(_to_java_column(col))\\n    return Column(jc)\\n\\n\\ndef explode_outer(col):\\n    \"\"\"\\n    Returns a new row for each element in the given array or map.\\n    Unlike explode, if the array/map is null or empty then null is produced.\\n    Uses the default column name `col` for elements in the array and\\n    `key` and `value` for elements in the map unless specified otherwise.\\n\\n    .. versionadded:: 2.3.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame(\\n    ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\\n    ...     (\"id\", \"an_array\", \"a_map\")\\n    ... )\\n    >>> df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\\n    +---+----------+----+-----+\\n    | id|  an_array| key|value|\\n    +---+----------+----+-----+\\n    |  1|[foo, bar]|   x|  1.0|\\n    |  2|        []|null| null|\\n    |  3|      null|null| null|\\n    +---+----------+----+-----+\\n\\n    >>> df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()\\n    +---+----------+----+\\n    | id|     a_map| col|\\n    +---+----------+----+\\n    |  1|{x -> 1.0}| foo|\\n    |  1|{x -> 1.0}| bar|\\n    |  2|        {}|null|\\n    |  3|      null|null|\\n    +---+----------+----+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.explode_outer(_to_java_column(col))\\n    return Column(jc)\\n\\n\\ndef posexplode_outer(col):\\n    \"\"\"\\n    Returns a new row for each element with position in the given array or map.\\n    Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\\n    Uses the default column name `pos` for position, and `col` for elements in the\\n    array and `key` and `value` for elements in the map unless specified otherwise.\\n\\n    .. versionadded:: 2.3.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame(\\n    ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\\n    ...     (\"id\", \"an_array\", \"a_map\")\\n    ... )\\n    >>> df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()\\n    +---+----------+----+----+-----+\\n    | id|  an_array| pos| key|value|\\n    +---+----------+----+----+-----+\\n    |  1|[foo, bar]|   0|   x|  1.0|\\n    |  2|        []|null|null| null|\\n    |  3|      null|null|null| null|\\n    +---+----------+----+----+-----+\\n    >>> df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()\\n    +---+----------+----+----+\\n    | id|     a_map| pos| col|\\n    +---+----------+----+----+\\n    |  1|{x -> 1.0}|   0| foo|\\n    |  1|{x -> 1.0}|   1| bar|\\n    |  2|        {}|null|null|\\n    |  3|      null|null|null|\\n    +---+----------+----+----+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.posexplode_outer(_to_java_column(col))\\n    return Column(jc)\\n\\n\\ndef get_json_object(col, path):\\n    \"\"\"\\n    Extracts json object from a json string based on json path specified, and returns json string\\n    of the extracted json object. It will return null if the input json string is invalid.\\n\\n    .. versionadded:: 1.6.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        string column in json format\\n    path : str\\n        path to the json object to extract\\n\\n    Examples\\n    --------\\n    >>> data = [(\"1\", \\'\\'\\'{\"f1\": \"value1\", \"f2\": \"value2\"}\\'\\'\\'), (\"2\", \\'\\'\\'{\"f1\": \"value12\"}\\'\\'\\')]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\\n    >>> df.select(df.key, get_json_object(df.jstring, \\'$.f1\\').alias(\"c0\"), \\\\\\\\\\n    ...                   get_json_object(df.jstring, \\'$.f2\\').alias(\"c1\") ).collect()\\n    [Row(key=\\'1\\', c0=\\'value1\\', c1=\\'value2\\'), Row(key=\\'2\\', c0=\\'value12\\', c1=None)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.get_json_object(_to_java_column(col), path)\\n    return Column(jc)\\n\\n\\ndef json_tuple(col, *fields):\\n    \"\"\"Creates a new row for a json column according to the given field names.\\n\\n    .. versionadded:: 1.6.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        string column in json format\\n    fields : str\\n        fields to extract\\n\\n    Examples\\n    --------\\n    >>> data = [(\"1\", \\'\\'\\'{\"f1\": \"value1\", \"f2\": \"value2\"}\\'\\'\\'), (\"2\", \\'\\'\\'{\"f1\": \"value12\"}\\'\\'\\')]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\\n    >>> df.select(df.key, json_tuple(df.jstring, \\'f1\\', \\'f2\\')).collect()\\n    [Row(key=\\'1\\', c0=\\'value1\\', c1=\\'value2\\'), Row(key=\\'2\\', c0=\\'value12\\', c1=None)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.json_tuple(_to_java_column(col), _to_seq(sc, fields))\\n    return Column(jc)\\n\\n\\ndef from_json(col, schema, options=None):\\n    \"\"\"\\n    Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\\n    as keys type, :class:`StructType` or :class:`ArrayType` with\\n    the specified schema. Returns `null`, in the case of an unparseable string.\\n\\n    .. versionadded:: 2.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        string column in json format\\n    schema : :class:`DataType` or str\\n        a StructType or ArrayType of StructType to use when parsing the json column.\\n\\n        .. versionchanged:: 2.3\\n            the DDL-formatted string is also supported for ``schema``.\\n    options : dict, optional\\n        options to control parsing. accepts the same options as the json datasource.\\n        See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\\n        in the version you use.\\n\\n        .. # noqa\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql.types import *\\n    >>> data = [(1, \\'\\'\\'{\"a\": 1}\\'\\'\\')]\\n    >>> schema = StructType([StructField(\"a\", IntegerType())])\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\\n    [Row(json=Row(a=1))]\\n    >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\\n    [Row(json=Row(a=1))]\\n    >>> df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\\n    [Row(json={\\'a\\': 1})]\\n    >>> data = [(1, \\'\\'\\'[{\"a\": 1}]\\'\\'\\')]\\n    >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\\n    [Row(json=[Row(a=1)])]\\n    >>> schema = schema_of_json(lit(\\'\\'\\'{\"a\": 0}\\'\\'\\'))\\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\\n    [Row(json=Row(a=None))]\\n    >>> data = [(1, \\'\\'\\'[1, 2, 3]\\'\\'\\')]\\n    >>> schema = ArrayType(IntegerType())\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\\n    [Row(json=[1, 2, 3])]\\n    \"\"\"\\n\\n    sc = SparkContext._active_spark_context\\n    if isinstance(schema, DataType):\\n        schema = schema.json()\\n    elif isinstance(schema, Column):\\n        schema = _to_java_column(schema)\\n    jc = sc._jvm.functions.from_json(_to_java_column(col), schema, _options_to_str(options))\\n    return Column(jc)\\n\\n\\ndef to_json(col, options=None):\\n    \"\"\"\\n    Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\\n    into a JSON string. Throws an exception, in the case of an unsupported type.\\n\\n    .. versionadded:: 2.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column containing a struct, an array or a map.\\n    options : dict, optional\\n        options to control converting. accepts the same options as the JSON datasource.\\n        See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\\n        in the version you use.\\n        Additionally the function supports the `pretty` option which enables\\n        pretty JSON generation.\\n\\n        .. # noqa\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql import Row\\n    >>> from pyspark.sql.types import *\\n    >>> data = [(1, Row(age=2, name=\\'Alice\\'))]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(to_json(df.value).alias(\"json\")).collect()\\n    [Row(json=\\'{\"age\":2,\"name\":\"Alice\"}\\')]\\n    >>> data = [(1, [Row(age=2, name=\\'Alice\\'), Row(age=3, name=\\'Bob\\')])]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(to_json(df.value).alias(\"json\")).collect()\\n    [Row(json=\\'[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]\\')]\\n    >>> data = [(1, {\"name\": \"Alice\"})]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(to_json(df.value).alias(\"json\")).collect()\\n    [Row(json=\\'{\"name\":\"Alice\"}\\')]\\n    >>> data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(to_json(df.value).alias(\"json\")).collect()\\n    [Row(json=\\'[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]\\')]\\n    >>> data = [(1, [\"Alice\", \"Bob\"])]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(to_json(df.value).alias(\"json\")).collect()\\n    [Row(json=\\'[\"Alice\",\"Bob\"]\\')]\\n    \"\"\"\\n\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.to_json(_to_java_column(col), _options_to_str(options))\\n    return Column(jc)\\n\\n\\ndef schema_of_json(json, options=None):\\n    \"\"\"\\n    Parses a JSON string and infers its schema in DDL format.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    json : :class:`~pyspark.sql.Column` or str\\n        a JSON string or a foldable string column containing a JSON string.\\n    options : dict, optional\\n        options to control parsing. accepts the same options as the JSON datasource.\\n        See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\\n        in the version you use.\\n\\n        .. # noqa\\n\\n        .. versionchanged:: 3.0\\n           It accepts `options` parameter to control schema inferring.\\n\\n    Examples\\n    --------\\n    >>> df = spark.range(1)\\n    >>> df.select(schema_of_json(lit(\\'{\"a\": 0}\\')).alias(\"json\")).collect()\\n    [Row(json=\\'STRUCT<`a`: BIGINT>\\')]\\n    >>> schema = schema_of_json(\\'{a: 1}\\', {\\'allowUnquotedFieldNames\\':\\'true\\'})\\n    >>> df.select(schema.alias(\"json\")).collect()\\n    [Row(json=\\'STRUCT<`a`: BIGINT>\\')]\\n    \"\"\"\\n    if isinstance(json, str):\\n        col = _create_column_from_literal(json)\\n    elif isinstance(json, Column):\\n        col = _to_java_column(json)\\n    else:\\n        raise TypeError(\"schema argument should be a column or string\")\\n\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.schema_of_json(col, _options_to_str(options))\\n    return Column(jc)\\n\\n\\ndef schema_of_csv(csv, options=None):\\n    \"\"\"\\n    Parses a CSV string and infers its schema in DDL format.\\n\\n    .. versionadded:: 3.0.0\\n\\n    Parameters\\n    ----------\\n    csv : :class:`~pyspark.sql.Column` or str\\n        a CSV string or a foldable string column containing a CSV string.\\n    options : dict, optional\\n        options to control parsing. accepts the same options as the CSV datasource.\\n        See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\\n        in the version you use.\\n\\n        .. # noqa\\n\\n    Examples\\n    --------\\n    >>> df = spark.range(1)\\n    >>> df.select(schema_of_csv(lit(\\'1|a\\'), {\\'sep\\':\\'|\\'}).alias(\"csv\")).collect()\\n    [Row(csv=\\'STRUCT<`_c0`: INT, `_c1`: STRING>\\')]\\n    >>> df.select(schema_of_csv(\\'1|a\\', {\\'sep\\':\\'|\\'}).alias(\"csv\")).collect()\\n    [Row(csv=\\'STRUCT<`_c0`: INT, `_c1`: STRING>\\')]\\n    \"\"\"\\n    if isinstance(csv, str):\\n        col = _create_column_from_literal(csv)\\n    elif isinstance(csv, Column):\\n        col = _to_java_column(csv)\\n    else:\\n        raise TypeError(\"schema argument should be a column or string\")\\n\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.schema_of_csv(col, _options_to_str(options))\\n    return Column(jc)\\n\\n\\ndef to_csv(col, options=None):\\n    \"\"\"\\n    Converts a column containing a :class:`StructType` into a CSV string.\\n    Throws an exception, in the case of an unsupported type.\\n\\n    .. versionadded:: 3.0.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column containing a struct.\\n    options: dict, optional\\n        options to control converting. accepts the same options as the CSV datasource.\\n        See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\\n        in the version you use.\\n\\n        .. # noqa\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql import Row\\n    >>> data = [(1, Row(age=2, name=\\'Alice\\'))]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\\n    [Row(csv=\\'2,Alice\\')]\\n    \"\"\"\\n\\n    sc = SparkContext._active_spark_context\\n    jc = sc._jvm.functions.to_csv(_to_java_column(col), _options_to_str(options))\\n    return Column(jc)\\n\\n\\ndef size(col):\\n    \"\"\"\\n    Collection function: returns the length of the array or map stored in the column.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], [\\'data\\'])\\n    >>> df.select(size(df.data)).collect()\\n    [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.size(_to_java_column(col)))\\n\\n\\ndef array_min(col):\\n    \"\"\"\\n    Collection function: returns the minimum value of the array.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], [\\'data\\'])\\n    >>> df.select(array_min(df.data).alias(\\'min\\')).collect()\\n    [Row(min=1), Row(min=-1)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_min(_to_java_column(col)))\\n\\n\\ndef array_max(col):\\n    \"\"\"\\n    Collection function: returns the maximum value of the array.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], [\\'data\\'])\\n    >>> df.select(array_max(df.data).alias(\\'max\\')).collect()\\n    [Row(max=3), Row(max=10)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_max(_to_java_column(col)))\\n\\n\\ndef sort_array(col, asc=True):\\n    \"\"\"\\n    Collection function: sorts the input array in ascending or descending order according\\n    to the natural ordering of the array elements. Null elements will be placed at the beginning\\n    of the returned array in ascending order or at the end of the returned array in descending\\n    order.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    asc : bool, optional\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], [\\'data\\'])\\n    >>> df.select(sort_array(df.data).alias(\\'r\\')).collect()\\n    [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\\n    >>> df.select(sort_array(df.data, asc=False).alias(\\'r\\')).collect()\\n    [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.sort_array(_to_java_column(col), asc))\\n\\n\\ndef array_sort(col):\\n    \"\"\"\\n    Collection function: sorts the input array in ascending order. The elements of the input array\\n    must be orderable. Null elements will be placed at the end of the returned array.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], [\\'data\\'])\\n    >>> df.select(array_sort(df.data).alias(\\'r\\')).collect()\\n    [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_sort(_to_java_column(col)))\\n\\n\\ndef shuffle(col):\\n    \"\"\"\\n    Collection function: Generates a random permutation of the given array.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Notes\\n    -----\\n    The function is non-deterministic.\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], [\\'data\\'])\\n    >>> df.select(shuffle(df.data).alias(\\'s\\')).collect()  # doctest: +SKIP\\n    [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.shuffle(_to_java_column(col)))\\n\\n\\ndef reverse(col):\\n    \"\"\"\\n    Collection function: returns a reversed string or an array with reverse order of elements.\\n\\n    .. versionadded:: 1.5.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'Spark SQL\\',)], [\\'data\\'])\\n    >>> df.select(reverse(df.data).alias(\\'s\\')).collect()\\n    [Row(s=\\'LQS krapS\\')]\\n    >>> df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], [\\'data\\'])\\n    >>> df.select(reverse(df.data).alias(\\'r\\')).collect()\\n    [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]\\n     \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.reverse(_to_java_column(col)))\\n\\n\\ndef flatten(col):\\n    \"\"\"\\n    Collection function: creates a single array from an array of arrays.\\n    If a structure of nested arrays is deeper than two levels,\\n    only one level of nesting is removed.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], [\\'data\\'])\\n    >>> df.select(flatten(df.data).alias(\\'r\\')).collect()\\n    [Row(r=[1, 2, 3, 4, 5, 6]), Row(r=None)]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.flatten(_to_java_column(col)))\\n\\n\\ndef map_keys(col):\\n    \"\"\"\\n    Collection function: Returns an unordered array containing the keys of the map.\\n\\n    .. versionadded:: 2.3.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql.functions import map_keys\\n    >>> df = spark.sql(\"SELECT map(1, \\'a\\', 2, \\'b\\') as data\")\\n    >>> df.select(map_keys(\"data\").alias(\"keys\")).show()\\n    +------+\\n    |  keys|\\n    +------+\\n    |[1, 2]|\\n    +------+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.map_keys(_to_java_column(col)))\\n\\n\\ndef map_values(col):\\n    \"\"\"\\n    Collection function: Returns an unordered array containing the values of the map.\\n\\n    .. versionadded:: 2.3.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql.functions import map_values\\n    >>> df = spark.sql(\"SELECT map(1, \\'a\\', 2, \\'b\\') as data\")\\n    >>> df.select(map_values(\"data\").alias(\"values\")).show()\\n    +------+\\n    |values|\\n    +------+\\n    |[a, b]|\\n    +------+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.map_values(_to_java_column(col)))\\n\\n\\ndef map_entries(col):\\n    \"\"\"\\n    Collection function: Returns an unordered array of all entries in the given map.\\n\\n    .. versionadded:: 3.0.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql.functions import map_entries\\n    >>> df = spark.sql(\"SELECT map(1, \\'a\\', 2, \\'b\\') as data\")\\n    >>> df.select(map_entries(\"data\").alias(\"entries\")).show()\\n    +----------------+\\n    |         entries|\\n    +----------------+\\n    |[{1, a}, {2, b}]|\\n    +----------------+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.map_entries(_to_java_column(col)))\\n\\n\\ndef map_from_entries(col):\\n    \"\"\"\\n    Collection function: Returns a map created from the given array of entries.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql.functions import map_from_entries\\n    >>> df = spark.sql(\"SELECT array(struct(1, \\'a\\'), struct(2, \\'b\\')) as data\")\\n    >>> df.select(map_from_entries(\"data\").alias(\"map\")).show()\\n    +----------------+\\n    |             map|\\n    +----------------+\\n    |{1 -> a, 2 -> b}|\\n    +----------------+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.map_from_entries(_to_java_column(col)))\\n\\n\\ndef array_repeat(col, count):\\n    \"\"\"\\n    Collection function: creates an array containing a column repeated count times.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(\\'ab\\',)], [\\'data\\'])\\n    >>> df.select(array_repeat(df.data, 3).alias(\\'r\\')).collect()\\n    [Row(r=[\\'ab\\', \\'ab\\', \\'ab\\'])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.array_repeat(\\n        _to_java_column(col),\\n        _to_java_column(count) if isinstance(count, Column) else count\\n    ))\\n\\n\\ndef arrays_zip(*cols):\\n    \"\"\"\\n    Collection function: Returns a merged array of structs in which the N-th struct contains all\\n    N-th values of input arrays.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    cols : :class:`~pyspark.sql.Column` or str\\n        columns of arrays to be merged.\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql.functions import arrays_zip\\n    >>> df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], [\\'vals1\\', \\'vals2\\'])\\n    >>> df.select(arrays_zip(df.vals1, df.vals2).alias(\\'zipped\\')).collect()\\n    [Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.arrays_zip(_to_seq(sc, cols, _to_java_column)))\\n\\n\\ndef map_concat(*cols):\\n    \"\"\"Returns the union of all the given maps.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Parameters\\n    ----------\\n    cols : :class:`~pyspark.sql.Column` or str\\n        column names or :class:`~pyspark.sql.Column`\\\\\\\\s\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql.functions import map_concat\\n    >>> df = spark.sql(\"SELECT map(1, \\'a\\', 2, \\'b\\') as map1, map(3, \\'c\\') as map2\")\\n    >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\\n    +------------------------+\\n    |map3                    |\\n    +------------------------+\\n    |{1 -> a, 2 -> b, 3 -> c}|\\n    +------------------------+\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if len(cols) == 1 and isinstance(cols[0], (list, set)):\\n        cols = cols[0]\\n    jc = sc._jvm.functions.map_concat(_to_seq(sc, cols, _to_java_column))\\n    return Column(jc)\\n\\n\\ndef sequence(start, stop, step=None):\\n    \"\"\"\\n    Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\\n    If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\\n    otherwise -1.\\n\\n    .. versionadded:: 2.4.0\\n\\n    Examples\\n    --------\\n    >>> df1 = spark.createDataFrame([(-2, 2)], (\\'C1\\', \\'C2\\'))\\n    >>> df1.select(sequence(\\'C1\\', \\'C2\\').alias(\\'r\\')).collect()\\n    [Row(r=[-2, -1, 0, 1, 2])]\\n    >>> df2 = spark.createDataFrame([(4, -4, -2)], (\\'C1\\', \\'C2\\', \\'C3\\'))\\n    >>> df2.select(sequence(\\'C1\\', \\'C2\\', \\'C3\\').alias(\\'r\\')).collect()\\n    [Row(r=[4, 2, 0, -2, -4])]\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    if step is None:\\n        return Column(sc._jvm.functions.sequence(_to_java_column(start), _to_java_column(stop)))\\n    else:\\n        return Column(sc._jvm.functions.sequence(\\n            _to_java_column(start), _to_java_column(stop), _to_java_column(step)))\\n\\n\\ndef from_csv(col, schema, options=None):\\n    \"\"\"\\n    Parses a column containing a CSV string to a row with the specified schema.\\n    Returns `null`, in the case of an unparseable string.\\n\\n    .. versionadded:: 3.0.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        string column in CSV format\\n    schema :class:`~pyspark.sql.Column` or str\\n        a string with schema in DDL format to use when parsing the CSV column.\\n    options : dict, optional\\n        options to control parsing. accepts the same options as the CSV datasource.\\n        See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\\n        in the version you use.\\n\\n        .. # noqa\\n\\n    Examples\\n    --------\\n    >>> data = [(\"1,2,3\",)]\\n    >>> df = spark.createDataFrame(data, (\"value\",))\\n    >>> df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()\\n    [Row(csv=Row(a=1, b=2, c=3))]\\n    >>> value = data[0][0]\\n    >>> df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()\\n    [Row(csv=Row(_c0=1, _c1=2, _c2=3))]\\n    >>> data = [(\"   abc\",)]\\n    >>> df = spark.createDataFrame(data, (\"value\",))\\n    >>> options = {\\'ignoreLeadingWhiteSpace\\': True}\\n    >>> df.select(from_csv(df.value, \"s string\", options).alias(\"csv\")).collect()\\n    [Row(csv=Row(s=\\'abc\\'))]\\n    \"\"\"\\n\\n    sc = SparkContext._active_spark_context\\n    if isinstance(schema, str):\\n        schema = _create_column_from_literal(schema)\\n    elif isinstance(schema, Column):\\n        schema = _to_java_column(schema)\\n    else:\\n        raise TypeError(\"schema argument should be a column or string\")\\n\\n    jc = sc._jvm.functions.from_csv(_to_java_column(col), schema, _options_to_str(options))\\n    return Column(jc)\\n\\n\\ndef _unresolved_named_lambda_variable(*name_parts):\\n    \"\"\"\\n    Create `o.a.s.sql.expressions.UnresolvedNamedLambdaVariable`,\\n    convert it to o.s.sql.Column and wrap in Python `Column`\\n\\n    Parameters\\n    ----------\\n    name_parts : str\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    name_parts_seq = _to_seq(sc, name_parts)\\n    expressions = sc._jvm.org.apache.spark.sql.catalyst.expressions\\n    return Column(\\n        sc._jvm.Column(\\n            expressions.UnresolvedNamedLambdaVariable(name_parts_seq)\\n        )\\n    )\\n\\n\\ndef _get_lambda_parameters(f):\\n    import inspect\\n\\n    signature = inspect.signature(f)\\n    parameters = signature.parameters.values()\\n\\n    # We should exclude functions that use\\n    # variable args and keyword argnames\\n    # as well as keyword only args\\n    supported_parameter_types = {\\n        inspect.Parameter.POSITIONAL_OR_KEYWORD,\\n        inspect.Parameter.POSITIONAL_ONLY,\\n    }\\n\\n    # Validate that\\n    # function arity is between 1 and 3\\n    if not (1 <= len(parameters) <= 3):\\n        raise ValueError(\\n            \"f should take between 1 and 3 arguments, but provided function takes {}\".format(\\n                len(parameters)\\n            )\\n        )\\n\\n    # and all arguments can be used as positional\\n    if not all(p.kind in supported_parameter_types for p in parameters):\\n        raise ValueError(\\n            \"f should use only POSITIONAL or POSITIONAL OR KEYWORD arguments\"\\n        )\\n\\n    return parameters\\n\\n\\ndef _create_lambda(f):\\n    \"\"\"\\n    Create `o.a.s.sql.expressions.LambdaFunction` corresponding\\n    to transformation described by f\\n\\n    :param f: A Python of one of the following forms:\\n            - (Column) -> Column: ...\\n            - (Column, Column) -> Column: ...\\n            - (Column, Column, Column) -> Column: ...\\n    \"\"\"\\n    parameters = _get_lambda_parameters(f)\\n\\n    sc = SparkContext._active_spark_context\\n    expressions = sc._jvm.org.apache.spark.sql.catalyst.expressions\\n\\n    argnames = [\"x\", \"y\", \"z\"]\\n    args = [\\n        _unresolved_named_lambda_variable(\\n            expressions.UnresolvedNamedLambdaVariable.freshVarName(arg)\\n        )\\n        for arg in argnames[: len(parameters)]\\n    ]\\n\\n    result = f(*args)\\n\\n    if not isinstance(result, Column):\\n        raise ValueError(\"f should return Column, got {}\".format(type(result)))\\n\\n    jexpr = result._jc.expr()\\n    jargs = _to_seq(sc, [arg._jc.expr() for arg in args])\\n\\n    return expressions.LambdaFunction(jexpr, jargs, False)\\n\\n\\ndef _invoke_higher_order_function(name, cols, funs):\\n    \"\"\"\\n    Invokes expression identified by name,\\n    (relative to ```org.apache.spark.sql.catalyst.expressions``)\\n    and wraps the result with Column (first Scala one, then Python).\\n\\n    :param name: Name of the expression\\n    :param cols: a list of columns\\n    :param funs: a list of((*Column) -> Column functions.\\n\\n    :return: a Column\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    expressions = sc._jvm.org.apache.spark.sql.catalyst.expressions\\n    expr = getattr(expressions, name)\\n\\n    jcols = [_to_java_column(col).expr() for col in cols]\\n    jfuns = [_create_lambda(f) for f in funs]\\n\\n    return Column(sc._jvm.Column(expr(*jcols + jfuns)))\\n\\n\\ndef transform(col, f):\\n    \"\"\"\\n    Returns an array of elements after applying a transformation to each element in the input array.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    f : function\\n        a function that is applied to each element of the input array.\\n        Can take one of the following forms:\\n\\n        - Unary ``(x: Column) -> Column: ...``\\n        - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\\n            a 0-based index of the element.\\n\\n        and can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n        Python ``UserDefinedFunctions`` are not supported\\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\\n    >>> df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()\\n    +------------+\\n    |     doubled|\\n    +------------+\\n    |[2, 4, 6, 8]|\\n    +------------+\\n\\n    >>> def alternate(x, i):\\n    ...     return when(i % 2 == 0, x).otherwise(-x)\\n    >>> df.select(transform(\"values\", alternate).alias(\"alternated\")).show()\\n    +--------------+\\n    |    alternated|\\n    +--------------+\\n    |[1, -2, 3, -4]|\\n    +--------------+\\n    \"\"\"\\n    return _invoke_higher_order_function(\"ArrayTransform\", [col], [f])\\n\\n\\ndef exists(col, f):\\n    \"\"\"\\n    Returns whether a predicate holds for one or more elements in the array.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    f : function\\n        ``(x: Column) -> Column: ...``  returning the Boolean expression.\\n        Can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n        Python ``UserDefinedFunctions`` are not supported\\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n    :return: a :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\\n    >>> df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()\\n    +------------+\\n    |any_negative|\\n    +------------+\\n    |       false|\\n    |        true|\\n    +------------+\\n    \"\"\"\\n    return _invoke_higher_order_function(\"ArrayExists\", [col], [f])\\n\\n\\ndef forall(col, f):\\n    \"\"\"\\n    Returns whether a predicate holds for every element in the array.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    f : function\\n        ``(x: Column) -> Column: ...``  returning the Boolean expression.\\n        Can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n        Python ``UserDefinedFunctions`` are not supported\\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame(\\n    ...     [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\\n    ...     (\"key\", \"values\")\\n    ... )\\n    >>> df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()\\n    +-------+\\n    |all_foo|\\n    +-------+\\n    |  false|\\n    |  false|\\n    |   true|\\n    +-------+\\n    \"\"\"\\n    return _invoke_higher_order_function(\"ArrayForAll\", [col], [f])\\n\\n\\ndef filter(col, f):\\n    \"\"\"\\n    Returns an array of elements for which a predicate holds in a given array.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    f : function\\n        A function that returns the Boolean expression.\\n        Can take one of the following forms:\\n\\n        - Unary ``(x: Column) -> Column: ...``\\n        - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\\n            a 0-based index of the element.\\n\\n        and can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n        Python ``UserDefinedFunctions`` are not supported\\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame(\\n    ...     [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\"])],\\n    ...     (\"key\", \"values\")\\n    ... )\\n    >>> def after_second_quarter(x):\\n    ...     return month(to_date(x)) > 6\\n    >>> df.select(\\n    ...     filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\\n    ... ).show(truncate=False)\\n    +------------------------+\\n    |after_second_quarter    |\\n    +------------------------+\\n    |[2018-09-20, 2019-07-01]|\\n    +------------------------+\\n    \"\"\"\\n    return _invoke_higher_order_function(\"ArrayFilter\", [col], [f])\\n\\n\\ndef aggregate(col, initialValue, merge, finish=None):\\n    \"\"\"\\n    Applies a binary operator to an initial state and all elements in the array,\\n    and reduces this to a single state. The final state is converted into the final result\\n    by applying a finish function.\\n\\n    Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n    :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n    Python ``UserDefinedFunctions`` are not supported\\n    (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    initialValue : :class:`~pyspark.sql.Column` or str\\n        initial value. Name of column or expression\\n    merge : function\\n        a binary function ``(acc: Column, x: Column) -> Column...`` returning expression\\n        of the same type as ``zero``\\n    finish : function\\n        an optional unary function ``(x: Column) -> Column: ...``\\n        used to convert accumulated value.\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\\n    >>> df.select(aggregate(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\\n    +----+\\n    | sum|\\n    +----+\\n    |42.0|\\n    +----+\\n\\n    >>> def merge(acc, x):\\n    ...     count = acc.count + 1\\n    ...     sum = acc.sum + x\\n    ...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\\n    >>> df.select(\\n    ...     aggregate(\\n    ...         \"values\",\\n    ...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\\n    ...         merge,\\n    ...         lambda acc: acc.sum / acc.count,\\n    ...     ).alias(\"mean\")\\n    ... ).show()\\n    +----+\\n    |mean|\\n    +----+\\n    | 8.4|\\n    +----+\\n    \"\"\"\\n    if finish is not None:\\n        return _invoke_higher_order_function(\\n            \"ArrayAggregate\",\\n            [col, initialValue],\\n            [merge, finish]\\n        )\\n\\n    else:\\n        return _invoke_higher_order_function(\\n            \"ArrayAggregate\",\\n            [col, initialValue],\\n            [merge]\\n        )\\n\\n\\ndef zip_with(left, right, f):\\n    \"\"\"\\n    Merge two given arrays, element-wise, into a single array using a function.\\n    If one array is shorter, nulls are appended at the end to match the length of the longer\\n    array, before applying the function.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    left : :class:`~pyspark.sql.Column` or str\\n        name of the first column or expression\\n    right : :class:`~pyspark.sql.Column` or str\\n        name of the second column or expression\\n    f : function\\n        a binary function ``(x1: Column, x2: Column) -> Column...``\\n        Can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n        Python ``UserDefinedFunctions`` are not supported\\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\"))\\n    >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)\\n    +---------------------------+\\n    |powers                     |\\n    +---------------------------+\\n    |[1.0, 9.0, 625.0, 262144.0]|\\n    +---------------------------+\\n\\n    >>> df = spark.createDataFrame([(1, [\"foo\", \"bar\"], [1, 2, 3])], (\"id\", \"xs\", \"ys\"))\\n    >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: concat_ws(\"_\", x, y)).alias(\"xs_ys\")).show()\\n    +-----------------+\\n    |            xs_ys|\\n    +-----------------+\\n    |[foo_1, bar_2, 3]|\\n    +-----------------+\\n    \"\"\"\\n    return _invoke_higher_order_function(\"ZipWith\", [left, right], [f])\\n\\n\\ndef transform_keys(col, f):\\n    \"\"\"\\n    Applies a function to every key-value pair in a map and returns\\n    a map with the results of those applications as the new keys for the pairs.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    f : function\\n        a binary function ``(k: Column, v: Column) -> Column...``\\n        Can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n        Python ``UserDefinedFunctions`` are not supported\\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\"))\\n    >>> df.select(transform_keys(\\n    ...     \"data\", lambda k, _: upper(k)).alias(\"data_upper\")\\n    ... ).show(truncate=False)\\n    +-------------------------+\\n    |data_upper               |\\n    +-------------------------+\\n    |{BAR -> 2.0, FOO -> -2.0}|\\n    +-------------------------+\\n    \"\"\"\\n    return _invoke_higher_order_function(\"TransformKeys\", [col], [f])\\n\\n\\ndef transform_values(col, f):\\n    \"\"\"\\n    Applies a function to every key-value pair in a map and returns\\n    a map with the results of those applications as the new values for the pairs.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    f : function\\n        a binary function ``(k: Column, v: Column) -> Column...``\\n        Can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n        Python ``UserDefinedFunctions`` are not supported\\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, {\"IT\": 10.0, \"SALES\": 2.0, \"OPS\": 24.0})], (\"id\", \"data\"))\\n    >>> df.select(transform_values(\\n    ...     \"data\", lambda k, v: when(k.isin(\"IT\", \"OPS\"), v + 10.0).otherwise(v)\\n    ... ).alias(\"new_data\")).show(truncate=False)\\n    +---------------------------------------+\\n    |new_data                               |\\n    +---------------------------------------+\\n    |{OPS -> 34.0, IT -> 20.0, SALES -> 2.0}|\\n    +---------------------------------------+\\n    \"\"\"\\n    return _invoke_higher_order_function(\"TransformValues\", [col], [f])\\n\\n\\ndef map_filter(col, f):\\n    \"\"\"\\n    Returns a map whose key-value pairs satisfy a predicate.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col : :class:`~pyspark.sql.Column` or str\\n        name of column or expression\\n    f : function\\n        a binary function ``(k: Column, v: Column) -> Column...``\\n        Can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n        Python ``UserDefinedFunctions`` are not supported\\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\\n    >>> df.select(map_filter(\\n    ...     \"data\", lambda _, v: v > 30.0).alias(\"data_filtered\")\\n    ... ).show(truncate=False)\\n    +--------------------------+\\n    |data_filtered             |\\n    +--------------------------+\\n    |{baz -> 32.0, foo -> 42.0}|\\n    +--------------------------+\\n    \"\"\"\\n    return _invoke_higher_order_function(\"MapFilter\", [col], [f])\\n\\n\\ndef map_zip_with(col1, col2, f):\\n    \"\"\"\\n    Merge two given maps, key-wise into a single map using a function.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Parameters\\n    ----------\\n    col1 : :class:`~pyspark.sql.Column` or str\\n        name of the first column or expression\\n    col2 : :class:`~pyspark.sql.Column` or str\\n        name of the second column or expression\\n    f : function\\n        a ternary function ``(k: Column, v1: Column, v2: Column) -> Column...``\\n        Can use methods of :class:`~pyspark.sql.Column`, functions defined in\\n        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\\n        Python ``UserDefinedFunctions`` are not supported\\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\\n\\n    Returns\\n    -------\\n    :class:`~pyspark.sql.Column`\\n\\n    Examples\\n    --------\\n    >>> df = spark.createDataFrame([\\n    ...     (1, {\"IT\": 24.0, \"SALES\": 12.00}, {\"IT\": 2.0, \"SALES\": 1.4})],\\n    ...     (\"id\", \"base\", \"ratio\")\\n    ... )\\n    >>> df.select(map_zip_with(\\n    ...     \"base\", \"ratio\", lambda k, v1, v2: round(v1 * v2, 2)).alias(\"updated_data\")\\n    ... ).show(truncate=False)\\n    +---------------------------+\\n    |updated_data               |\\n    +---------------------------+\\n    |{SALES -> 16.8, IT -> 48.0}|\\n    +---------------------------+\\n    \"\"\"\\n    return _invoke_higher_order_function(\"MapZipWith\", [col1, col2], [f])\\n\\n\\n# ---------------------- Partition transform functions --------------------------------\\n\\ndef years(col):\\n    \"\"\"\\n    Partition transform function: A transform for timestamps and dates\\n    to partition data into years.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Examples\\n    --------\\n    >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\\n    ...     years(\"ts\")\\n    ... ).createOrReplace()\\n\\n    Notes\\n    -----\\n    This function can be used only in combination with\\n    :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\\n    method of the `DataFrameWriterV2`.\\n\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.years(_to_java_column(col)))\\n\\n\\ndef months(col):\\n    \"\"\"\\n    Partition transform function: A transform for timestamps and dates\\n    to partition data into months.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Examples\\n    --------\\n    >>> df.writeTo(\"catalog.db.table\").partitionedBy(\\n    ...     months(\"ts\")\\n    ... ).createOrReplace()  # doctest: +SKIP\\n\\n    Notes\\n    -----\\n    This function can be used only in combination with\\n    :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\\n    method of the `DataFrameWriterV2`.\\n\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.months(_to_java_column(col)))\\n\\n\\ndef days(col):\\n    \"\"\"\\n    Partition transform function: A transform for timestamps and dates\\n    to partition data into days.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Examples\\n    --------\\n    >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\\n    ...     days(\"ts\")\\n    ... ).createOrReplace()\\n\\n    Notes\\n    -----\\n    This function can be used only in combination with\\n    :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\\n    method of the `DataFrameWriterV2`.\\n\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.days(_to_java_column(col)))\\n\\n\\ndef hours(col):\\n    \"\"\"\\n    Partition transform function: A transform for timestamps\\n    to partition data into hours.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Examples\\n    --------\\n    >>> df.writeTo(\"catalog.db.table\").partitionedBy(   # doctest: +SKIP\\n    ...     hours(\"ts\")\\n    ... ).createOrReplace()\\n\\n    Notes\\n    -----\\n    This function can be used only in combination with\\n    :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\\n    method of the `DataFrameWriterV2`.\\n\\n    \"\"\"\\n    sc = SparkContext._active_spark_context\\n    return Column(sc._jvm.functions.hours(_to_java_column(col)))\\n\\n\\ndef bucket(numBuckets, col):\\n    \"\"\"\\n    Partition transform function: A transform for any type that partitions\\n    by a hash of the input column.\\n\\n    .. versionadded:: 3.1.0\\n\\n    Examples\\n    --------\\n    >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\\n    ...     bucket(42, \"ts\")\\n    ... ).createOrReplace()\\n\\n    Notes\\n    -----\\n    This function can be used only in combination with\\n    :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\\n    method of the `DataFrameWriterV2`.\\n\\n    \"\"\"\\n    if not isinstance(numBuckets, (int, Column)):\\n        raise TypeError(\\n            \"numBuckets should be a Column or an int, got {}\".format(type(numBuckets))\\n        )\\n\\n    sc = SparkContext._active_spark_context\\n    numBuckets = (\\n        _create_column_from_literal(numBuckets)\\n        if isinstance(numBuckets, int)\\n        else _to_java_column(numBuckets)\\n    )\\n    return Column(sc._jvm.functions.bucket(numBuckets, _to_java_column(col)))\\n\\n\\n# ---------------------------- User Defined Function ----------------------------------\\n\\ndef udf(f=None, returnType=StringType()):\\n    \"\"\"Creates a user defined function (UDF).\\n\\n    .. versionadded:: 1.3.0\\n\\n    Parameters\\n    ----------\\n    f : function\\n        python function if used as a standalone function\\n    returnType : :class:`pyspark.sql.types.DataType` or str\\n        the return type of the user-defined function. The value can be either a\\n        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\\n\\n    Examples\\n    --------\\n    >>> from pyspark.sql.types import IntegerType\\n    >>> slen = udf(lambda s: len(s), IntegerType())\\n    >>> @udf\\n    ... def to_upper(s):\\n    ...     if s is not None:\\n    ...         return s.upper()\\n    ...\\n    >>> @udf(returnType=IntegerType())\\n    ... def add_one(x):\\n    ...     if x is not None:\\n    ...         return x + 1\\n    ...\\n    >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\\n    >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\\n    +----------+--------------+------------+\\n    |slen(name)|to_upper(name)|add_one(age)|\\n    +----------+--------------+------------+\\n    |         8|      JOHN DOE|          22|\\n    +----------+--------------+------------+\\n\\n    Notes\\n    -----\\n    The user-defined functions are considered deterministic by default. Due to\\n    optimization, duplicate invocations may be eliminated or the function may even be invoked\\n    more times than it is present in the query. If your function is not deterministic, call\\n    `asNondeterministic` on the user defined function. E.g.:\\n\\n    >>> from pyspark.sql.types import IntegerType\\n    >>> import random\\n    >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\\n\\n    The user-defined functions do not support conditional expressions or short circuiting\\n    in boolean expressions and it ends up with being executed all internally. If the functions\\n    can fail on special rows, the workaround is to incorporate the condition into the functions.\\n\\n    The user-defined functions do not take keyword arguments on the calling side.\\n    \"\"\"\\n\\n    # The following table shows most of Python data and SQL type conversions in normal UDFs that\\n    # are not yet visible to the user. Some of behaviors are buggy and might be changed in the near\\n    # future. The table might have to be eventually documented externally.\\n    # Please see SPARK-28131\\'s PR to see the codes in order to generate the table below.\\n    #\\n    # +-----------------------------+--------------+----------+------+---------------+--------------------+-----------------------------+----------+----------------------+---------+--------------------+----------------------------+------------+--------------+------------------+----------------------+  # noqa\\n    # |SQL Type \\\\ Python Value(Type)|None(NoneType)|True(bool)|1(int)|         a(str)|    1970-01-01(date)|1970-01-01 00:00:00(datetime)|1.0(float)|array(\\'i\\', [1])(array)|[1](list)|         (1,)(tuple)|bytearray(b\\'ABC\\')(bytearray)|  1(Decimal)|{\\'a\\': 1}(dict)|Row(kwargs=1)(Row)|Row(namedtuple=1)(Row)|  # noqa\\n    # +-----------------------------+--------------+----------+------+---------------+--------------------+-----------------------------+----------+----------------------+---------+--------------------+----------------------------+------------+--------------+------------------+----------------------+  # noqa\\n    # |                      boolean|          None|      True|  None|           None|                None|                         None|      None|                  None|     None|                None|                        None|        None|          None|                 X|                     X|  # noqa\\n    # |                      tinyint|          None|      None|     1|           None|                None|                         None|      None|                  None|     None|                None|                        None|        None|          None|                 X|                     X|  # noqa\\n    # |                     smallint|          None|      None|     1|           None|                None|                         None|      None|                  None|     None|                None|                        None|        None|          None|                 X|                     X|  # noqa\\n    # |                          int|          None|      None|     1|           None|                None|                         None|      None|                  None|     None|                None|                        None|        None|          None|                 X|                     X|  # noqa\\n    # |                       bigint|          None|      None|     1|           None|                None|                         None|      None|                  None|     None|                None|                        None|        None|          None|                 X|                     X|  # noqa\\n    # |                       string|          None|    \\'true\\'|   \\'1\\'|            \\'a\\'|\\'java.util.Gregor...|         \\'java.util.Gregor...|     \\'1.0\\'|         \\'[I@66cbb73a\\'|    \\'[1]\\'|\\'[Ljava.lang.Obje...|               \\'[B@5a51eb1a\\'|         \\'1\\'|       \\'{a=1}\\'|                 X|                     X|  # noqa\\n    # |                         date|          None|         X|     X|              X|datetime.date(197...|         datetime.date(197...|         X|                     X|        X|                   X|                           X|           X|             X|                 X|                     X|  # noqa\\n    # |                    timestamp|          None|         X|     X|              X|                   X|         datetime.datetime...|         X|                     X|        X|                   X|                           X|           X|             X|                 X|                     X|  # noqa\\n    # |                        float|          None|      None|  None|           None|                None|                         None|       1.0|                  None|     None|                None|                        None|        None|          None|                 X|                     X|  # noqa\\n    # |                       double|          None|      None|  None|           None|                None|                         None|       1.0|                  None|     None|                None|                        None|        None|          None|                 X|                     X|  # noqa\\n    # |                   array<int>|          None|      None|  None|           None|                None|                         None|      None|                   [1]|      [1]|                 [1]|                [65, 66, 67]|        None|          None|                 X|                     X|  # noqa\\n    # |                       binary|          None|      None|  None|bytearray(b\\'a\\')|                None|                         None|      None|                  None|     None|                None|           bytearray(b\\'ABC\\')|        None|          None|                 X|                     X|  # noqa\\n    # |                decimal(10,0)|          None|      None|  None|           None|                None|                         None|      None|                  None|     None|                None|                        None|Decimal(\\'1\\')|          None|                 X|                     X|  # noqa\\n    # |              map<string,int>|          None|      None|  None|           None|                None|                         None|      None|                  None|     None|                None|                        None|        None|      {\\'a\\': 1}|                 X|                     X|  # noqa\\n    # |               struct<_1:int>|          None|         X|     X|              X|                   X|                            X|         X|                     X|Row(_1=1)|           Row(_1=1)|                           X|           X|  Row(_1=None)|         Row(_1=1)|             Row(_1=1)|  # noqa\\n    # +-----------------------------+--------------+----------+------+---------------+--------------------+-----------------------------+----------+----------------------+---------+--------------------+----------------------------+------------+--------------+------------------+----------------------+  # noqa\\n    #\\n    # Note: DDL formatted string is used for \\'SQL Type\\' for simplicity. This string can be\\n    #       used in `returnType`.\\n    # Note: The values inside of the table are generated by `repr`.\\n    # Note: \\'X\\' means it throws an exception during the conversion.\\n    # Note: Python 3.7.3 is used.\\n\\n    # decorator @udf, @udf(), @udf(dataType())\\n    if f is None or isinstance(f, (str, DataType)):\\n        # If DataType has been passed as a positional argument\\n        # for decorator use it as a returnType\\n        return_type = f or returnType\\n        return functools.partial(_create_udf, returnType=return_type,\\n                                 evalType=PythonEvalType.SQL_BATCHED_UDF)\\n    else:\\n        return _create_udf(f=f, returnType=returnType,\\n                           evalType=PythonEvalType.SQL_BATCHED_UDF)\\n\\n\\ndef _test():\\n    import doctest\\n    from pyspark.sql import Row, SparkSession\\n    import pyspark.sql.functions\\n    globs = pyspark.sql.functions.__dict__.copy()\\n    spark = SparkSession.builder\\\\\\n        .master(\"local[4]\")\\\\\\n        .appName(\"sql.functions tests\")\\\\\\n        .getOrCreate()\\n    sc = spark.sparkContext\\n    globs[\\'sc\\'] = sc\\n    globs[\\'spark\\'] = spark\\n    globs[\\'df\\'] = spark.createDataFrame([Row(age=2, name=\\'Alice\\'), Row(age=5, name=\\'Bob\\')])\\n    (failure_count, test_count) = doctest.testmod(\\n        pyspark.sql.functions, globs=globs,\\n        optionflags=doctest.ELLIPSIS | doctest.NORMALIZE_WHITESPACE)\\n    spark.stop()\\n    if failure_count:\\n        sys.exit(-1)\\n\\n\\nif __name__ == \"__main__\":\\n    _test()\\n', 'from matplotlib import pyplot\\nfrom os import listdir\\n\\n\\ndef is_numeric(x):\\n    try:\\n        float(x)\\n    except ValueError:\\n        return False\\n    return True\\n    \\n    \\navg_lap_time = {}\\navg_pos = {}\\navg_speed = {}\\navg_top = {}\\ntotal_rescued = {}\\n        \\ntests = len(listdir(\\'../../batch\\'))-1    \\nfor file in listdir(\\'../../batch\\'):\\n    if (file == \\'.DS_Store\\'):\\n        continue\\n    f = open(\\'../../batch/\\'+file,\\'r\\')\\n    \\n    \\n    \\'\\'\\'\\n    name_index = file.find(\\'.\\')\\n    kart_name = str(file[:name_index])\\n    first = file.find(\\'.\\',name_index+1)\\n    track_name = file[name_index+1:first]\\n    second = file.find(\\'.\\',first+1)\\n    run = int(file[first+1:second])\\n    \\'\\'\\'\\n    track_name = \"snowmountain\"\\n    kart_names = [\"gnu\", \"sara\", \"tux\", \"elephpant\"]\\n    \\n    if track_name == \"snowmountain\":\\n        contents = f.readlines()\\n        \\'\\'\\'\\n        contents = contents[2:contents.index(\"[debug  ] profile: \\\\n\")-1]\\n        content = [s for s in contents if kart_name in s]\\n        data = [float(x) for x in content[0].split() if is_numeric(x)]\\n        if kart_name not in avg_lap_time:\\n            avg_lap_time[kart_name] = []\\n            avg_pos[kart_name] = []\\n            avg_speed[kart_name] = []\\n            avg_top[kart_name] = []\\n            total_rescued[kart_name] = []\\n\\n            avg_lap_time[kart_name].append(data[2]/4)\\n            avg_pos[kart_name].append(data[1])\\n            avg_speed[kart_name].append(data[3])\\n            avg_top[kart_name].append(data[4])\\n            total_rescued[kart_name].append(data[7])\\n        \\'\\'\\'\\n        \\n        contents = contents[2:6] #TODO check if all is in here\\n        for kart in kart_names:\\n            content = [s for s in contents if kart in s]\\n            data = [float(x) for x in content[0].split() if is_numeric(x)]\\n            if kart not in avg_lap_time:\\n                avg_lap_time[kart] = []\\n                avg_pos[kart] = []\\n                avg_speed[kart] = []\\n                avg_top[kart] = []\\n                total_rescued[kart] = []\\n    \\n            avg_lap_time[kart].append(data[2]/4)\\n            avg_pos[kart].append(data[1])\\n            avg_speed[kart].append(data[3])\\n            avg_top[kart].append(data[4])\\n            total_rescued[kart].append(data[7])\\n\\ntests = len(avg_lap_time[\"gnu\"])        \\nprint total_rescued\\n\\n\\nfor kart in kart_names:\\n    print \"rescues for \", kart , \": \", sum(total_rescued[kart])/tests\\n    print \"avg_lap_time for \" , kart , \": \" , sum(avg_lap_time[kart])/tests\\n    print \"avg_pos for \" , kart , \": \" , sum(avg_pos[kart])/tests\\n    print \"avg_speed for \" , kart , \": \" , sum(avg_speed[kart])/tests\\n    print \"avg_top for \" , kart , \": \" , sum(avg_top[kart])/tests\\n    \\n\\npyplot.subplot(2,2,1)\\npyplot.plot(list(xrange(tests)),avg_pos[\"gnu\"], \"b-\")\\npyplot.xlabel(\"tests\")\\npyplot.ylabel(\"gnu\")\\npyplot.subplot(2,2,2)\\npyplot.plot(list(xrange(tests)),avg_pos[\"sara\"], \"r-\")\\npyplot.xlabel(\"tests\")\\npyplot.ylabel(\"sara\")\\npyplot.subplot(2,2,3)\\npyplot.plot(list(xrange(tests)),avg_pos[\"elephpant\"], \"y-\")\\npyplot.xlabel(\"tests\")\\npyplot.ylabel(\"elephpant\")\\npyplot.subplot(2,2,4)\\npyplot.plot(list(xrange(tests)),avg_pos[\"tux\"], \"g-\")\\npyplot.xlabel(\"tests\")\\npyplot.ylabel(\"tux\")\\n\\npyplot.show()\\n', '\"\"\"Principal Component Analysis Base Classes\"\"\"\\n\\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\\n#         Olivier Grisel <olivier.grisel@ensta.org>\\n#         Mathieu Blondel <mathieu@mblondel.org>\\n#         Denis A. Engemann <d.engemann@fz-juelich.de>\\n#         Kyle Kastner <kastnerkyle@gmail.com>\\n#\\n# License: BSD 3 clause\\n\\nimport numpy as np\\nfrom scipy import linalg\\n\\nfrom ..base import BaseEstimator, TransformerMixin\\nfrom ..utils import check_array\\nfrom ..utils.extmath import fast_dot\\nfrom ..utils.validation import check_is_fitted\\nfrom ..externals import six\\nfrom abc import ABCMeta, abstractmethod\\n\\n\\nclass _BasePCA(six.with_metaclass(ABCMeta, BaseEstimator, TransformerMixin)):\\n    \"\"\"Base class for PCA methods.\\n\\n    Warning: This class should not be used directly.\\n    Use derived classes instead.\\n    \"\"\"\\n    def get_covariance(self):\\n        \"\"\"Compute data covariance with the generative model.\\n\\n        ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\\n        where  S**2 contains the explained variances, and sigma2 contains the\\n        noise variances.\\n\\n        Returns\\n        -------\\n        cov : array, shape=(n_features, n_features)\\n            Estimated covariance of data.\\n        \"\"\"\\n        components_ = self.components_\\n        exp_var = self.explained_variance_\\n        if self.whiten:\\n            components_ = components_ * np.sqrt(exp_var[:, np.newaxis])\\n        exp_var_diff = np.maximum(exp_var - self.noise_variance_, 0.)\\n        cov = np.dot(components_.T * exp_var_diff, components_)\\n        cov.flat[::len(cov) + 1] += self.noise_variance_  # modify diag inplace\\n        return cov\\n\\n    def get_precision(self):\\n        \"\"\"Compute data precision matrix with the generative model.\\n\\n        Equals the inverse of the covariance but computed with\\n        the matrix inversion lemma for efficiency.\\n\\n        Returns\\n        -------\\n        precision : array, shape=(n_features, n_features)\\n            Estimated precision of data.\\n        \"\"\"\\n        n_features = self.components_.shape[1]\\n\\n        # handle corner cases first\\n        if self.n_components_ == 0:\\n            return np.eye(n_features) / self.noise_variance_\\n        if self.n_components_ == n_features:\\n            return linalg.inv(self.get_covariance())\\n\\n        # Get precision using matrix inversion lemma\\n        components_ = self.components_\\n        exp_var = self.explained_variance_\\n        if self.whiten:\\n            components_ = components_ * np.sqrt(exp_var[:, np.newaxis])\\n        exp_var_diff = np.maximum(exp_var - self.noise_variance_, 0.)\\n        precision = np.dot(components_, components_.T) / self.noise_variance_\\n        precision.flat[::len(precision) + 1] += 1. / exp_var_diff\\n        precision = np.dot(components_.T,\\n                           np.dot(linalg.inv(precision), components_))\\n        precision /= -(self.noise_variance_ ** 2)\\n        precision.flat[::len(precision) + 1] += 1. / self.noise_variance_\\n        return precision\\n\\n    @abstractmethod\\n    def fit(X, y=None):\\n        \"\"\"Placeholder for fit. Subclasses should implement this method!\\n\\n        Fit the model with X.\\n\\n        Parameters\\n        ----------\\n        X : array-like, shape (n_samples, n_features)\\n            Training data, where n_samples is the number of samples and\\n            n_features is the number of features.\\n\\n        Returns\\n        -------\\n        self : object\\n            Returns the instance itself.\\n        \"\"\"\\n\\n\\n    def transform(self, X, y=None):\\n        \"\"\"Apply dimensionality reduction to X.\\n\\n        X is projected on the first principal components previously extracted\\n        from a training set.\\n\\n        Parameters\\n        ----------\\n        X : array-like, shape (n_samples, n_features)\\n            New data, where n_samples is the number of samples\\n            and n_features is the number of features.\\n\\n        Returns\\n        -------\\n        X_new : array-like, shape (n_samples, n_components)\\n\\n        Examples\\n        --------\\n\\n        >>> import numpy as np\\n        >>> from sklearn.decomposition import IncrementalPCA\\n        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\n        >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\\n        >>> ipca.fit(X)\\n        IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False)\\n        >>> ipca.transform(X) # doctest: +SKIP\\n        \"\"\"\\n        check_is_fitted(self, [\\'mean_\\', \\'components_\\'], all_or_any=all)\\n\\n        X = check_array(X)\\n        if self.mean_ is not None:\\n            X = X - self.mean_\\n        X_transformed = fast_dot(X, self.components_.T)\\n        if self.whiten:\\n            X_transformed /= np.sqrt(self.explained_variance_)\\n        return X_transformed\\n\\n    def inverse_transform(self, X, y=None):\\n        \"\"\"Transform data back to its original space.\\n\\n        In other words, return an input X_original whose transform would be X.\\n\\n        Parameters\\n        ----------\\n        X : array-like, shape (n_samples, n_components)\\n            New data, where n_samples is the number of samples\\n            and n_components is the number of components.\\n\\n        Returns\\n        -------\\n        X_original array-like, shape (n_samples, n_features)\\n\\n        Notes\\n        -----\\n        If whitening is enabled, inverse_transform will compute the\\n        exact inverse operation, which includes reversing whitening.\\n        \"\"\"\\n        if self.whiten:\\n            return fast_dot(X, np.sqrt(self.explained_variance_[:, np.newaxis]) *\\n                            self.components_) + self.mean_\\n        else:\\n            return fast_dot(X, self.components_) + self.mean_\\n', '\"\"\"\\nTesting for the partial dependence module.\\n\"\"\"\\n\\nimport numpy as np\\nfrom numpy.testing import assert_array_equal\\n\\nfrom sklearn.utils.testing import assert_raises\\nfrom sklearn.utils.testing import if_matplotlib\\nfrom sklearn.ensemble.partial_dependence import partial_dependence\\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn import datasets\\n\\n\\n# toy sample\\nX = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\\ny = [-1, -1, -1, 1, 1, 1]\\nT = [[-1, -1], [2, 2], [3, 2]]\\ntrue_result = [-1, 1, 1]\\n\\n# also load the boston dataset\\nboston = datasets.load_boston()\\n\\n# also load the iris dataset\\niris = datasets.load_iris()\\n\\n\\ndef test_partial_dependence_classifier():\\n    # Test partial dependence for classifier\\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\\n    clf.fit(X, y)\\n\\n    pdp, axes = partial_dependence(clf, [0], X=X, grid_resolution=5)\\n\\n    # only 4 grid points instead of 5 because only 4 unique X[:,0] vals\\n    assert pdp.shape == (1, 4)\\n    assert axes[0].shape[0] == 4\\n\\n    # now with our own grid\\n    X_ = np.asarray(X)\\n    grid = np.unique(X_[:, 0])\\n    pdp_2, axes = partial_dependence(clf, [0], grid=grid)\\n\\n    assert axes is None\\n    assert_array_equal(pdp, pdp_2)\\n\\n\\ndef test_partial_dependence_multiclass():\\n    # Test partial dependence for multi-class classifier\\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\\n    clf.fit(iris.data, iris.target)\\n\\n    grid_resolution = 25\\n    n_classes = clf.n_classes_\\n    pdp, axes = partial_dependence(\\n        clf, [0], X=iris.data, grid_resolution=grid_resolution)\\n\\n    assert pdp.shape == (n_classes, grid_resolution)\\n    assert len(axes) == 1\\n    assert axes[0].shape[0] == grid_resolution\\n\\n\\ndef test_partial_dependence_regressor():\\n    # Test partial dependence for regressor\\n    clf = GradientBoostingRegressor(n_estimators=10, random_state=1)\\n    clf.fit(boston.data, boston.target)\\n\\n    grid_resolution = 25\\n    pdp, axes = partial_dependence(\\n        clf, [0], X=boston.data, grid_resolution=grid_resolution)\\n\\n    assert pdp.shape == (1, grid_resolution)\\n    assert axes[0].shape[0] == grid_resolution\\n\\n\\ndef test_partial_dependecy_input():\\n    # Test input validation of partial dependence.\\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\\n    clf.fit(X, y)\\n\\n    assert_raises(ValueError, partial_dependence,\\n                  clf, [0], grid=None, X=None)\\n\\n    assert_raises(ValueError, partial_dependence,\\n                  clf, [0], grid=[0, 1], X=X)\\n\\n    # first argument must be an instance of BaseGradientBoosting\\n    assert_raises(ValueError, partial_dependence,\\n                  {}, [0], X=X)\\n\\n    # Gradient boosting estimator must be fit\\n    assert_raises(ValueError, partial_dependence,\\n                  GradientBoostingClassifier(), [0], X=X)\\n\\n    assert_raises(ValueError, partial_dependence, clf, [-1], X=X)\\n\\n    assert_raises(ValueError, partial_dependence, clf, [100], X=X)\\n\\n    # wrong ndim for grid\\n    grid = np.random.rand(10, 2, 1)\\n    assert_raises(ValueError, partial_dependence, clf, [0], grid=grid)\\n\\n\\n@if_matplotlib\\ndef test_plot_partial_dependence():\\n    # Test partial dependence plot function.\\n    clf = GradientBoostingRegressor(n_estimators=10, random_state=1)\\n    clf.fit(boston.data, boston.target)\\n\\n    grid_resolution = 25\\n    fig, axs = plot_partial_dependence(clf, boston.data, [0, 1, (0, 1)],\\n                                       grid_resolution=grid_resolution,\\n                                       feature_names=boston.feature_names)\\n    assert len(axs) == 3\\n    assert all(ax.has_data for ax in axs)\\n\\n    # check with str features and array feature names\\n    fig, axs = plot_partial_dependence(clf, boston.data, [\\'CRIM\\', \\'ZN\\',\\n                                                          (\\'CRIM\\', \\'ZN\\')],\\n                                       grid_resolution=grid_resolution,\\n                                       feature_names=boston.feature_names)\\n\\n    assert len(axs) == 3\\n    assert all(ax.has_data for ax in axs)\\n\\n    # check with list feature_names\\n    feature_names = boston.feature_names.tolist()\\n    fig, axs = plot_partial_dependence(clf, boston.data, [\\'CRIM\\', \\'ZN\\',\\n                                                          (\\'CRIM\\', \\'ZN\\')],\\n                                       grid_resolution=grid_resolution,\\n                                       feature_names=feature_names)\\n    assert len(axs) == 3\\n    assert all(ax.has_data for ax in axs)\\n\\n\\n@if_matplotlib\\ndef test_plot_partial_dependence_input():\\n    # Test partial dependence plot function input checks.\\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\\n\\n    # not fitted yet\\n    assert_raises(ValueError, plot_partial_dependence,\\n                  clf, X, [0])\\n\\n    clf.fit(X, y)\\n\\n    assert_raises(ValueError, plot_partial_dependence,\\n                  clf, np.array(X)[:, :0], [0])\\n\\n    # first argument must be an instance of BaseGradientBoosting\\n    assert_raises(ValueError, plot_partial_dependence,\\n                  {}, X, [0])\\n\\n    # must be larger than -1\\n    assert_raises(ValueError, plot_partial_dependence,\\n                  clf, X, [-1])\\n\\n    # too large feature value\\n    assert_raises(ValueError, plot_partial_dependence,\\n                  clf, X, [100])\\n\\n    # str feature but no feature_names\\n    assert_raises(ValueError, plot_partial_dependence,\\n                  clf, X, [\\'foobar\\'])\\n\\n    # not valid features value\\n    assert_raises(ValueError, plot_partial_dependence,\\n                  clf, X, [{\\'foo\\': \\'bar\\'}])\\n\\n\\n@if_matplotlib\\ndef test_plot_partial_dependence_multiclass():\\n    # Test partial dependence plot function on multi-class input.\\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\\n    clf.fit(iris.data, iris.target)\\n\\n    grid_resolution = 25\\n    fig, axs = plot_partial_dependence(clf, iris.data, [0, 1],\\n                                       label=0,\\n                                       grid_resolution=grid_resolution)\\n    assert len(axs) == 2\\n    assert all(ax.has_data for ax in axs)\\n\\n    # now with symbol labels\\n    target = iris.target_names[iris.target]\\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\\n    clf.fit(iris.data, target)\\n\\n    grid_resolution = 25\\n    fig, axs = plot_partial_dependence(clf, iris.data, [0, 1],\\n                                       label=\\'setosa\\',\\n                                       grid_resolution=grid_resolution)\\n    assert len(axs) == 2\\n    assert all(ax.has_data for ax in axs)\\n\\n    # label not in gbrt.classes_\\n    assert_raises(ValueError, plot_partial_dependence,\\n                  clf, iris.data, [0, 1], label=\\'foobar\\',\\n                  grid_resolution=grid_resolution)\\n\\n    # label not provided\\n    assert_raises(ValueError, plot_partial_dependence,\\n                  clf, iris.data, [0, 1],\\n                  grid_resolution=grid_resolution)\\n', '\"\"\"\\nMatplotlib Exporter\\n===================\\nThis submodule contains tools for crawling a matplotlib figure and exporting\\nrelevant pieces to a renderer.\\n\"\"\"\\nimport warnings\\nimport io\\nfrom . import utils\\n\\nimport matplotlib\\nfrom matplotlib import transforms\\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\\n\\nclass Exporter(object):\\n    \"\"\"Matplotlib Exporter\\n\\n    Parameters\\n    ----------\\n    renderer : Renderer object\\n        The renderer object called by the exporter to create a figure\\n        visualization.  See mplexporter.Renderer for information on the\\n        methods which should be defined within the renderer.\\n    close_mpl : bool\\n        If True (default), close the matplotlib figure as it is rendered. This\\n        is useful for when the exporter is used within the notebook, or with\\n        an interactive matplotlib backend.\\n    \"\"\"\\n\\n    def __init__(self, renderer, close_mpl=True):\\n        self.close_mpl = close_mpl\\n        self.renderer = renderer\\n\\n    def run(self, fig):\\n        \"\"\"\\n        Run the exporter on the given figure\\n\\n        Parmeters\\n        ---------\\n        fig : matplotlib.Figure instance\\n            The figure to export\\n        \"\"\"\\n        # Calling savefig executes the draw() command, putting elements\\n        # in the correct place.\\n        if fig.canvas is None:\\n            fig.canvas = FigureCanvasAgg(fig)\\n        fig.savefig(io.BytesIO(), format=\\'png\\', dpi=fig.dpi)\\n        if self.close_mpl:\\n            import matplotlib.pyplot as plt\\n            plt.close(fig)\\n        self.crawl_fig(fig)\\n\\n    @staticmethod\\n    def process_transform(transform, ax=None, data=None, return_trans=False,\\n                          force_trans=None):\\n        \"\"\"Process the transform and convert data to figure or data coordinates\\n\\n        Parameters\\n        ----------\\n        transform : matplotlib Transform object\\n            The transform applied to the data\\n        ax : matplotlib Axes object (optional)\\n            The axes the data is associated with\\n        data : ndarray (optional)\\n            The array of data to be transformed.\\n        return_trans : bool (optional)\\n            If true, return the final transform of the data\\n        force_trans : matplotlib.transform instance (optional)\\n            If supplied, first force the data to this transform\\n\\n        Returns\\n        -------\\n        code : string\\n            Code is either \"data\", \"axes\", \"figure\", or \"display\", indicating\\n            the type of coordinates output.\\n        transform : matplotlib transform\\n            the transform used to map input data to output data.\\n            Returned only if return_trans is True\\n        new_data : ndarray\\n            Data transformed to match the given coordinate code.\\n            Returned only if data is specified\\n        \"\"\"\\n        if isinstance(transform, transforms.BlendedGenericTransform):\\n            warnings.warn(\"Blended transforms not yet supported. \"\\n                          \"Zoom behavior may not work as expected.\")\\n\\n        if force_trans is not None:\\n            if data is not None:\\n                data = (transform - force_trans).transform(data)\\n            transform = force_trans\\n\\n        code = \"display\"\\n        if ax is not None:\\n            for (c, trans) in [(\"data\", ax.transData),\\n                               (\"axes\", ax.transAxes),\\n                               (\"figure\", ax.figure.transFigure),\\n                               (\"display\", transforms.IdentityTransform())]:\\n                if transform.contains_branch(trans):\\n                    code, transform = (c, transform - trans)\\n                    break\\n\\n        if data is not None:\\n            if return_trans:\\n                return code, transform.transform(data), transform\\n            else:\\n                return code, transform.transform(data)\\n        else:\\n            if return_trans:\\n                return code, transform\\n            else:\\n                return code\\n\\n    def crawl_fig(self, fig):\\n        \"\"\"Crawl the figure and process all axes\"\"\"\\n        with self.renderer.draw_figure(fig=fig,\\n                                       props=utils.get_figure_properties(fig)):\\n            for ax in fig.axes:\\n                self.crawl_ax(ax)\\n\\n    def crawl_ax(self, ax):\\n        \"\"\"Crawl the axes and process all elements within\"\"\"\\n        with self.renderer.draw_axes(ax=ax,\\n                                     props=utils.get_axes_properties(ax)):\\n            for line in ax.lines:\\n                self.draw_line(ax, line)\\n            for text in ax.texts:\\n                self.draw_text(ax, text)\\n            for (text, ttp) in zip([ax.xaxis.label, ax.yaxis.label, ax.title],\\n                                   [\"xlabel\", \"ylabel\", \"title\"]):\\n                if(hasattr(text, \\'get_text\\') and text.get_text()):\\n                    self.draw_text(ax, text, force_trans=ax.transAxes,\\n                                   text_type=ttp)\\n            for artist in ax.artists:\\n                # TODO: process other artists\\n                if isinstance(artist, matplotlib.text.Text):\\n                    self.draw_text(ax, artist)\\n            for patch in ax.patches:\\n                self.draw_patch(ax, patch)\\n            for collection in ax.collections:\\n                self.draw_collection(ax, collection)\\n            for image in ax.images:\\n                self.draw_image(ax, image)\\n\\n            legend = ax.get_legend()\\n            if legend is not None:\\n                props = utils.get_legend_properties(ax, legend)\\n                with self.renderer.draw_legend(legend=legend, props=props):\\n                    if props[\\'visible\\']:\\n                        self.crawl_legend(ax, legend)\\n\\n    def crawl_legend(self, ax, legend):\\n        \"\"\"\\n        Recursively look through objects in legend children\\n        \"\"\"\\n        legendElements = list(utils.iter_all_children(legend._legend_box,\\n                                                      skipContainers=True))\\n        legendElements.append(legend.legendPatch)\\n        for child in legendElements:\\n            # force a large zorder so it appears on top\\n            child.set_zorder(1E6 + child.get_zorder())\\n\\n            try:\\n                # What kind of object...\\n                if isinstance(child, matplotlib.patches.Patch):\\n                    self.draw_patch(ax, child, force_trans=ax.transAxes)\\n                elif isinstance(child, matplotlib.text.Text):\\n                    if not (child is legend.get_children()[-1]\\n                            and child.get_text() == \\'None\\'):\\n                        self.draw_text(ax, child, force_trans=ax.transAxes)\\n                elif isinstance(child, matplotlib.lines.Line2D):\\n                    self.draw_line(ax, child, force_trans=ax.transAxes)\\n                elif isinstance(child, matplotlib.collections.Collection):\\n                    self.draw_collection(ax, child,\\n                                         force_pathtrans=ax.transAxes)\\n                else:\\n                    warnings.warn(\"Legend element %s not impemented\" % child)\\n            except NotImplementedError:\\n                warnings.warn(\"Legend element %s not impemented\" % child)\\n\\n    def draw_line(self, ax, line, force_trans=None):\\n        \"\"\"Process a matplotlib line and call renderer.draw_line\"\"\"\\n        coordinates, data = self.process_transform(line.get_transform(),\\n                                                   ax, line.get_xydata(),\\n                                                   force_trans=force_trans)\\n        linestyle = utils.get_line_style(line)\\n        if linestyle[\\'dasharray\\'] is None:\\n            linestyle = None\\n        markerstyle = utils.get_marker_style(line)\\n        if (markerstyle[\\'marker\\'] in [\\'None\\', \\'none\\', None]\\n                or markerstyle[\\'markerpath\\'][0].size == 0):\\n            markerstyle = None\\n        label = line.get_label()\\n        if markerstyle or linestyle:\\n            self.renderer.draw_marked_line(data=data, coordinates=coordinates,\\n                                           linestyle=linestyle,\\n                                           markerstyle=markerstyle,\\n                                           label=label,\\n                                           mplobj=line)\\n\\n    def draw_text(self, ax, text, force_trans=None, text_type=None):\\n        \"\"\"Process a matplotlib text object and call renderer.draw_text\"\"\"\\n        content = text.get_text()\\n        if content:\\n            transform = text.get_transform()\\n            position = text.get_position()\\n            coords, position = self.process_transform(transform, ax,\\n                                                      position,\\n                                                      force_trans=force_trans)\\n            style = utils.get_text_style(text)\\n            self.renderer.draw_text(text=content, position=position,\\n                                    coordinates=coords,\\n                                    text_type=text_type,\\n                                    style=style, mplobj=text)\\n\\n    def draw_patch(self, ax, patch, force_trans=None):\\n        \"\"\"Process a matplotlib patch object and call renderer.draw_path\"\"\"\\n        vertices, pathcodes = utils.SVG_path(patch.get_path())\\n        transform = patch.get_transform()\\n        coordinates, vertices = self.process_transform(transform,\\n                                                       ax, vertices,\\n                                                       force_trans=force_trans)\\n        linestyle = utils.get_path_style(patch, fill=patch.get_fill())\\n        self.renderer.draw_path(data=vertices,\\n                                coordinates=coordinates,\\n                                pathcodes=pathcodes,\\n                                style=linestyle,\\n                                mplobj=patch)\\n\\n    def draw_collection(self, ax, collection,\\n                        force_pathtrans=None,\\n                        force_offsettrans=None):\\n        \"\"\"Process a matplotlib collection and call renderer.draw_collection\"\"\"\\n        (transform, transOffset,\\n         offsets, paths) = collection._prepare_points()\\n\\n        offset_coords, offsets = self.process_transform(\\n            transOffset, ax, offsets, force_trans=force_offsettrans)\\n        path_coords = self.process_transform(\\n            transform, ax, force_trans=force_pathtrans)\\n\\n        processed_paths = [utils.SVG_path(path) for path in paths]\\n        processed_paths = [(self.process_transform(\\n            transform, ax, path[0], force_trans=force_pathtrans)[1], path[1])\\n                           for path in processed_paths]\\n\\n        path_transforms = collection.get_transforms()\\n        try:\\n            # matplotlib 1.3: path_transforms are transform objects.\\n            # Convert them to numpy arrays.\\n            path_transforms = [t.get_matrix() for t in path_transforms]\\n        except AttributeError:\\n            # matplotlib 1.4: path transforms are already numpy arrays.\\n            pass\\n\\n        styles = {\\'linewidth\\': collection.get_linewidths(),\\n                  \\'facecolor\\': collection.get_facecolors(),\\n                  \\'edgecolor\\': collection.get_edgecolors(),\\n                  \\'alpha\\': collection._alpha,\\n                  \\'zorder\\': collection.get_zorder()}\\n\\n        offset_dict = {\"data\": \"before\",\\n                       \"screen\": \"after\"}\\n        offset_order = offset_dict[collection.get_offset_position()]\\n\\n        self.renderer.draw_path_collection(paths=processed_paths,\\n                                           path_coordinates=path_coords,\\n                                           path_transforms=path_transforms,\\n                                           offsets=offsets,\\n                                           offset_coordinates=offset_coords,\\n                                           offset_order=offset_order,\\n                                           styles=styles,\\n                                           mplobj=collection)\\n\\n    def draw_image(self, ax, image):\\n        \"\"\"Process a matplotlib image object and call renderer.draw_image\"\"\"\\n        self.renderer.draw_image(imdata=utils.image_to_base64(image),\\n                                 extent=image.get_extent(),\\n                                 coordinates=\"data\",\\n                                 style={\"alpha\": image.get_alpha(),\\n                                        \"zorder\": image.get_zorder()},\\n                                 mplobj=image)\\n', '#!/usr/bin/env python\\n\"\"\"\\nAn example using networkx.Graph().\\n\\nmiles_graph() returns an undirected graph over the 128 US cities from\\nthe datafile miles_dat.txt. The cities each have location and population\\ndata.  The edges are labeled with the distance betwen the two cities.\\n\\nThis example is described in Section 1.1 in Knuth\\'s book [1,2].\\n\\nReferences.\\n-----------\\n\\n[1] Donald E. Knuth,\\n    \"The Stanford GraphBase: A Platform for Combinatorial Computing\",\\n    ACM Press, New York, 1993.\\n[2] http://www-cs-faculty.stanford.edu/~knuth/sgb.html\\n\\n\\n\"\"\"\\n__author__ = \"\"\"Aric Hagberg (hagberg@lanl.gov)\"\"\"\\n#    Copyright (C) 2004-2015 by \\n#    Aric Hagberg <hagberg@lanl.gov>\\n#    Dan Schult <dschult@colgate.edu>\\n#    Pieter Swart <swart@lanl.gov>\\n#    All rights reserved.\\n#    BSD license.\\n\\nimport networkx as nx\\n\\n\\ndef miles_graph():\\n    \"\"\" Return the cites example graph in miles_dat.txt\\n        from the Stanford GraphBase.\\n    \"\"\"\\n    # open file miles_dat.txt.gz (or miles_dat.txt)\\n    import gzip\\n    fh = gzip.open(\\'knuth_miles.txt.gz\\',\\'r\\')\\n\\n    G=nx.Graph()\\n    G.position={}\\n    G.population={}\\n\\n    cities=[]\\n    for line in fh.readlines():\\n        line = line.decode()\\n        if line.startswith(\"*\"): # skip comments\\n            continue\\n\\n        numfind=re.compile(\"^\\\\d+\") \\n\\n        if numfind.match(line): # this line is distances\\n            dist=line.split()\\n            for d in dist:\\n                G.add_edge(city,cities[i],weight=int(d))\\n                i=i+1\\n        else: # this line is a city, position, population\\n            i=1\\n            (city,coordpop)=line.split(\"[\")\\n            cities.insert(0,city)\\n            (coord,pop)=coordpop.split(\"]\")\\n            (y,x)=coord.split(\",\")\\n        \\n            G.add_node(city)\\n            # assign position - flip x axis for matplotlib, shift origin\\n            G.position[city]=(-int(x)+7500,int(y)-3000)\\n            G.population[city]=float(pop)/1000.0\\n    return G            \\n\\nif __name__ == \\'__main__\\':\\n    import networkx as nx\\n    import re\\n    import sys\\n\\n    G=miles_graph()\\n\\n    print(\"Loaded miles_dat.txt containing 128 cities.\")\\n    print(\"digraph has %d nodes with %d edges\"\\\\\\n          %(nx.number_of_nodes(G),nx.number_of_edges(G)))\\n\\n\\n    # make new graph of cites, edge if less then 300 miles between them\\n    H=nx.Graph()\\n    for v in G:\\n        H.add_node(v)\\n    for (u,v,d) in G.edges(data=True):\\n        if d[\\'weight\\'] < 300:\\n            H.add_edge(u,v)\\n\\n    # draw with matplotlib/pylab            \\n\\n    try:\\n        import matplotlib.pyplot as plt\\n        plt.figure(figsize=(8,8))\\n        # with nodes colored by degree sized by population\\n        node_color=[float(H.degree(v)) for v in H]\\n        nx.draw(H,G.position,\\n             node_size=[G.population[v] for v in H],\\n             node_color=node_color,\\n             with_labels=False)\\n\\n        # scale the axes equally\\n        plt.xlim(-5000,500)\\n        plt.ylim(-2000,3500)\\n\\n        plt.savefig(\"knuth_miles.png\")\\n    except:\\n        pass\\n\\n\\n\\n', '\"\"\"Restricted Boltzmann Machine\\n\"\"\"\\n\\n# Authors: Yann N. Dauphin <dauphiya@iro.umontreal.ca>\\n#          Vlad Niculae\\n#          Gabriel Synnaeve\\n#          Lars Buitinck\\n# License: BSD 3 clause\\n\\nimport time\\n\\nimport numpy as np\\nimport scipy.sparse as sp\\n\\nfrom ..base import BaseEstimator\\nfrom ..base import TransformerMixin\\nfrom ..externals.six.moves import xrange\\nfrom ..utils import check_array\\nfrom ..utils import check_random_state\\nfrom ..utils import gen_even_slices\\nfrom ..utils import issparse\\nfrom ..utils.extmath import safe_sparse_dot\\nfrom ..utils.extmath import log_logistic\\nfrom ..utils.fixes import expit             # logistic function\\nfrom ..utils.validation import check_is_fitted\\n\\n\\nclass BernoulliRBM(BaseEstimator, TransformerMixin):\\n    \"\"\"Bernoulli Restricted Boltzmann Machine (RBM).\\n\\n    A Restricted Boltzmann Machine with binary visible units and\\n    binary hidden units. Parameters are estimated using Stochastic Maximum\\n    Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)\\n    [2].\\n\\n    The time complexity of this implementation is ``O(d ** 2)`` assuming\\n    d ~ n_features ~ n_components.\\n\\n    Read more in the :ref:`User Guide <rbm>`.\\n\\n    Parameters\\n    ----------\\n    n_components : int, optional\\n        Number of binary hidden units.\\n\\n    learning_rate : float, optional\\n        The learning rate for weight updates. It is *highly* recommended\\n        to tune this hyper-parameter. Reasonable values are in the\\n        10**[0., -3.] range.\\n\\n    batch_size : int, optional\\n        Number of examples per minibatch.\\n\\n    n_iter : int, optional\\n        Number of iterations/sweeps over the training dataset to perform\\n        during training.\\n\\n    verbose : int, optional\\n        The verbosity level. The default, zero, means silent mode.\\n\\n    random_state : integer or numpy.RandomState, optional\\n        A random number generator instance to define the state of the\\n        random permutations generator. If an integer is given, it fixes the\\n        seed. Defaults to the global numpy random number generator.\\n\\n    Attributes\\n    ----------\\n    intercept_hidden_ : array-like, shape (n_components,)\\n        Biases of the hidden units.\\n\\n    intercept_visible_ : array-like, shape (n_features,)\\n        Biases of the visible units.\\n\\n    components_ : array-like, shape (n_components, n_features)\\n        Weight matrix, where n_features in the number of\\n        visible units and n_components is the number of hidden units.\\n\\n    Examples\\n    --------\\n\\n    >>> import numpy as np\\n    >>> from sklearn.neural_network import BernoulliRBM\\n    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\\n    >>> model = BernoulliRBM(n_components=2)\\n    >>> model.fit(X)\\n    BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\\n           random_state=None, verbose=0)\\n\\n    References\\n    ----------\\n\\n    [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for\\n        deep belief nets. Neural Computation 18, pp 1527-1554.\\n        http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\\n\\n    [2] Tieleman, T. Training Restricted Boltzmann Machines using\\n        Approximations to the Likelihood Gradient. International Conference\\n        on Machine Learning (ICML) 2008\\n    \"\"\"\\n    def __init__(self, n_components=256, learning_rate=0.1, batch_size=10,\\n                 n_iter=10, verbose=0, random_state=None):\\n        self.n_components = n_components\\n        self.learning_rate = learning_rate\\n        self.batch_size = batch_size\\n        self.n_iter = n_iter\\n        self.verbose = verbose\\n        self.random_state = random_state\\n\\n    def transform(self, X):\\n        \"\"\"Compute the hidden layer activation probabilities, P(h=1|v=X).\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix} shape (n_samples, n_features)\\n            The data to be transformed.\\n\\n        Returns\\n        -------\\n        h : array, shape (n_samples, n_components)\\n            Latent representations of the data.\\n        \"\"\"\\n        check_is_fitted(self, \"components_\")\\n\\n        X = check_array(X, accept_sparse=\\'csr\\', dtype=np.float64)\\n        return self._mean_hiddens(X)\\n\\n    def _mean_hiddens(self, v):\\n        \"\"\"Computes the probabilities P(h=1|v).\\n\\n        Parameters\\n        ----------\\n        v : array-like, shape (n_samples, n_features)\\n            Values of the visible layer.\\n\\n        Returns\\n        -------\\n        h : array-like, shape (n_samples, n_components)\\n            Corresponding mean field values for the hidden layer.\\n        \"\"\"\\n        p = safe_sparse_dot(v, self.components_.T)\\n        p += self.intercept_hidden_\\n        return expit(p, out=p)\\n\\n    def _sample_hiddens(self, v, rng):\\n        \"\"\"Sample from the distribution P(h|v).\\n\\n        Parameters\\n        ----------\\n        v : array-like, shape (n_samples, n_features)\\n            Values of the visible layer to sample from.\\n\\n        rng : RandomState\\n            Random number generator to use.\\n\\n        Returns\\n        -------\\n        h : array-like, shape (n_samples, n_components)\\n            Values of the hidden layer.\\n        \"\"\"\\n        p = self._mean_hiddens(v)\\n        return (rng.random_sample(size=p.shape) < p)\\n\\n    def _sample_visibles(self, h, rng):\\n        \"\"\"Sample from the distribution P(v|h).\\n\\n        Parameters\\n        ----------\\n        h : array-like, shape (n_samples, n_components)\\n            Values of the hidden layer to sample from.\\n\\n        rng : RandomState\\n            Random number generator to use.\\n\\n        Returns\\n        -------\\n        v : array-like, shape (n_samples, n_features)\\n            Values of the visible layer.\\n        \"\"\"\\n        p = np.dot(h, self.components_)\\n        p += self.intercept_visible_\\n        expit(p, out=p)\\n        return (rng.random_sample(size=p.shape) < p)\\n\\n    def _free_energy(self, v):\\n        \"\"\"Computes the free energy F(v) = - log sum_h exp(-E(v,h)).\\n\\n        Parameters\\n        ----------\\n        v : array-like, shape (n_samples, n_features)\\n            Values of the visible layer.\\n\\n        Returns\\n        -------\\n        free_energy : array-like, shape (n_samples,)\\n            The value of the free energy.\\n        \"\"\"\\n        return (- safe_sparse_dot(v, self.intercept_visible_)\\n                - np.logaddexp(0, safe_sparse_dot(v, self.components_.T)\\n                               + self.intercept_hidden_).sum(axis=1))\\n\\n    def gibbs(self, v):\\n        \"\"\"Perform one Gibbs sampling step.\\n\\n        Parameters\\n        ----------\\n        v : array-like, shape (n_samples, n_features)\\n            Values of the visible layer to start from.\\n\\n        Returns\\n        -------\\n        v_new : array-like, shape (n_samples, n_features)\\n            Values of the visible layer after one Gibbs step.\\n        \"\"\"\\n        check_is_fitted(self, \"components_\")\\n        if not hasattr(self, \"random_state_\"):\\n            self.random_state_ = check_random_state(self.random_state)\\n        h_ = self._sample_hiddens(v, self.random_state_)\\n        v_ = self._sample_visibles(h_, self.random_state_)\\n\\n        return v_\\n\\n    def partial_fit(self, X, y=None):\\n        \"\"\"Fit the model to the data X which should contain a partial\\n        segment of the data.\\n\\n        Parameters\\n        ----------\\n        X : array-like, shape (n_samples, n_features)\\n            Training data.\\n\\n        Returns\\n        -------\\n        self : BernoulliRBM\\n            The fitted model.\\n        \"\"\"\\n        X = check_array(X, accept_sparse=\\'csr\\', dtype=np.float64)\\n        if not hasattr(self, \\'random_state_\\'):\\n            self.random_state_ = check_random_state(self.random_state)\\n        if not hasattr(self, \\'components_\\'):\\n            self.components_ = np.asarray(\\n                self.random_state_.normal(\\n                    0,\\n                    0.01,\\n                    (self.n_components, X.shape[1])\\n                ),\\n                order=\\'F\\')\\n        if not hasattr(self, \\'intercept_hidden_\\'):\\n            self.intercept_hidden_ = np.zeros(self.n_components, )\\n        if not hasattr(self, \\'intercept_visible_\\'):\\n            self.intercept_visible_ = np.zeros(X.shape[1], )\\n        if not hasattr(self, \\'h_samples_\\'):\\n            self.h_samples_ = np.zeros((self.batch_size, self.n_components))\\n\\n        self._fit(X, self.random_state_)\\n\\n    def _fit(self, v_pos, rng):\\n        \"\"\"Inner fit for one mini-batch.\\n\\n        Adjust the parameters to maximize the likelihood of v using\\n        Stochastic Maximum Likelihood (SML).\\n\\n        Parameters\\n        ----------\\n        v_pos : array-like, shape (n_samples, n_features)\\n            The data to use for training.\\n\\n        rng : RandomState\\n            Random number generator to use for sampling.\\n        \"\"\"\\n        h_pos = self._mean_hiddens(v_pos)\\n        v_neg = self._sample_visibles(self.h_samples_, rng)\\n        h_neg = self._mean_hiddens(v_neg)\\n\\n        lr = float(self.learning_rate) / v_pos.shape[0]\\n        update = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T\\n        update -= np.dot(h_neg.T, v_neg)\\n        self.components_ += lr * update\\n        self.intercept_hidden_ += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\\n        self.intercept_visible_ += lr * (np.asarray(\\n                                         v_pos.sum(axis=0)).squeeze() -\\n                                         v_neg.sum(axis=0))\\n\\n        h_neg[rng.uniform(size=h_neg.shape) < h_neg] = 1.0  # sample binomial\\n        self.h_samples_ = np.floor(h_neg, h_neg)\\n\\n    def score_samples(self, X):\\n        \"\"\"Compute the pseudo-likelihood of X.\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix} shape (n_samples, n_features)\\n            Values of the visible layer. Must be all-boolean (not checked).\\n\\n        Returns\\n        -------\\n        pseudo_likelihood : array-like, shape (n_samples,)\\n            Value of the pseudo-likelihood (proxy for likelihood).\\n\\n        Notes\\n        -----\\n        This method is not deterministic: it computes a quantity called the\\n        free energy on X, then on a randomly corrupted version of X, and\\n        returns the log of the logistic function of the difference.\\n        \"\"\"\\n        check_is_fitted(self, \"components_\")\\n\\n        v = check_array(X, accept_sparse=\\'csr\\')\\n        rng = check_random_state(self.random_state)\\n\\n        # Randomly corrupt one feature in each sample in v.\\n        ind = (np.arange(v.shape[0]),\\n               rng.randint(0, v.shape[1], v.shape[0]))\\n        if issparse(v):\\n            data = -2 * v[ind] + 1\\n            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\\n        else:\\n            v_ = v.copy()\\n            v_[ind] = 1 - v_[ind]\\n\\n        fe = self._free_energy(v)\\n        fe_ = self._free_energy(v_)\\n        return v.shape[1] * log_logistic(fe_ - fe)\\n\\n    def fit(self, X, y=None):\\n        \"\"\"Fit the model to the data X.\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix} shape (n_samples, n_features)\\n            Training data.\\n\\n        Returns\\n        -------\\n        self : BernoulliRBM\\n            The fitted model.\\n        \"\"\"\\n        X = check_array(X, accept_sparse=\\'csr\\', dtype=np.float64)\\n        n_samples = X.shape[0]\\n        rng = check_random_state(self.random_state)\\n\\n        self.components_ = np.asarray(\\n            rng.normal(0, 0.01, (self.n_components, X.shape[1])),\\n            order=\\'F\\')\\n        self.intercept_hidden_ = np.zeros(self.n_components, )\\n        self.intercept_visible_ = np.zeros(X.shape[1], )\\n        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\\n\\n        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\\n        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\\n                                            n_batches, n_samples))\\n        verbose = self.verbose\\n        begin = time.time()\\n        for iteration in xrange(1, self.n_iter + 1):\\n            for batch_slice in batch_slices:\\n                self._fit(X[batch_slice], rng)\\n\\n            if verbose:\\n                end = time.time()\\n                print(\"[%s] Iteration %d, pseudo-likelihood = %.2f,\"\\n                      \" time = %.2fs\"\\n                      % (type(self).__name__, iteration,\\n                         self.score_samples(X).mean(), end - begin))\\n                begin = end\\n\\n        return self\\n', '#!/usr/bin/env python\\n \\n##\\n##  Copyright 2016 SRI International\\n##  See COPYING file distributed along with the package for the copyright and license terms.\\n##\\n\\nimport pandas\\n\\nimport Rwrapper\\n\\n#\\n# Variables from surveys needed for CTQ\\n#\\n\\n# LimeSurvey field names\\nlime_fields = [ \"ctq_set1 [ctq1]\", \"ctq_set1 [ctq2]\", \"ctq_set1 [ctq3]\", \"ctq_set1 [ctq4]\", \"ctq_set1 [ctq5]\", \"ctq_set1 [ctq6]\", \"ctq_set1 [ctq7]\", \"ctq_set2 [ctq8]\", \"ctq_set2 [ctq9]\", \"ctq_set2 [ct10]\", \"ctq_set2 [ct11]\",\\n                \"ctq_set2 [ct12]\", \"ctq_set2 [ct13]\", \"ctq_set2 [ct14]\", \"ctq_set3 [ctq15]\", \"ctq_set3 [ctq16]\", \"ctq_set3 [ctq17]\", \"ctq_set3 [ctq18]\", \"ctq_set3 [ctq19]\", \"ctq_set3 [ctq20]\", \"ctq_set3 [ctq21]\",\\n                \"ctq_set4 [ctq22]\", \"ctq_set4 [ctq23]\", \"ctq_set4 [ctq24]\", \"ctq_set4 [ctq25]\", \"ctq_set4 [ctq26]\", \"ctq_set4 [ctq27]\", \"ctq_set4 [ctq28]\" ]\\n\\n# Dictionary to recover LimeSurvey field names from REDCap names\\nrc2lime = dict()\\nfor field in lime_fields:\\n    rc2lime[Rwrapper.label_to_sri( \\'youthreport2\\', field )] = field\\n\\n# REDCap fields names\\ninput_fields = { \\'mrireport\\' : [ \\'youth_report_2_complete\\',  \\'youthreport2_missing\\' ] + rc2lime.keys() }\\n\\n#\\n# This determines the name of the form in REDCap where the results are posted.\\n#\\noutput_form = \\'clinical\\'\\n\\n#\\n# CTQ field names mapping from R to REDCap\\n#\\nR2rc = { \\'Emotional Abuse Scale Total Score\\' : \\'ctq_ea\\', \\n         \\'Physical Abuse Scale Total Score\\' : \\'ctq_pa\\', \\n         \\'Sexual Abuse Scale Total Score\\' : \\'ctq_sa\\', \\n         \\'Emotional Neglect Scale Total Score\\' : \\'ctq_en\\', \\n         \\'Physical Neglect Scale Total Score\\' : \\'ctq_pn\\', \\n         \\'Minimization/Denial Scale Total Score\\' : \\'ctq_minds\\' }\\n\\n#\\n# Scoring function - take requested data (as requested by \"input_fields\") for each (subject,event), and demographics (date of birth, gender) for each subject.\\n#\\ndef compute_scores( data, demographics ):\\n    # Get rid of all records that don\\'t have YR2\\n    data.dropna( axis=1, subset=[\\'youth_report_2_complete\\'] )\\n    data = data[ data[\\'youth_report_2_complete\\'] > 0 ]\\n    data = data[ ~(data[\\'youthreport2_missing\\'] > 0) ]\\n\\n    # If no records to score, return empty DF\\n    if len( data ) == 0:\\n        return pandas.DataFrame()\\n\\n    # Replace all column labels with the original LimeSurvey names\\n    data.columns = Rwrapper.map_labels( data.columns, rc2lime )\\n\\n    # Call the scoring function for all table rows\\n    scores = data.apply( Rwrapper.runscript, axis=1, Rscript=\\'ctq/CTQ.R\\', scores_key=\\'CTQ.ary\\' )\\n\\n    # Replace all score columns with REDCap field names\\n    scores.columns = Rwrapper.map_labels( scores.columns, R2rc )\\n\\n    # Simply copy completion status from the input surveys\\n    scores[\\'ctq_complete\\'] = data[\\'youth_report_2_complete\\'].map( int )\\n\\n    # Make a proper multi-index for the scores table\\n    scores.index = pandas.MultiIndex.from_tuples(scores.index)\\n    scores.index.names = [\\'study_id\\', \\'redcap_event_name\\']\\n\\n    # Return the computed scores - this is what will be imported back into REDCap\\n    outfield_list = [ \\'ctq_complete\\' ] + R2rc.values()\\n    return scores[ outfield_list ]\\n\\n', '\"\"\"\\nRandomized Lasso/Logistic: feature selection based on Lasso and\\nsparse Logistic Regression\\n\"\"\"\\n\\n# Author: Gael Varoquaux, Alexandre Gramfort\\n#\\n# License: BSD 3 clause\\nimport itertools\\nfrom abc import ABCMeta, abstractmethod\\nimport warnings\\n\\nimport numpy as np\\nfrom scipy.sparse import issparse\\nfrom scipy import sparse\\nfrom scipy.interpolate import interp1d\\n\\nfrom .base import center_data\\nfrom ..base import BaseEstimator, TransformerMixin\\nfrom ..externals import six\\nfrom ..externals.joblib import Memory, Parallel, delayed\\nfrom ..utils import (as_float_array, check_random_state, check_X_y,\\n                     check_array, safe_mask, ConvergenceWarning)\\nfrom ..utils.validation import check_is_fitted\\nfrom .least_angle import lars_path, LassoLarsIC\\nfrom .logistic import LogisticRegression\\n\\n\\n###############################################################################\\n# Randomized linear model: feature selection\\n\\ndef _resample_model(estimator_func, X, y, scaling=.5, n_resampling=200,\\n                    n_jobs=1, verbose=False, pre_dispatch=\\'3*n_jobs\\',\\n                    random_state=None, sample_fraction=.75, **params):\\n    random_state = check_random_state(random_state)\\n    # We are generating 1 - weights, and not weights\\n    n_samples, n_features = X.shape\\n\\n    if not (0 < scaling < 1):\\n        raise ValueError(\\n            \"\\'scaling\\' should be between 0 and 1. Got %r instead.\" % scaling)\\n\\n    scaling = 1. - scaling\\n    scores_ = 0.0\\n    for active_set in Parallel(n_jobs=n_jobs, verbose=verbose,\\n                               pre_dispatch=pre_dispatch)(\\n            delayed(estimator_func)(\\n                X, y, weights=scaling * random_state.random_integers(\\n                    0, 1, size=(n_features,)),\\n                mask=(random_state.rand(n_samples) < sample_fraction),\\n                verbose=max(0, verbose - 1),\\n                **params)\\n            for _ in range(n_resampling)):\\n        scores_ += active_set\\n\\n    scores_ /= n_resampling\\n    return scores_\\n\\n\\nclass BaseRandomizedLinearModel(six.with_metaclass(ABCMeta, BaseEstimator,\\n                                                   TransformerMixin)):\\n    \"\"\"Base class to implement randomized linear models for feature selection\\n\\n    This implements the strategy by Meinshausen and Buhlman:\\n    stability selection with randomized sampling, and random re-weighting of\\n    the penalty.\\n    \"\"\"\\n\\n    @abstractmethod\\n    def __init__(self):\\n        pass\\n\\n    _center_data = staticmethod(center_data)\\n\\n    def fit(self, X, y):\\n        \"\"\"Fit the model using X, y as training data.\\n\\n        Parameters\\n        ----------\\n        X : array-like, sparse matrix shape = [n_samples, n_features]\\n            Training data.\\n\\n        y : array-like, shape = [n_samples]\\n            Target values.\\n\\n        Returns\\n        -------\\n        self : object\\n            Returns an instance of self.\\n        \"\"\"\\n        X, y = check_X_y(X, y, [\\'csr\\', \\'csc\\', \\'coo\\'], y_numeric=True)\\n        X = as_float_array(X, copy=False)\\n        n_samples, n_features = X.shape\\n\\n        X, y, X_mean, y_mean, X_std = self._center_data(X, y,\\n                                                        self.fit_intercept,\\n                                                        self.normalize)\\n\\n        estimator_func, params = self._make_estimator_and_params(X, y)\\n        memory = self.memory\\n        if isinstance(memory, six.string_types):\\n            memory = Memory(cachedir=memory)\\n\\n        scores_ = memory.cache(\\n            _resample_model, ignore=[\\'verbose\\', \\'n_jobs\\', \\'pre_dispatch\\']\\n        )(\\n            estimator_func, X, y,\\n            scaling=self.scaling, n_resampling=self.n_resampling,\\n            n_jobs=self.n_jobs, verbose=self.verbose,\\n            pre_dispatch=self.pre_dispatch, random_state=self.random_state,\\n            sample_fraction=self.sample_fraction, **params)\\n\\n        if scores_.ndim == 1:\\n            scores_ = scores_[:, np.newaxis]\\n        self.all_scores_ = scores_\\n        self.scores_ = np.max(self.all_scores_, axis=1)\\n        return self\\n\\n    def _make_estimator_and_params(self, X, y):\\n        \"\"\"Return the parameters passed to the estimator\"\"\"\\n        raise NotImplementedError\\n\\n    def get_support(self, indices=False):\\n        \"\"\"Return a mask, or list, of the features/indices selected.\"\"\"\\n        check_is_fitted(self, \\'scores_\\')\\n\\n        mask = self.scores_ > self.selection_threshold\\n        return mask if not indices else np.where(mask)[0]\\n\\n    # XXX: the two function below are copy/pasted from feature_selection,\\n    # Should we add an intermediate base class?\\n    def transform(self, X):\\n        \"\"\"Transform a new matrix using the selected features\"\"\"\\n        mask = self.get_support()\\n        X = check_array(X)\\n        if len(mask) != X.shape[1]:\\n            raise ValueError(\"X has a different shape than during fitting.\")\\n        return check_array(X)[:, safe_mask(X, mask)]\\n\\n    def inverse_transform(self, X):\\n        \"\"\"Transform a new matrix using the selected features\"\"\"\\n        support = self.get_support()\\n        if X.ndim == 1:\\n            X = X[None, :]\\n        Xt = np.zeros((X.shape[0], support.size))\\n        Xt[:, support] = X\\n        return Xt\\n\\n\\n###############################################################################\\n# Randomized lasso: regression settings\\n\\ndef _randomized_lasso(X, y, weights, mask, alpha=1., verbose=False,\\n                      precompute=False, eps=np.finfo(np.float).eps,\\n                      max_iter=500):\\n    X = X[safe_mask(X, mask)]\\n    y = y[mask]\\n\\n    # Center X and y to avoid fit the intercept\\n    X -= X.mean(axis=0)\\n    y -= y.mean()\\n\\n    alpha = np.atleast_1d(np.asarray(alpha, dtype=np.float))\\n\\n    X = (1 - weights) * X\\n    with warnings.catch_warnings():\\n        warnings.simplefilter(\\'ignore\\', ConvergenceWarning)\\n        alphas_, _, coef_ = lars_path(X, y,\\n                                      Gram=precompute, copy_X=False,\\n                                      copy_Gram=False, alpha_min=np.min(alpha),\\n                                      method=\\'lasso\\', verbose=verbose,\\n                                      max_iter=max_iter, eps=eps)\\n\\n    if len(alpha) > 1:\\n        if len(alphas_) > 1:  # np.min(alpha) < alpha_min\\n            interpolator = interp1d(alphas_[::-1], coef_[:, ::-1],\\n                                    bounds_error=False, fill_value=0.)\\n            scores = (interpolator(alpha) != 0.0)\\n        else:\\n            scores = np.zeros((X.shape[1], len(alpha)), dtype=np.bool)\\n    else:\\n        scores = coef_[:, -1] != 0.0\\n    return scores\\n\\n\\nclass RandomizedLasso(BaseRandomizedLinearModel):\\n    \"\"\"Randomized Lasso.\\n\\n    Randomized Lasso works by resampling the train data and computing\\n    a Lasso on each resampling. In short, the features selected more\\n    often are good features. It is also known as stability selection.\\n\\n    Read more in the :ref:`User Guide <randomized_l1>`.\\n\\n    Parameters\\n    ----------\\n    alpha : float, \\'aic\\', or \\'bic\\', optional\\n        The regularization parameter alpha parameter in the Lasso.\\n        Warning: this is not the alpha parameter in the stability selection\\n        article which is scaling.\\n\\n    scaling : float, optional\\n        The alpha parameter in the stability selection article used to\\n        randomly scale the features. Should be between 0 and 1.\\n\\n    sample_fraction : float, optional\\n        The fraction of samples to be used in each randomized design.\\n        Should be between 0 and 1. If 1, all samples are used.\\n\\n    n_resampling : int, optional\\n        Number of randomized models.\\n\\n    selection_threshold: float, optional\\n        The score above which features should be selected.\\n\\n    fit_intercept : boolean, optional\\n        whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (e.g. data is expected to be already centered).\\n\\n    verbose : boolean or integer, optional\\n        Sets the verbosity amount\\n\\n    normalize : boolean, optional, default True\\n        If True, the regressors X will be normalized before regression.\\n\\n    precompute : True | False | \\'auto\\'\\n        Whether to use a precomputed Gram matrix to speed up\\n        calculations. If set to \\'auto\\' let us decide. The Gram\\n        matrix can also be passed as argument.\\n\\n    max_iter : integer, optional\\n        Maximum number of iterations to perform in the Lars algorithm.\\n\\n    eps : float, optional\\n        The machine-precision regularization in the computation of the\\n        Cholesky diagonal factors. Increase this for very ill-conditioned\\n        systems. Unlike the \\'tol\\' parameter in some iterative\\n        optimization-based algorithms, this parameter does not control\\n        the tolerance of the optimization.\\n\\n    n_jobs : integer, optional\\n        Number of CPUs to use during the resampling. If \\'-1\\', use\\n        all the CPUs\\n\\n    random_state : int, RandomState instance or None, optional (default=None)\\n        If int, random_state is the seed used by the random number generator;\\n        If RandomState instance, random_state is the random number generator;\\n        If None, the random number generator is the RandomState instance used\\n        by `np.random`.\\n\\n    pre_dispatch : int, or string, optional\\n        Controls the number of jobs that get dispatched during parallel\\n        execution. Reducing this number can be useful to avoid an\\n        explosion of memory consumption when more jobs get dispatched\\n        than CPUs can process. This parameter can be:\\n\\n            - None, in which case all the jobs are immediately\\n              created and spawned. Use this for lightweight and\\n              fast-running jobs, to avoid delays due to on-demand\\n              spawning of the jobs\\n\\n            - An int, giving the exact number of total jobs that are\\n              spawned\\n\\n            - A string, giving an expression as a function of n_jobs,\\n              as in \\'2*n_jobs\\'\\n\\n    memory : Instance of joblib.Memory or string\\n        Used for internal caching. By default, no caching is done.\\n        If a string is given, it is the path to the caching directory.\\n\\n    Attributes\\n    ----------\\n    scores_ : array, shape = [n_features]\\n        Feature scores between 0 and 1.\\n\\n    all_scores_ : array, shape = [n_features, n_reg_parameter]\\n        Feature scores between 0 and 1 for all values of the regularization \\\\\\n        parameter. The reference article suggests ``scores_`` is the max of \\\\\\n        ``all_scores_``.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import RandomizedLasso\\n    >>> randomized_lasso = RandomizedLasso()\\n\\n    Notes\\n    -----\\n    See examples/linear_model/plot_sparse_recovery.py for an example.\\n\\n    References\\n    ----------\\n    Stability selection\\n    Nicolai Meinshausen, Peter Buhlmann\\n    Journal of the Royal Statistical Society: Series B\\n    Volume 72, Issue 4, pages 417-473, September 2010\\n    DOI: 10.1111/j.1467-9868.2010.00740.x\\n\\n    See also\\n    --------\\n    RandomizedLogisticRegression, LogisticRegression\\n    \"\"\"\\n    def __init__(self, alpha=\\'aic\\', scaling=.5, sample_fraction=.75,\\n                 n_resampling=200, selection_threshold=.25,\\n                 fit_intercept=True, verbose=False,\\n                 normalize=True, precompute=\\'auto\\',\\n                 max_iter=500,\\n                 eps=np.finfo(np.float).eps, random_state=None,\\n                 n_jobs=1, pre_dispatch=\\'3*n_jobs\\',\\n                 memory=Memory(cachedir=None, verbose=0)):\\n        self.alpha = alpha\\n        self.scaling = scaling\\n        self.sample_fraction = sample_fraction\\n        self.n_resampling = n_resampling\\n        self.fit_intercept = fit_intercept\\n        self.max_iter = max_iter\\n        self.verbose = verbose\\n        self.normalize = normalize\\n        self.precompute = precompute\\n        self.eps = eps\\n        self.random_state = random_state\\n        self.n_jobs = n_jobs\\n        self.selection_threshold = selection_threshold\\n        self.pre_dispatch = pre_dispatch\\n        self.memory = memory\\n\\n    def _make_estimator_and_params(self, X, y):\\n        assert self.precompute in (True, False, None, \\'auto\\')\\n        alpha = self.alpha\\n        if alpha in (\\'aic\\', \\'bic\\'):\\n            model = LassoLarsIC(precompute=self.precompute,\\n                                criterion=self.alpha,\\n                                max_iter=self.max_iter,\\n                                eps=self.eps)\\n            model.fit(X, y)\\n            self.alpha_ = alpha = model.alpha_\\n        return _randomized_lasso, dict(alpha=alpha, max_iter=self.max_iter,\\n                                       eps=self.eps,\\n                                       precompute=self.precompute)\\n\\n\\n###############################################################################\\n# Randomized logistic: classification settings\\n\\ndef _randomized_logistic(X, y, weights, mask, C=1., verbose=False,\\n                         fit_intercept=True, tol=1e-3):\\n    X = X[safe_mask(X, mask)]\\n    y = y[mask]\\n    if issparse(X):\\n        size = len(weights)\\n        weight_dia = sparse.dia_matrix((1 - weights, 0), (size, size))\\n        X = X * weight_dia\\n    else:\\n        X *= (1 - weights)\\n\\n    C = np.atleast_1d(np.asarray(C, dtype=np.float))\\n    scores = np.zeros((X.shape[1], len(C)), dtype=np.bool)\\n\\n    for this_C, this_scores in zip(C, scores.T):\\n        # XXX : would be great to do it with a warm_start ...\\n        clf = LogisticRegression(C=this_C, tol=tol, penalty=\\'l1\\', dual=False,\\n                                 fit_intercept=fit_intercept)\\n        clf.fit(X, y)\\n        this_scores[:] = np.any(\\n            np.abs(clf.coef_) > 10 * np.finfo(np.float).eps, axis=0)\\n    return scores\\n\\n\\nclass RandomizedLogisticRegression(BaseRandomizedLinearModel):\\n    \"\"\"Randomized Logistic Regression\\n\\n    Randomized Regression works by resampling the train data and computing\\n    a LogisticRegression on each resampling. In short, the features selected\\n    more often are good features. It is also known as stability selection.\\n\\n    Read more in the :ref:`User Guide <randomized_l1>`.\\n\\n    Parameters\\n    ----------\\n    C : float, optional, default=1\\n        The regularization parameter C in the LogisticRegression.\\n\\n    scaling : float, optional, default=0.5\\n        The alpha parameter in the stability selection article used to\\n        randomly scale the features. Should be between 0 and 1.\\n\\n    sample_fraction : float, optional, default=0.75\\n        The fraction of samples to be used in each randomized design.\\n        Should be between 0 and 1. If 1, all samples are used.\\n\\n    n_resampling : int, optional, default=200\\n        Number of randomized models.\\n\\n    selection_threshold : float, optional, default=0.25\\n        The score above which features should be selected.\\n\\n    fit_intercept : boolean, optional, default=True\\n        whether to calculate the intercept for this model. If set\\n        to false, no intercept will be used in calculations\\n        (e.g. data is expected to be already centered).\\n\\n    verbose : boolean or integer, optional\\n        Sets the verbosity amount\\n\\n    normalize : boolean, optional, default=True\\n        If True, the regressors X will be normalized before regression.\\n\\n    tol : float, optional, default=1e-3\\n         tolerance for stopping criteria of LogisticRegression\\n\\n    n_jobs : integer, optional\\n        Number of CPUs to use during the resampling. If \\'-1\\', use\\n        all the CPUs\\n\\n    random_state : int, RandomState instance or None, optional (default=None)\\n        If int, random_state is the seed used by the random number generator;\\n        If RandomState instance, random_state is the random number generator;\\n        If None, the random number generator is the RandomState instance used\\n        by `np.random`.\\n\\n    pre_dispatch : int, or string, optional\\n        Controls the number of jobs that get dispatched during parallel\\n        execution. Reducing this number can be useful to avoid an\\n        explosion of memory consumption when more jobs get dispatched\\n        than CPUs can process. This parameter can be:\\n\\n            - None, in which case all the jobs are immediately\\n              created and spawned. Use this for lightweight and\\n              fast-running jobs, to avoid delays due to on-demand\\n              spawning of the jobs\\n\\n            - An int, giving the exact number of total jobs that are\\n              spawned\\n\\n            - A string, giving an expression as a function of n_jobs,\\n              as in \\'2*n_jobs\\'\\n\\n    memory : Instance of joblib.Memory or string\\n        Used for internal caching. By default, no caching is done.\\n        If a string is given, it is the path to the caching directory.\\n\\n    Attributes\\n    ----------\\n    scores_ : array, shape = [n_features]\\n        Feature scores between 0 and 1.\\n\\n    all_scores_ : array, shape = [n_features, n_reg_parameter]\\n        Feature scores between 0 and 1 for all values of the regularization \\\\\\n        parameter. The reference article suggests ``scores_`` is the max \\\\\\n        of ``all_scores_``.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.linear_model import RandomizedLogisticRegression\\n    >>> randomized_logistic = RandomizedLogisticRegression()\\n\\n    Notes\\n    -----\\n    See examples/linear_model/plot_sparse_recovery.py for an example.\\n\\n    References\\n    ----------\\n    Stability selection\\n    Nicolai Meinshausen, Peter Buhlmann\\n    Journal of the Royal Statistical Society: Series B\\n    Volume 72, Issue 4, pages 417-473, September 2010\\n    DOI: 10.1111/j.1467-9868.2010.00740.x\\n\\n    See also\\n    --------\\n    RandomizedLasso, Lasso, ElasticNet\\n    \"\"\"\\n    def __init__(self, C=1, scaling=.5, sample_fraction=.75,\\n                 n_resampling=200,\\n                 selection_threshold=.25, tol=1e-3,\\n                 fit_intercept=True, verbose=False,\\n                 normalize=True,\\n                 random_state=None,\\n                 n_jobs=1, pre_dispatch=\\'3*n_jobs\\',\\n                 memory=Memory(cachedir=None, verbose=0)):\\n        self.C = C\\n        self.scaling = scaling\\n        self.sample_fraction = sample_fraction\\n        self.n_resampling = n_resampling\\n        self.fit_intercept = fit_intercept\\n        self.verbose = verbose\\n        self.normalize = normalize\\n        self.tol = tol\\n        self.random_state = random_state\\n        self.n_jobs = n_jobs\\n        self.selection_threshold = selection_threshold\\n        self.pre_dispatch = pre_dispatch\\n        self.memory = memory\\n\\n    def _make_estimator_and_params(self, X, y):\\n        params = dict(C=self.C, tol=self.tol,\\n                      fit_intercept=self.fit_intercept)\\n        return _randomized_logistic, params\\n\\n    def _center_data(self, X, y, fit_intercept, normalize=False):\\n        \"\"\"Center the data in X but not in y\"\"\"\\n        X, _, Xmean, _, X_std = center_data(X, y, fit_intercept,\\n                                            normalize=normalize)\\n        return X, y, Xmean, y, X_std\\n\\n\\n###############################################################################\\n# Stability paths\\ndef _lasso_stability_path(X, y, mask, weights, eps):\\n    \"Inner loop of lasso_stability_path\"\\n    X = X * weights[np.newaxis, :]\\n    X = X[safe_mask(X, mask), :]\\n    y = y[mask]\\n\\n    alpha_max = np.max(np.abs(np.dot(X.T, y))) / X.shape[0]\\n    alpha_min = eps * alpha_max  # set for early stopping in path\\n    with warnings.catch_warnings():\\n        warnings.simplefilter(\\'ignore\\', ConvergenceWarning)\\n        alphas, _, coefs = lars_path(X, y, method=\\'lasso\\', verbose=False,\\n                                     alpha_min=alpha_min)\\n    # Scale alpha by alpha_max\\n    alphas /= alphas[0]\\n    # Sort alphas in assending order\\n    alphas = alphas[::-1]\\n    coefs = coefs[:, ::-1]\\n    # Get rid of the alphas that are too small\\n    mask = alphas >= eps\\n    # We also want to keep the first one: it should be close to the OLS\\n    # solution\\n    mask[0] = True\\n    alphas = alphas[mask]\\n    coefs = coefs[:, mask]\\n    return alphas, coefs\\n\\n\\ndef lasso_stability_path(X, y, scaling=0.5, random_state=None,\\n                         n_resampling=200, n_grid=100,\\n                         sample_fraction=0.75,\\n                         eps=4 * np.finfo(np.float).eps, n_jobs=1,\\n                         verbose=False):\\n    \"\"\"Stabiliy path based on randomized Lasso estimates\\n\\n    Read more in the :ref:`User Guide <randomized_l1>`.\\n\\n    Parameters\\n    ----------\\n    X : array-like, shape = [n_samples, n_features]\\n        training data.\\n\\n    y : array-like, shape = [n_samples]\\n        target values.\\n\\n    scaling : float, optional, default=0.5\\n        The alpha parameter in the stability selection article used to\\n        randomly scale the features. Should be between 0 and 1.\\n\\n    random_state : integer or numpy.random.RandomState, optional\\n        The generator used to randomize the design.\\n\\n    n_resampling : int, optional, default=200\\n        Number of randomized models.\\n\\n    n_grid : int, optional, default=100\\n        Number of grid points. The path is linearly reinterpolated\\n        on a grid between 0 and 1 before computing the scores.\\n\\n    sample_fraction : float, optional, default=0.75\\n        The fraction of samples to be used in each randomized design.\\n        Should be between 0 and 1. If 1, all samples are used.\\n\\n    eps : float, optional\\n        Smallest value of alpha / alpha_max considered\\n\\n    n_jobs : integer, optional\\n        Number of CPUs to use during the resampling. If \\'-1\\', use\\n        all the CPUs\\n\\n    verbose : boolean or integer, optional\\n        Sets the verbosity amount\\n\\n    Returns\\n    -------\\n    alphas_grid : array, shape ~ [n_grid]\\n        The grid points between 0 and 1: alpha/alpha_max\\n\\n    scores_path : array, shape = [n_features, n_grid]\\n        The scores for each feature along the path.\\n\\n    Notes\\n    -----\\n    See examples/linear_model/plot_sparse_recovery.py for an example.\\n    \"\"\"\\n    rng = check_random_state(random_state)\\n\\n    if not (0 < scaling < 1):\\n        raise ValueError(\"Parameter \\'scaling\\' should be between 0 and 1.\"\\n                         \" Got %r instead.\" % scaling)\\n\\n    n_samples, n_features = X.shape\\n\\n    paths = Parallel(n_jobs=n_jobs, verbose=verbose)(\\n        delayed(_lasso_stability_path)(\\n            X, y, mask=rng.rand(n_samples) < sample_fraction,\\n            weights=1. - scaling * rng.random_integers(0, 1,\\n                                                       size=(n_features,)),\\n            eps=eps)\\n        for k in range(n_resampling))\\n\\n    all_alphas = sorted(list(set(itertools.chain(*[p[0] for p in paths]))))\\n    # Take approximately n_grid values\\n    stride = int(max(1, int(len(all_alphas) / float(n_grid))))\\n    all_alphas = all_alphas[::stride]\\n    if not all_alphas[-1] == 1:\\n        all_alphas.append(1.)\\n    all_alphas = np.array(all_alphas)\\n    scores_path = np.zeros((n_features, len(all_alphas)))\\n\\n    for alphas, coefs in paths:\\n        if alphas[0] != 0:\\n            alphas = np.r_[0, alphas]\\n            coefs = np.c_[np.ones((n_features, 1)), coefs]\\n        if alphas[-1] != all_alphas[-1]:\\n            alphas = np.r_[alphas, all_alphas[-1]]\\n            coefs = np.c_[coefs, np.zeros((n_features, 1))]\\n        scores_path += (interp1d(alphas, coefs,\\n                        kind=\\'nearest\\', bounds_error=False,\\n                        fill_value=0, axis=-1)(all_alphas) != 0)\\n\\n    scores_path /= n_resampling\\n    return all_alphas, scores_path\\n', \"from __future__ import division\\nfrom __future__ import absolute_import\\nfrom __future__ import print_function\\nfrom __future__ import unicode_literals\\n\\nimport numpy as np\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.calibration import CalibratedClassifierCV\\nfrom sklearn.metrics import f1_score\\n\\nimport argparse\\nfrom os import path\\nimport os\\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\\nfrom utils import *\\nimport pickle\\n\\nnp.random.seed(54568464)\\n\\n\\ndef parse_args():\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument('--yix', type=int, default=0)\\n    return parser.parse_args()\\n\\n\\n# functions for hyperparameters optimization\\nclass Score:\\n    def __init__(self, X, y):\\n        self.y = y\\n        self.X = X\\n\\n    def get_score(self, params):\\n        params['n_estimators'] = int(params['n_estimators'])\\n        params['max_depth'] = int(params['max_depth'])\\n        params['min_samples_split'] = int(params['min_samples_split'])\\n        params['min_samples_leaf'] = int(params['min_samples_leaf'])\\n        params['n_estimators'] = int(params['n_estimators'])\\n\\n        print('Training with params:')\\n        print(params)\\n\\n        # cross validation here\\n        scores = []\\n        for train_ix, test_ix in makeKFold(5, self.y, 1):\\n            X_train, y_train = self.X[train_ix, :], self.y[train_ix]\\n            X_test, y_test = self.X[test_ix, :], self.y[test_ix]\\n            weight = y_train.shape[0] / (2 * np.bincount(y_train))\\n            sample_weight = np.array([weight[i] for i in y_train])\\n\\n            clf = RandomForestClassifier(**params)\\n            cclf = CalibratedClassifierCV(base_estimator=clf,\\n                                          method='isotonic',\\n                                          cv=makeKFold(3, y_train, 1))\\n            cclf.fit(X_train, y_train, sample_weight)\\n            pred = cclf.predict(X_test)\\n            scores.append(f1_score(y_true=y_test, y_pred=pred))\\n\\n        print(scores)\\n        score = np.mean(scores)\\n\\n        print(score)\\n        return {'loss': -score, 'status': STATUS_OK}\\n\\n\\ndef optimize(trials, X, y, max_evals):\\n    space = {\\n        'n_estimators': hp.quniform('n_estimators', 100, 500, 50),\\n        'criterion': hp.choice('criterion', ['gini', 'entropy']),\\n        'max_depth': hp.quniform('max_depth', 1, 7, 1),\\n        'min_samples_split': hp.quniform('min_samples_split', 1, 9, 2),\\n        'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 5, 1),\\n        'bootstrap': True,\\n        'oob_score': True,\\n        'n_jobs': -1\\n    }\\n    s = Score(X, y)\\n    best = fmin(s.get_score,\\n                space,\\n                algo=tpe.suggest,\\n                trials=trials,\\n                max_evals=max_evals\\n                )\\n    best['n_estimators'] = int(best['n_estimators'])\\n    best['max_depth'] = int(best['max_depth'])\\n    best['min_samples_split'] = int(best['min_samples_split'])\\n    best['min_samples_leaf'] = int(best['min_samples_leaf'])\\n    best['n_estimators'] = int(best['n_estimators'])\\n    best['criterion'] = ['gini', 'entropy'][best['criterion']]\\n    best['bootstrap'] = True\\n    best['oob_score'] = True\\n    best['n_jobs'] = -1\\n    del s\\n    return best\\n\\n\\ndef out_fold_pred(params, X, y):\\n    # cross validation here\\n    preds = np.zeros((y.shape[0]))\\n\\n    for train_ix, test_ix in makeKFold(5, y, 1):\\n        X_train, y_train = X[train_ix, :], y[train_ix]\\n        X_test = X[test_ix, :]\\n        weight = y_train.shape[0] / (2 * np.bincount(y_train))\\n        sample_weight = np.array([weight[i] for i in y_train])\\n\\n        clf = RandomForestClassifier(**params)\\n        cclf = CalibratedClassifierCV(base_estimator=clf,\\n                                      method='isotonic',\\n                                      cv=makeKFold(3, y_train, 1))\\n        cclf.fit(X_train, y_train, sample_weight)\\n        pred = cclf.predict_proba(X_test)[:, 1]\\n        preds[test_ix] = pred\\n    return preds\\n\\n\\ndef get_model(params, X, y):\\n    clf = RandomForestClassifier(**params)\\n    cclf = CalibratedClassifierCV(base_estimator=clf,\\n                                  method='isotonic',\\n                                  cv=makeKFold(3, y, 1))\\n    weight = y.shape[0] / (2 * np.bincount(y))\\n    sample_weight = np.array([weight[i] for i in y])\\n    cclf.fit(X, y, sample_weight)\\n    return cclf\\n\\n\\nargs = parse_args()\\ndata_dir = '../level3-feature/' + str(args.yix)\\nX_train = np.load(path.join(data_dir, 'X_train.npy'))\\nX_test = np.load(path.join(data_dir, 'X_test.npy'))\\ny_train = np.load(path.join(data_dir, 'y_train.npy'))\\nprint(X_train.shape, X_test.shape, y_train.shape)\\n\\nX_train_ext = np.load('../extra_ftrs/' + str(args.yix) + '/X_train_ext.npy')\\nX_test_ext = np.load('../extra_ftrs/' + str(args.yix) + '/X_test_ext.npy')\\nprint(X_train_ext.shape, X_test_ext.shape)\\n\\nX_train = np.hstack((X_train, X_train_ext))\\nX_test = np.hstack((X_test, X_test_ext))\\nprint('Add Extra')\\nprint(X_train.shape, X_test.shape, y_train.shape)\\n\\n\\n# Now we have X_train, X_test, y_train\\ntrials = Trials()\\nparams = optimize(trials, X_train, y_train, 50)\\nout_fold = out_fold_pred(params, X_train, y_train)\\nclf = get_model(params, X_train, y_train)\\npreds = clf.predict_proba(X_test)[:, 1]\\n\\nsave_dir = '../level3-model-final/' + str(args.yix)\\nprint(save_dir)\\nif not path.exists(save_dir):\\n    os.makedirs(save_dir)\\n\\n# save model, parameter, outFold_pred, pred\\nwith open(path.join(save_dir, 'model_rf.pkl'), 'wb') as f_model:\\n    pickle.dump(clf.calibrated_classifiers_, f_model)\\n\\nwith open(path.join(save_dir, 'param_rf.pkl'), 'wb') as f_param:\\n    pickle.dump(params, f_param)\\n\\nnp.save(path.join(save_dir, 'pred_rf.npy'), preds)\\nnp.save(path.join(save_dir, 'outFold_rf.npy'), out_fold)\\n\", '\"\"\"\\nModule for formatting output data in HTML.\\n\"\"\"\\n\\nfrom textwrap import dedent\\nfrom typing import Any, Dict, Iterable, List, Mapping, Optional, Tuple, Union, cast\\n\\nfrom pandas._config import get_option\\n\\nfrom pandas._libs import lib\\n\\nfrom pandas import MultiIndex, option_context\\n\\nfrom pandas.io.common import is_url\\nfrom pandas.io.formats.format import DataFrameFormatter, get_level_lengths\\nfrom pandas.io.formats.printing import pprint_thing\\n\\n\\nclass HTMLFormatter:\\n    \"\"\"\\n    Internal class for formatting output data in html.\\n    This class is intended for shared functionality between\\n    DataFrame.to_html() and DataFrame._repr_html_().\\n    Any logic in common with other output formatting methods\\n    should ideally be inherited from classes in format.py\\n    and this class responsible for only producing html markup.\\n    \"\"\"\\n\\n    indent_delta = 2\\n\\n    def __init__(\\n        self,\\n        formatter: DataFrameFormatter,\\n        classes: Optional[Union[str, List[str], Tuple[str, ...]]] = None,\\n        border: Optional[int] = None,\\n        table_id: Optional[str] = None,\\n        render_links: bool = False,\\n    ) -> None:\\n        self.fmt = formatter\\n        self.classes = classes\\n\\n        self.frame = self.fmt.frame\\n        self.columns = self.fmt.tr_frame.columns\\n        self.elements: List[str] = []\\n        self.bold_rows = self.fmt.bold_rows\\n        self.escape = self.fmt.escape\\n        self.show_dimensions = self.fmt.show_dimensions\\n        if border is None:\\n            border = cast(int, get_option(\"display.html.border\"))\\n        self.border = border\\n        self.table_id = table_id\\n        self.render_links = render_links\\n\\n        self.col_space = {\\n            column: f\"{value}px\" if isinstance(value, int) else value\\n            for column, value in self.fmt.col_space.items()\\n        }\\n\\n    def to_string(self) -> str:\\n        lines = self.render()\\n        if any(isinstance(x, str) for x in lines):\\n            lines = [str(x) for x in lines]\\n        return \"\\\\n\".join(lines)\\n\\n    def render(self) -> List[str]:\\n        self._write_table()\\n\\n        if self.should_show_dimensions:\\n            by = chr(215)  # \\n            self.write(\\n                f\"<p>{len(self.frame)} rows {by} {len(self.frame.columns)} columns</p>\"\\n            )\\n\\n        return self.elements\\n\\n    @property\\n    def should_show_dimensions(self):\\n        return self.fmt.should_show_dimensions\\n\\n    @property\\n    def show_row_idx_names(self) -> bool:\\n        return self.fmt.show_row_idx_names\\n\\n    @property\\n    def show_col_idx_names(self) -> bool:\\n        return self.fmt.show_col_idx_names\\n\\n    @property\\n    def row_levels(self) -> int:\\n        if self.fmt.index:\\n            # showing (row) index\\n            return self.frame.index.nlevels\\n        elif self.show_col_idx_names:\\n            # see gh-22579\\n            # Column misalignment also occurs for\\n            # a standard index when the columns index is named.\\n            # If the row index is not displayed a column of\\n            # blank cells need to be included before the DataFrame values.\\n            return 1\\n        # not showing (row) index\\n        return 0\\n\\n    def _get_columns_formatted_values(self) -> Iterable:\\n        return self.columns\\n\\n    @property\\n    def is_truncated(self) -> bool:\\n        return self.fmt.is_truncated\\n\\n    @property\\n    def ncols(self) -> int:\\n        return len(self.fmt.tr_frame.columns)\\n\\n    def write(self, s: Any, indent: int = 0) -> None:\\n        rs = pprint_thing(s)\\n        self.elements.append(\" \" * indent + rs)\\n\\n    def write_th(\\n        self, s: Any, header: bool = False, indent: int = 0, tags: Optional[str] = None\\n    ) -> None:\\n        \"\"\"\\n        Method for writing a formatted <th> cell.\\n\\n        If col_space is set on the formatter then that is used for\\n        the value of min-width.\\n\\n        Parameters\\n        ----------\\n        s : object\\n            The data to be written inside the cell.\\n        header : bool, default False\\n            Set to True if the <th> is for use inside <thead>.  This will\\n            cause min-width to be set if there is one.\\n        indent : int, default 0\\n            The indentation level of the cell.\\n        tags : str, default None\\n            Tags to include in the cell.\\n\\n        Returns\\n        -------\\n        A written <th> cell.\\n        \"\"\"\\n        col_space = self.col_space.get(s, None)\\n\\n        if header and col_space is not None:\\n            tags = tags or \"\"\\n            tags += f\\'style=\"min-width: {col_space};\"\\'\\n\\n        self._write_cell(s, kind=\"th\", indent=indent, tags=tags)\\n\\n    def write_td(self, s: Any, indent: int = 0, tags: Optional[str] = None) -> None:\\n        self._write_cell(s, kind=\"td\", indent=indent, tags=tags)\\n\\n    def _write_cell(\\n        self, s: Any, kind: str = \"td\", indent: int = 0, tags: Optional[str] = None\\n    ) -> None:\\n        if tags is not None:\\n            start_tag = f\"<{kind} {tags}>\"\\n        else:\\n            start_tag = f\"<{kind}>\"\\n\\n        if self.escape:\\n            # escape & first to prevent double escaping of &\\n            esc = {\"&\": r\"&amp;\", \"<\": r\"&lt;\", \">\": r\"&gt;\"}\\n        else:\\n            esc = {}\\n\\n        rs = pprint_thing(s, escape_chars=esc).strip()\\n\\n        if self.render_links and is_url(rs):\\n            rs_unescaped = pprint_thing(s, escape_chars={}).strip()\\n            start_tag += f\\'<a href=\"{rs_unescaped}\" target=\"_blank\">\\'\\n            end_a = \"</a>\"\\n        else:\\n            end_a = \"\"\\n\\n        self.write(f\"{start_tag}{rs}{end_a}</{kind}>\", indent)\\n\\n    def write_tr(\\n        self,\\n        line: Iterable,\\n        indent: int = 0,\\n        indent_delta: int = 0,\\n        header: bool = False,\\n        align: Optional[str] = None,\\n        tags: Optional[Dict[int, str]] = None,\\n        nindex_levels: int = 0,\\n    ) -> None:\\n        if tags is None:\\n            tags = {}\\n\\n        if align is None:\\n            self.write(\"<tr>\", indent)\\n        else:\\n            self.write(f\\'<tr style=\"text-align: {align};\">\\', indent)\\n        indent += indent_delta\\n\\n        for i, s in enumerate(line):\\n            val_tag = tags.get(i, None)\\n            if header or (self.bold_rows and i < nindex_levels):\\n                self.write_th(s, indent=indent, header=header, tags=val_tag)\\n            else:\\n                self.write_td(s, indent, tags=val_tag)\\n\\n        indent -= indent_delta\\n        self.write(\"</tr>\", indent)\\n\\n    def _write_table(self, indent: int = 0) -> None:\\n        _classes = [\"dataframe\"]  # Default class.\\n        use_mathjax = get_option(\"display.html.use_mathjax\")\\n        if not use_mathjax:\\n            _classes.append(\"tex2jax_ignore\")\\n        if self.classes is not None:\\n            if isinstance(self.classes, str):\\n                self.classes = self.classes.split()\\n            if not isinstance(self.classes, (list, tuple)):\\n                raise TypeError(\\n                    \"classes must be a string, list, \"\\n                    f\"or tuple, not {type(self.classes)}\"\\n                )\\n            _classes.extend(self.classes)\\n\\n        if self.table_id is None:\\n            id_section = \"\"\\n        else:\\n            id_section = f\\' id=\"{self.table_id}\"\\'\\n\\n        self.write(\\n            f\\'<table border=\"{self.border}\" class=\"{\" \".join(_classes)}\"{id_section}>\\',\\n            indent,\\n        )\\n\\n        if self.fmt.header or self.show_row_idx_names:\\n            self._write_header(indent + self.indent_delta)\\n\\n        self._write_body(indent + self.indent_delta)\\n\\n        self.write(\"</table>\", indent)\\n\\n    def _write_col_header(self, indent: int) -> None:\\n        is_truncated_horizontally = self.fmt.is_truncated_horizontally\\n        if isinstance(self.columns, MultiIndex):\\n            template = \\'colspan=\"{span:d}\" halign=\"left\"\\'\\n\\n            if self.fmt.sparsify:\\n                # GH3547\\n                sentinel = lib.no_default\\n            else:\\n                sentinel = False\\n            levels = self.columns.format(sparsify=sentinel, adjoin=False, names=False)\\n            level_lengths = get_level_lengths(levels, sentinel)\\n            inner_lvl = len(level_lengths) - 1\\n            for lnum, (records, values) in enumerate(zip(level_lengths, levels)):\\n                if is_truncated_horizontally:\\n                    # modify the header lines\\n                    ins_col = self.fmt.tr_col_num\\n                    if self.fmt.sparsify:\\n                        recs_new = {}\\n                        # Increment tags after ... col.\\n                        for tag, span in list(records.items()):\\n                            if tag >= ins_col:\\n                                recs_new[tag + 1] = span\\n                            elif tag + span > ins_col:\\n                                recs_new[tag] = span + 1\\n                                if lnum == inner_lvl:\\n                                    values = (\\n                                        values[:ins_col] + (\"...\",) + values[ins_col:]\\n                                    )\\n                                else:\\n                                    # sparse col headers do not receive a ...\\n                                    values = (\\n                                        values[:ins_col]\\n                                        + (values[ins_col - 1],)\\n                                        + values[ins_col:]\\n                                    )\\n                            else:\\n                                recs_new[tag] = span\\n                            # if ins_col lies between tags, all col headers\\n                            # get ...\\n                            if tag + span == ins_col:\\n                                recs_new[ins_col] = 1\\n                                values = values[:ins_col] + (\"...\",) + values[ins_col:]\\n                        records = recs_new\\n                        inner_lvl = len(level_lengths) - 1\\n                        if lnum == inner_lvl:\\n                            records[ins_col] = 1\\n                    else:\\n                        recs_new = {}\\n                        for tag, span in list(records.items()):\\n                            if tag >= ins_col:\\n                                recs_new[tag + 1] = span\\n                            else:\\n                                recs_new[tag] = span\\n                        recs_new[ins_col] = 1\\n                        records = recs_new\\n                        values = values[:ins_col] + [\"...\"] + values[ins_col:]\\n\\n                # see gh-22579\\n                # Column Offset Bug with to_html(index=False) with\\n                # MultiIndex Columns and Index.\\n                # Initially fill row with blank cells before column names.\\n                # TODO: Refactor to remove code duplication with code\\n                # block below for standard columns index.\\n                row = [\"\"] * (self.row_levels - 1)\\n                if self.fmt.index or self.show_col_idx_names:\\n                    # see gh-22747\\n                    # If to_html(index_names=False) do not show columns\\n                    # index names.\\n                    # TODO: Refactor to use _get_column_name_list from\\n                    # DataFrameFormatter class and create a\\n                    # _get_formatted_column_labels function for code\\n                    # parity with DataFrameFormatter class.\\n                    if self.fmt.show_index_names:\\n                        name = self.columns.names[lnum]\\n                        row.append(pprint_thing(name or \"\"))\\n                    else:\\n                        row.append(\"\")\\n\\n                tags = {}\\n                j = len(row)\\n                for i, v in enumerate(values):\\n                    if i in records:\\n                        if records[i] > 1:\\n                            tags[j] = template.format(span=records[i])\\n                    else:\\n                        continue\\n                    j += 1\\n                    row.append(v)\\n                self.write_tr(row, indent, self.indent_delta, tags=tags, header=True)\\n        else:\\n            # see gh-22579\\n            # Column misalignment also occurs for\\n            # a standard index when the columns index is named.\\n            # Initially fill row with blank cells before column names.\\n            # TODO: Refactor to remove code duplication with code block\\n            # above for columns MultiIndex.\\n            row = [\"\"] * (self.row_levels - 1)\\n            if self.fmt.index or self.show_col_idx_names:\\n                # see gh-22747\\n                # If to_html(index_names=False) do not show columns\\n                # index names.\\n                # TODO: Refactor to use _get_column_name_list from\\n                # DataFrameFormatter class.\\n                if self.fmt.show_index_names:\\n                    row.append(self.columns.name or \"\")\\n                else:\\n                    row.append(\"\")\\n            row.extend(self._get_columns_formatted_values())\\n            align = self.fmt.justify\\n\\n            if is_truncated_horizontally:\\n                ins_col = self.row_levels + self.fmt.tr_col_num\\n                row.insert(ins_col, \"...\")\\n\\n            self.write_tr(row, indent, self.indent_delta, header=True, align=align)\\n\\n    def _write_row_header(self, indent: int) -> None:\\n        is_truncated_horizontally = self.fmt.is_truncated_horizontally\\n        row = [x if x is not None else \"\" for x in self.frame.index.names] + [\"\"] * (\\n            self.ncols + (1 if is_truncated_horizontally else 0)\\n        )\\n        self.write_tr(row, indent, self.indent_delta, header=True)\\n\\n    def _write_header(self, indent: int) -> None:\\n        self.write(\"<thead>\", indent)\\n\\n        if self.fmt.header:\\n            self._write_col_header(indent + self.indent_delta)\\n\\n        if self.show_row_idx_names:\\n            self._write_row_header(indent + self.indent_delta)\\n\\n        self.write(\"</thead>\", indent)\\n\\n    def _get_formatted_values(self) -> Dict[int, List[str]]:\\n        with option_context(\"display.max_colwidth\", None):\\n            fmt_values = {i: self.fmt.format_col(i) for i in range(self.ncols)}\\n        return fmt_values\\n\\n    def _write_body(self, indent: int) -> None:\\n        self.write(\"<tbody>\", indent)\\n        fmt_values = self._get_formatted_values()\\n\\n        # write values\\n        if self.fmt.index and isinstance(self.frame.index, MultiIndex):\\n            self._write_hierarchical_rows(fmt_values, indent + self.indent_delta)\\n        else:\\n            self._write_regular_rows(fmt_values, indent + self.indent_delta)\\n\\n        self.write(\"</tbody>\", indent)\\n\\n    def _write_regular_rows(\\n        self, fmt_values: Mapping[int, List[str]], indent: int\\n    ) -> None:\\n        is_truncated_horizontally = self.fmt.is_truncated_horizontally\\n        is_truncated_vertically = self.fmt.is_truncated_vertically\\n\\n        nrows = len(self.fmt.tr_frame)\\n\\n        if self.fmt.index:\\n            fmt = self.fmt._get_formatter(\"__index__\")\\n            if fmt is not None:\\n                index_values = self.fmt.tr_frame.index.map(fmt)\\n            else:\\n                index_values = self.fmt.tr_frame.index.format()\\n\\n        row: List[str] = []\\n        for i in range(nrows):\\n\\n            if is_truncated_vertically and i == (self.fmt.tr_row_num):\\n                str_sep_row = [\"...\"] * len(row)\\n                self.write_tr(\\n                    str_sep_row,\\n                    indent,\\n                    self.indent_delta,\\n                    tags=None,\\n                    nindex_levels=self.row_levels,\\n                )\\n\\n            row = []\\n            if self.fmt.index:\\n                row.append(index_values[i])\\n            # see gh-22579\\n            # Column misalignment also occurs for\\n            # a standard index when the columns index is named.\\n            # Add blank cell before data cells.\\n            elif self.show_col_idx_names:\\n                row.append(\"\")\\n            row.extend(fmt_values[j][i] for j in range(self.ncols))\\n\\n            if is_truncated_horizontally:\\n                dot_col_ix = self.fmt.tr_col_num + self.row_levels\\n                row.insert(dot_col_ix, \"...\")\\n            self.write_tr(\\n                row, indent, self.indent_delta, tags=None, nindex_levels=self.row_levels\\n            )\\n\\n    def _write_hierarchical_rows(\\n        self, fmt_values: Mapping[int, List[str]], indent: int\\n    ) -> None:\\n        template = \\'rowspan=\"{span}\" valign=\"top\"\\'\\n\\n        is_truncated_horizontally = self.fmt.is_truncated_horizontally\\n        is_truncated_vertically = self.fmt.is_truncated_vertically\\n        frame = self.fmt.tr_frame\\n        nrows = len(frame)\\n\\n        assert isinstance(frame.index, MultiIndex)\\n        idx_values = frame.index.format(sparsify=False, adjoin=False, names=False)\\n        idx_values = list(zip(*idx_values))\\n\\n        if self.fmt.sparsify:\\n            # GH3547\\n            sentinel = lib.no_default\\n            levels = frame.index.format(sparsify=sentinel, adjoin=False, names=False)\\n\\n            level_lengths = get_level_lengths(levels, sentinel)\\n            inner_lvl = len(level_lengths) - 1\\n            if is_truncated_vertically:\\n                # Insert ... row and adjust idx_values and\\n                # level_lengths to take this into account.\\n                ins_row = self.fmt.tr_row_num\\n                inserted = False\\n                for lnum, records in enumerate(level_lengths):\\n                    rec_new = {}\\n                    for tag, span in list(records.items()):\\n                        if tag >= ins_row:\\n                            rec_new[tag + 1] = span\\n                        elif tag + span > ins_row:\\n                            rec_new[tag] = span + 1\\n\\n                            # GH 14882 - Make sure insertion done once\\n                            if not inserted:\\n                                dot_row = list(idx_values[ins_row - 1])\\n                                dot_row[-1] = \"...\"\\n                                idx_values.insert(ins_row, tuple(dot_row))\\n                                inserted = True\\n                            else:\\n                                dot_row = list(idx_values[ins_row])\\n                                dot_row[inner_lvl - lnum] = \"...\"\\n                                idx_values[ins_row] = tuple(dot_row)\\n                        else:\\n                            rec_new[tag] = span\\n                        # If ins_row lies between tags, all cols idx cols\\n                        # receive ...\\n                        if tag + span == ins_row:\\n                            rec_new[ins_row] = 1\\n                            if lnum == 0:\\n                                idx_values.insert(\\n                                    ins_row, tuple([\"...\"] * len(level_lengths))\\n                                )\\n\\n                            # GH 14882 - Place ... in correct level\\n                            elif inserted:\\n                                dot_row = list(idx_values[ins_row])\\n                                dot_row[inner_lvl - lnum] = \"...\"\\n                                idx_values[ins_row] = tuple(dot_row)\\n                    level_lengths[lnum] = rec_new\\n\\n                level_lengths[inner_lvl][ins_row] = 1\\n                for ix_col in range(len(fmt_values)):\\n                    fmt_values[ix_col].insert(ins_row, \"...\")\\n                nrows += 1\\n\\n            for i in range(nrows):\\n                row = []\\n                tags = {}\\n\\n                sparse_offset = 0\\n                j = 0\\n                for records, v in zip(level_lengths, idx_values[i]):\\n                    if i in records:\\n                        if records[i] > 1:\\n                            tags[j] = template.format(span=records[i])\\n                    else:\\n                        sparse_offset += 1\\n                        continue\\n\\n                    j += 1\\n                    row.append(v)\\n\\n                row.extend(fmt_values[j][i] for j in range(self.ncols))\\n                if is_truncated_horizontally:\\n                    row.insert(\\n                        self.row_levels - sparse_offset + self.fmt.tr_col_num, \"...\"\\n                    )\\n                self.write_tr(\\n                    row,\\n                    indent,\\n                    self.indent_delta,\\n                    tags=tags,\\n                    nindex_levels=len(levels) - sparse_offset,\\n                )\\n        else:\\n            row = []\\n            for i in range(len(frame)):\\n                if is_truncated_vertically and i == (self.fmt.tr_row_num):\\n                    str_sep_row = [\"...\"] * len(row)\\n                    self.write_tr(\\n                        str_sep_row,\\n                        indent,\\n                        self.indent_delta,\\n                        tags=None,\\n                        nindex_levels=self.row_levels,\\n                    )\\n\\n                idx_values = list(\\n                    zip(*frame.index.format(sparsify=False, adjoin=False, names=False))\\n                )\\n                row = []\\n                row.extend(idx_values[i])\\n                row.extend(fmt_values[j][i] for j in range(self.ncols))\\n                if is_truncated_horizontally:\\n                    row.insert(self.row_levels + self.fmt.tr_col_num, \"...\")\\n                self.write_tr(\\n                    row,\\n                    indent,\\n                    self.indent_delta,\\n                    tags=None,\\n                    nindex_levels=frame.index.nlevels,\\n                )\\n\\n\\nclass NotebookFormatter(HTMLFormatter):\\n    \"\"\"\\n    Internal class for formatting output data in html for display in Jupyter\\n    Notebooks. This class is intended for functionality specific to\\n    DataFrame._repr_html_() and DataFrame.to_html(notebook=True)\\n    \"\"\"\\n\\n    def _get_formatted_values(self) -> Dict[int, List[str]]:\\n        return {i: self.fmt.format_col(i) for i in range(self.ncols)}\\n\\n    def _get_columns_formatted_values(self) -> List[str]:\\n        return self.columns.format()\\n\\n    def write_style(self) -> None:\\n        # We use the \"scoped\" attribute here so that the desired\\n        # style properties for the data frame are not then applied\\n        # throughout the entire notebook.\\n        template_first = \"\"\"\\\\\\n            <style scoped>\"\"\"\\n        template_last = \"\"\"\\\\\\n            </style>\"\"\"\\n        template_select = \"\"\"\\\\\\n                .dataframe %s {\\n                    %s: %s;\\n                }\"\"\"\\n        element_props = [\\n            (\"tbody tr th:only-of-type\", \"vertical-align\", \"middle\"),\\n            (\"tbody tr th\", \"vertical-align\", \"top\"),\\n        ]\\n        if isinstance(self.columns, MultiIndex):\\n            element_props.append((\"thead tr th\", \"text-align\", \"left\"))\\n            if self.show_row_idx_names:\\n                element_props.append(\\n                    (\"thead tr:last-of-type th\", \"text-align\", \"right\")\\n                )\\n        else:\\n            element_props.append((\"thead th\", \"text-align\", \"right\"))\\n        template_mid = \"\\\\n\\\\n\".join(map(lambda t: template_select % t, element_props))\\n        template = dedent(\"\\\\n\".join((template_first, template_mid, template_last)))\\n        self.write(template)\\n\\n    def render(self) -> List[str]:\\n        self.write(\"<div>\")\\n        self.write_style()\\n        super().render()\\n        self.write(\"</div>\")\\n        return self.elements\\n', 'import numpy as np\\nimport scipy\\nfrom matplotlib import pyplot as plt\\nfrom numpy import pi as pi\\n\\n# Plotting logic switches\\ntime_plot = True\\nfreq_plot = True\\n\\n# Oversample to make things look purty\\noversample = 100\\n\\n# Frequencies to simulate\\nf_min = 5 #[Hz]\\nf_max = 10 #[Hz]\\nf_list = np.arange(f_min,f_max) # Note: arange does not include the stop pt\\n\\n# Time array\\nt_start = 0                 #[s]\\nt_stop = oversample/f_min   #[s]\\nf_samp = oversample*f_max   #[Hz]\\nt_step = 1/f_samp           #[s]\\n\\n# Create a time span, but do not care about the number of points.\\n# This will likely create sinc functions in the FFT.\\n#t = np.arange(t_start,t_stop,t_step)\\n\\n# Use N points to make a faster FFT and to avoid\\n# the addition of zeros at the end of the FFT array.\\n# The addition of zeros will result in the mulitplication\\n# of a box filter in the time domain, which results in\\n# a sinc function in the frequency domain\\nN = int(np.power(2,np.ceil(np.log2(t_stop/t_step))))\\n\\n# Create a time span, but care about the number of points such that\\n# the signal does not look like a sinc function in the freq. domain.\\n# Source: U of RI ECE, ELE 436: Comm. Sys., FFT Tutorial\\nt = np.linspace(t_start,t_stop,num=N,endpoint=True)\\n\\n# Create random amplitudes\\na_list = [np.random.randint(1,10) for i in f_list]\\n\\n# Create a time signal with random amplitudes for each frequency\\nx = 0\\nfor a,f in zip(a_list,f_list):\\n    x += a*np.sin(2*pi*f*t)\\n\\n# Take the FFT of the signal\\n# Normalize by the size of x due to how a DTFT is taken\\n# Take absoulte value because we only care about the real part\\n# of the signal.\\nX = np.abs(np.fft.fft(x)/x.size)\\n# Get the labels for the frequencies, num pts and delta between them\\nfreq_labels = np.fft.fftfreq(N,t[1]-t[0])\\n\\n# Plot the time signal\\nif time_plot and not freq_plot:\\n    plt.figure(\\'Time Domain View\\')\\n    plt.title(\"Time domain view of signal x\")\\n    plt.plot(t,x)\\n    plt.xlim([0,5/f_min])\\n    plt.xlabel(\"Time [s]\")\\n    plt.ylabel(\"Amplitude\")\\n\\n    plt.show()\\n\\n# Or plot the frequecy\\nif freq_plot and not time_plot:\\n    plt.figure(\\'Frequency Domain View\\')\\n    plt.title(\"Frequency domain view of signal x\")\\n    plt.plot(freq_labels,X)\\n    plt.xlim([-f_max,f_max])\\n    \\n    plt.show()\\n\\n# Or plot both\\nif freq_plot and time_plot:\\n    plt.subplot(211)\\n    plt.title(\"Time and frequency domain view of real signal x\")\\n    plt.plot(t,x)\\n    plt.xlim([0,5/f_min]) # Limit the time shown to a small amount\\n    plt.xlabel(\"Time [s]\")\\n    plt.ylabel(\"Amplitude\")\\n\\n    plt.subplot(212)\\n    plt.plot(freq_labels,X)\\n    plt.xlim([-f_max,f_max]) # Limit the freq shown to a small amount\\n    plt.xlabel(\"Frequency [Hz]\")\\n    plt.ylabel(\"Magnitude (linear)\")\\n\\n    plt.show()\\n', \"import numpy as np \\nimport matplotlib.pyplot as plt \\n\\ndef plot_decision_function(classifier, fea, gnd, title):\\n    '''\\n        plot the decision function in 2-d plane\\n        classifiers: the svm models\\n        fea: array like, shape = (smp_num, fea_num)\\n        gnd: array like, shape = (smp_num,)\\n        title: title of plot\\n    ''' \\n    fea_min = fea.min(axis = 0)\\n    fea_max = fea.max(axis = 0)\\n    mesh_num = 100\\n    # meshgrid\\n    xx, yy = np.meshgrid(np.linspace(fea_min[0], fea_max[0], mesh_num), \\\\\\n        np.linspace(fea_min[1], fea_max[1], mesh_num))\\n\\n    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()], last_model_flag = False)\\n    Z_first = Z[:, 0].copy()\\n    Z_last = Z[:, -1].copy()\\n    Z_first = Z_first.reshape(xx.shape)\\n    Z_last = Z_last.reshape(xx.shape)\\n    del Z\\n\\n    # plot the line, the points\\n    leg_svm = plt.contour(xx, yy, Z_first, levels = [0.0], colors = 'k')\\n    leg_rsvm = plt.contour(xx, yy, Z_last, levels = [0.0], colors = 'r')\\n    posi_index = gnd == 1\\n    nega_index = gnd == -1\\n    marker_size = 70\\n    plt.scatter(fea[:, 0], fea[:, 1], marker = 'o', \\\\\\n        s = classifier.smp_weights_mat[:, -1] * marker_size * 4, c = 'w', alpha = 1.0, edgecolors = 'm', label = 'weights')\\n    plt.scatter(fea[posi_index, 0], fea[posi_index, 1], marker = '^', s = marker_size, c = 'g', alpha = 0.8, label = 'posi')\\n    plt.scatter(fea[nega_index, 0], fea[nega_index, 1], marker = 'x', s = marker_size, c = 'b', label = 'nega')\\n    leg_svm.collections[0].set_label('svm')\\n    leg_rsvm.collections[0].set_label('rsvm')\\n    plt.legend(loc = 'upper left')\\n    plt.axis('on')\\n    plt.title(title)\", '\"\"\"\\nTesting for the tree module (sklearn.tree).\\n\"\"\"\\nimport pickle\\nfrom functools import partial\\nfrom itertools import product\\nimport platform\\n\\nimport numpy as np\\nfrom scipy.sparse import csc_matrix\\nfrom scipy.sparse import csr_matrix\\nfrom scipy.sparse import coo_matrix\\n\\nfrom sklearn.random_projection import sparse_random_matrix\\n\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import mean_squared_error\\n\\nfrom sklearn.utils.testing import assert_array_equal\\nfrom sklearn.utils.testing import assert_array_almost_equal\\nfrom sklearn.utils.testing import assert_almost_equal\\nfrom sklearn.utils.testing import assert_equal\\nfrom sklearn.utils.testing import assert_in\\nfrom sklearn.utils.testing import assert_raises\\nfrom sklearn.utils.testing import assert_greater\\nfrom sklearn.utils.testing import assert_greater_equal\\nfrom sklearn.utils.testing import assert_less\\nfrom sklearn.utils.testing import assert_true\\nfrom sklearn.utils.testing import raises\\nfrom sklearn.utils.validation import check_random_state\\nfrom sklearn.utils.validation import NotFittedError\\nfrom sklearn.utils.testing import ignore_warnings\\n\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.tree import ExtraTreeClassifier\\nfrom sklearn.tree import ExtraTreeRegressor\\n\\nfrom sklearn import tree\\nfrom sklearn.tree.tree import SPARSE_SPLITTERS\\nfrom sklearn.tree._tree import TREE_LEAF\\nfrom sklearn import datasets\\n\\nfrom sklearn.preprocessing._weights import _balance_weights\\n\\n\\nCLF_CRITERIONS = (\"gini\", \"entropy\")\\nREG_CRITERIONS = (\"mse\", )\\n\\nCLF_TREES = {\\n    \"DecisionTreeClassifier\": DecisionTreeClassifier,\\n    \"Presort-DecisionTreeClassifier\": partial(DecisionTreeClassifier,\\n                                              splitter=\"presort-best\"),\\n    \"ExtraTreeClassifier\": ExtraTreeClassifier,\\n}\\n\\nREG_TREES = {\\n    \"DecisionTreeRegressor\": DecisionTreeRegressor,\\n    \"Presort-DecisionTreeRegressor\": partial(DecisionTreeRegressor,\\n                                             splitter=\"presort-best\"),\\n    \"ExtraTreeRegressor\": ExtraTreeRegressor,\\n}\\n\\nALL_TREES = dict()\\nALL_TREES.update(CLF_TREES)\\nALL_TREES.update(REG_TREES)\\n\\nSPARSE_TREES = [name for name, Tree in ALL_TREES.items()\\n                if Tree().splitter in SPARSE_SPLITTERS]\\n\\n\\nX_small = np.array([\\n    [0, 0, 4, 0, 0, 0, 1, -14, 0, -4, 0, 0, 0, 0, ],\\n    [0, 0, 5, 3, 0, -4, 0, 0, 1, -5, 0.2, 0, 4, 1, ],\\n    [-1, -1, 0, 0, -4.5, 0, 0, 2.1, 1, 0, 0, -4.5, 0, 1, ],\\n    [-1, -1, 0, -1.2, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 1, ],\\n    [-1, -1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, ],\\n    [-1, -2, 0, 4, -3, 10, 4, 0, -3.2, 0, 4, 3, -4, 1, ],\\n    [2.11, 0, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0.5, 0, -3, 1, ],\\n    [2.11, 0, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0, 0, -2, 1, ],\\n    [2.11, 8, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0, 0, -2, 1, ],\\n    [2.11, 8, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0.5, 0, -1, 0, ],\\n    [2, 8, 5, 1, 0.5, -4, 10, 0, 1, -5, 3, 0, 2, 0, ],\\n    [2, 0, 1, 1, 1, -1, 1, 0, 0, -2, 3, 0, 1, 0, ],\\n    [2, 0, 1, 2, 3, -1, 10, 2, 0, -1, 1, 2, 2, 0, ],\\n    [1, 1, 0, 2, 2, -1, 1, 2, 0, -5, 1, 2, 3, 0, ],\\n    [3, 1, 0, 3, 0, -4, 10, 0, 1, -5, 3, 0, 3, 1, ],\\n    [2.11, 8, -6, -0.5, 0, 1, 0, 0, -3.2, 6, 0.5, 0, -3, 1, ],\\n    [2.11, 8, -6, -0.5, 0, 1, 0, 0, -3.2, 6, 1.5, 1, -1, -1, ],\\n    [2.11, 8, -6, -0.5, 0, 10, 0, 0, -3.2, 6, 0.5, 0, -1, -1, ],\\n    [2, 0, 5, 1, 0.5, -2, 10, 0, 1, -5, 3, 1, 0, -1, ],\\n    [2, 0, 1, 1, 1, -2, 1, 0, 0, -2, 0, 0, 0, 1, ],\\n    [2, 1, 1, 1, 2, -1, 10, 2, 0, -1, 0, 2, 1, 1, ],\\n    [1, 1, 0, 0, 1, -3, 1, 2, 0, -5, 1, 2, 1, 1, ],\\n    [3, 1, 0, 1, 0, -4, 1, 0, 1, -2, 0, 0, 1, 0, ]])\\n\\ny_small = [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\\n           0, 0]\\ny_small_reg = [1.0, 2.1, 1.2, 0.05, 10, 2.4, 3.1, 1.01, 0.01, 2.98, 3.1, 1.1,\\n               0.0, 1.2, 2, 11, 0, 0, 4.5, 0.201, 1.06, 0.9, 0]\\n\\n# toy sample\\nX = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\\ny = [-1, -1, -1, 1, 1, 1]\\nT = [[-1, -1], [2, 2], [3, 2]]\\ntrue_result = [-1, 1, 1]\\n\\n# also load the iris dataset\\n# and randomly permute it\\niris = datasets.load_iris()\\nrng = np.random.RandomState(1)\\nperm = rng.permutation(iris.target.size)\\niris.data = iris.data[perm]\\niris.target = iris.target[perm]\\n\\n# also load the boston dataset\\n# and randomly permute it\\nboston = datasets.load_boston()\\nperm = rng.permutation(boston.target.size)\\nboston.data = boston.data[perm]\\nboston.target = boston.target[perm]\\n\\ndigits = datasets.load_digits()\\nperm = rng.permutation(digits.target.size)\\ndigits.data = digits.data[perm]\\ndigits.target = digits.target[perm]\\n\\nrandom_state = check_random_state(0)\\nX_multilabel, y_multilabel = datasets.make_multilabel_classification(\\n    random_state=0, n_samples=30, n_features=10)\\n\\nX_sparse_pos = random_state.uniform(size=(20, 5))\\nX_sparse_pos[X_sparse_pos <= 0.8] = 0.\\ny_random = random_state.randint(0, 4, size=(20, ))\\nX_sparse_mix = sparse_random_matrix(20, 10, density=0.25, random_state=0)\\n\\n\\nDATASETS = {\\n    \"iris\": {\"X\": iris.data, \"y\": iris.target},\\n    \"boston\": {\"X\": boston.data, \"y\": boston.target},\\n    \"digits\": {\"X\": digits.data, \"y\": digits.target},\\n    \"toy\": {\"X\": X, \"y\": y},\\n    \"clf_small\": {\"X\": X_small, \"y\": y_small},\\n    \"reg_small\": {\"X\": X_small, \"y\": y_small_reg},\\n    \"multilabel\": {\"X\": X_multilabel, \"y\": y_multilabel},\\n    \"sparse-pos\": {\"X\": X_sparse_pos, \"y\": y_random},\\n    \"sparse-neg\": {\"X\": - X_sparse_pos, \"y\": y_random},\\n    \"sparse-mix\": {\"X\": X_sparse_mix, \"y\": y_random},\\n    \"zeros\": {\"X\": np.zeros((20, 3)), \"y\": y_random}\\n}\\n\\nfor name in DATASETS:\\n    DATASETS[name][\"X_sparse\"] = csc_matrix(DATASETS[name][\"X\"])\\n\\n\\ndef assert_tree_equal(d, s, message):\\n    assert_equal(s.node_count, d.node_count,\\n                 \"{0}: inequal number of node ({1} != {2})\"\\n                 \"\".format(message, s.node_count, d.node_count))\\n\\n    assert_array_equal(d.children_right, s.children_right,\\n                       message + \": inequal children_right\")\\n    assert_array_equal(d.children_left, s.children_left,\\n                       message + \": inequal children_left\")\\n\\n    external = d.children_right == TREE_LEAF\\n    internal = np.logical_not(external)\\n\\n    assert_array_equal(d.feature[internal], s.feature[internal],\\n                       message + \": inequal features\")\\n    assert_array_equal(d.threshold[internal], s.threshold[internal],\\n                       message + \": inequal threshold\")\\n    assert_array_equal(d.n_node_samples.sum(), s.n_node_samples.sum(),\\n                       message + \": inequal sum(n_node_samples)\")\\n    assert_array_equal(d.n_node_samples, s.n_node_samples,\\n                       message + \": inequal n_node_samples\")\\n\\n    assert_almost_equal(d.impurity, s.impurity,\\n                        err_msg=message + \": inequal impurity\")\\n\\n    assert_array_almost_equal(d.value[external], s.value[external],\\n                              err_msg=message + \": inequal value\")\\n\\n\\ndef test_classification_toy():\\n    # Check classification on a toy dataset.\\n    for name, Tree in CLF_TREES.items():\\n        clf = Tree(random_state=0)\\n        clf.fit(X, y)\\n        assert_array_equal(clf.predict(T), true_result,\\n                           \"Failed with {0}\".format(name))\\n\\n        clf = Tree(max_features=1, random_state=1)\\n        clf.fit(X, y)\\n        assert_array_equal(clf.predict(T), true_result,\\n                           \"Failed with {0}\".format(name))\\n\\n\\ndef test_weighted_classification_toy():\\n    # Check classification on a weighted toy dataset.\\n    for name, Tree in CLF_TREES.items():\\n        clf = Tree(random_state=0)\\n\\n        clf.fit(X, y, sample_weight=np.ones(len(X)))\\n        assert_array_equal(clf.predict(T), true_result,\\n                           \"Failed with {0}\".format(name))\\n\\n        clf.fit(X, y, sample_weight=np.ones(len(X)) * 0.5)\\n        assert_array_equal(clf.predict(T), true_result,\\n                           \"Failed with {0}\".format(name))\\n\\n\\ndef test_regression_toy():\\n    # Check regression on a toy dataset.\\n    for name, Tree in REG_TREES.items():\\n        reg = Tree(random_state=1)\\n        reg.fit(X, y)\\n        assert_almost_equal(reg.predict(T), true_result,\\n                            err_msg=\"Failed with {0}\".format(name))\\n\\n        clf = Tree(max_features=1, random_state=1)\\n        clf.fit(X, y)\\n        assert_almost_equal(reg.predict(T), true_result,\\n                            err_msg=\"Failed with {0}\".format(name))\\n\\n\\ndef test_xor():\\n    # Check on a XOR problem\\n    y = np.zeros((10, 10))\\n    y[:5, :5] = 1\\n    y[5:, 5:] = 1\\n\\n    gridx, gridy = np.indices(y.shape)\\n\\n    X = np.vstack([gridx.ravel(), gridy.ravel()]).T\\n    y = y.ravel()\\n\\n    for name, Tree in CLF_TREES.items():\\n        clf = Tree(random_state=0)\\n        clf.fit(X, y)\\n        assert_equal(clf.score(X, y), 1.0,\\n                     \"Failed with {0}\".format(name))\\n\\n        clf = Tree(random_state=0, max_features=1)\\n        clf.fit(X, y)\\n        assert_equal(clf.score(X, y), 1.0,\\n                     \"Failed with {0}\".format(name))\\n\\n\\ndef test_iris():\\n    # Check consistency on dataset iris.\\n    for (name, Tree), criterion in product(CLF_TREES.items(), CLF_CRITERIONS):\\n        clf = Tree(criterion=criterion, random_state=0)\\n        clf.fit(iris.data, iris.target)\\n        score = accuracy_score(clf.predict(iris.data), iris.target)\\n        assert_greater(score, 0.9,\\n                       \"Failed with {0}, criterion = {1} and score = {2}\"\\n                       \"\".format(name, criterion, score))\\n\\n        clf = Tree(criterion=criterion, max_features=2, random_state=0)\\n        clf.fit(iris.data, iris.target)\\n        score = accuracy_score(clf.predict(iris.data), iris.target)\\n        assert_greater(score, 0.5,\\n                       \"Failed with {0}, criterion = {1} and score = {2}\"\\n                       \"\".format(name, criterion, score))\\n\\n\\ndef test_boston():\\n    # Check consistency on dataset boston house prices.\\n\\n    for (name, Tree), criterion in product(REG_TREES.items(), REG_CRITERIONS):\\n        reg = Tree(criterion=criterion, random_state=0)\\n        reg.fit(boston.data, boston.target)\\n        score = mean_squared_error(boston.target, reg.predict(boston.data))\\n        assert_less(score, 1,\\n                    \"Failed with {0}, criterion = {1} and score = {2}\"\\n                    \"\".format(name, criterion, score))\\n\\n        # using fewer features reduces the learning ability of this tree,\\n        # but reduces training time.\\n        reg = Tree(criterion=criterion, max_features=6, random_state=0)\\n        reg.fit(boston.data, boston.target)\\n        score = mean_squared_error(boston.target, reg.predict(boston.data))\\n        assert_less(score, 2,\\n                    \"Failed with {0}, criterion = {1} and score = {2}\"\\n                    \"\".format(name, criterion, score))\\n\\n\\ndef test_probability():\\n    # Predict probabilities using DecisionTreeClassifier.\\n\\n    for name, Tree in CLF_TREES.items():\\n        clf = Tree(max_depth=1, max_features=1, random_state=42)\\n        clf.fit(iris.data, iris.target)\\n\\n        prob_predict = clf.predict_proba(iris.data)\\n        assert_array_almost_equal(np.sum(prob_predict, 1),\\n                                  np.ones(iris.data.shape[0]),\\n                                  err_msg=\"Failed with {0}\".format(name))\\n        assert_array_equal(np.argmax(prob_predict, 1),\\n                           clf.predict(iris.data),\\n                           err_msg=\"Failed with {0}\".format(name))\\n        assert_almost_equal(clf.predict_proba(iris.data),\\n                            np.exp(clf.predict_log_proba(iris.data)), 8,\\n                            err_msg=\"Failed with {0}\".format(name))\\n\\n\\ndef test_arrayrepr():\\n    # Check the array representation.\\n    # Check resize\\n    X = np.arange(10000)[:, np.newaxis]\\n    y = np.arange(10000)\\n\\n    for name, Tree in REG_TREES.items():\\n        reg = Tree(max_depth=None, random_state=0)\\n        reg.fit(X, y)\\n\\n\\ndef test_pure_set():\\n    # Check when y is pure.\\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\\n    y = [1, 1, 1, 1, 1, 1]\\n\\n    for name, TreeClassifier in CLF_TREES.items():\\n        clf = TreeClassifier(random_state=0)\\n        clf.fit(X, y)\\n        assert_array_equal(clf.predict(X), y,\\n                           err_msg=\"Failed with {0}\".format(name))\\n\\n    for name, TreeRegressor in REG_TREES.items():\\n        reg = TreeRegressor(random_state=0)\\n        reg.fit(X, y)\\n        assert_almost_equal(clf.predict(X), y,\\n                            err_msg=\"Failed with {0}\".format(name))\\n\\n\\ndef test_numerical_stability():\\n    # Check numerical stability.\\n    X = np.array([\\n        [152.08097839, 140.40744019, 129.75102234, 159.90493774],\\n        [142.50700378, 135.81935120, 117.82884979, 162.75781250],\\n        [127.28772736, 140.40744019, 129.75102234, 159.90493774],\\n        [132.37025452, 143.71923828, 138.35694885, 157.84558105],\\n        [103.10237122, 143.71928406, 138.35696411, 157.84559631],\\n        [127.71276855, 143.71923828, 138.35694885, 157.84558105],\\n        [120.91514587, 140.40744019, 129.75102234, 159.90493774]])\\n\\n    y = np.array(\\n        [1., 0.70209277, 0.53896582, 0., 0.90914464, 0.48026916, 0.49622521])\\n\\n    with np.errstate(all=\"raise\"):\\n        for name, Tree in REG_TREES.items():\\n            reg = Tree(random_state=0)\\n            reg.fit(X, y)\\n            reg.fit(X, -y)\\n            reg.fit(-X, y)\\n            reg.fit(-X, -y)\\n\\n\\ndef test_importances():\\n    # Check variable importances.\\n    X, y = datasets.make_classification(n_samples=2000,\\n                                        n_features=10,\\n                                        n_informative=3,\\n                                        n_redundant=0,\\n                                        n_repeated=0,\\n                                        shuffle=False,\\n                                        random_state=0)\\n\\n    for name, Tree in CLF_TREES.items():\\n        clf = Tree(random_state=0)\\n\\n        clf.fit(X, y)\\n        importances = clf.feature_importances_\\n        n_important = np.sum(importances > 0.1)\\n\\n        assert_equal(importances.shape[0], 10, \"Failed with {0}\".format(name))\\n        assert_equal(n_important, 3, \"Failed with {0}\".format(name))\\n\\n        X_new = clf.transform(X, threshold=\"mean\")\\n        assert_less(0, X_new.shape[1], \"Failed with {0}\".format(name))\\n        assert_less(X_new.shape[1], X.shape[1], \"Failed with {0}\".format(name))\\n\\n    # Check on iris that importances are the same for all builders\\n    clf = DecisionTreeClassifier(random_state=0)\\n    clf.fit(iris.data, iris.target)\\n    clf2 = DecisionTreeClassifier(random_state=0,\\n                                  max_leaf_nodes=len(iris.data))\\n    clf2.fit(iris.data, iris.target)\\n\\n    assert_array_equal(clf.feature_importances_,\\n                       clf2.feature_importances_)\\n\\n\\n@raises(ValueError)\\ndef test_importances_raises():\\n    # Check if variable importance before fit raises ValueError.\\n    clf = DecisionTreeClassifier()\\n    clf.feature_importances_\\n\\n\\ndef test_importances_gini_equal_mse():\\n    # Check that gini is equivalent to mse for binary output variable\\n\\n    X, y = datasets.make_classification(n_samples=2000,\\n                                        n_features=10,\\n                                        n_informative=3,\\n                                        n_redundant=0,\\n                                        n_repeated=0,\\n                                        shuffle=False,\\n                                        random_state=0)\\n\\n    # The gini index and the mean square error (variance) might differ due\\n    # to numerical instability. Since those instabilities mainly occurs at\\n    # high tree depth, we restrict this maximal depth.\\n    clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=5,\\n                                 random_state=0).fit(X, y)\\n    reg = DecisionTreeRegressor(criterion=\"mse\", max_depth=5,\\n                                random_state=0).fit(X, y)\\n\\n    assert_almost_equal(clf.feature_importances_, reg.feature_importances_)\\n    assert_array_equal(clf.tree_.feature, reg.tree_.feature)\\n    assert_array_equal(clf.tree_.children_left, reg.tree_.children_left)\\n    assert_array_equal(clf.tree_.children_right, reg.tree_.children_right)\\n    assert_array_equal(clf.tree_.n_node_samples, reg.tree_.n_node_samples)\\n\\n\\ndef test_max_features():\\n    # Check max_features.\\n    for name, TreeRegressor in REG_TREES.items():\\n        reg = TreeRegressor(max_features=\"auto\")\\n        reg.fit(boston.data, boston.target)\\n        assert_equal(reg.max_features_, boston.data.shape[1])\\n\\n    for name, TreeClassifier in CLF_TREES.items():\\n        clf = TreeClassifier(max_features=\"auto\")\\n        clf.fit(iris.data, iris.target)\\n        assert_equal(clf.max_features_, 2)\\n\\n    for name, TreeEstimator in ALL_TREES.items():\\n        est = TreeEstimator(max_features=\"sqrt\")\\n        est.fit(iris.data, iris.target)\\n        assert_equal(est.max_features_,\\n                     int(np.sqrt(iris.data.shape[1])))\\n\\n        est = TreeEstimator(max_features=\"log2\")\\n        est.fit(iris.data, iris.target)\\n        assert_equal(est.max_features_,\\n                     int(np.log2(iris.data.shape[1])))\\n\\n        est = TreeEstimator(max_features=1)\\n        est.fit(iris.data, iris.target)\\n        assert_equal(est.max_features_, 1)\\n\\n        est = TreeEstimator(max_features=3)\\n        est.fit(iris.data, iris.target)\\n        assert_equal(est.max_features_, 3)\\n\\n        est = TreeEstimator(max_features=0.01)\\n        est.fit(iris.data, iris.target)\\n        assert_equal(est.max_features_, 1)\\n\\n        est = TreeEstimator(max_features=0.5)\\n        est.fit(iris.data, iris.target)\\n        assert_equal(est.max_features_,\\n                     int(0.5 * iris.data.shape[1]))\\n\\n        est = TreeEstimator(max_features=1.0)\\n        est.fit(iris.data, iris.target)\\n        assert_equal(est.max_features_, iris.data.shape[1])\\n\\n        est = TreeEstimator(max_features=None)\\n        est.fit(iris.data, iris.target)\\n        assert_equal(est.max_features_, iris.data.shape[1])\\n\\n        # use values of max_features that are invalid\\n        est = TreeEstimator(max_features=10)\\n        assert_raises(ValueError, est.fit, X, y)\\n\\n        est = TreeEstimator(max_features=-1)\\n        assert_raises(ValueError, est.fit, X, y)\\n\\n        est = TreeEstimator(max_features=0.0)\\n        assert_raises(ValueError, est.fit, X, y)\\n\\n        est = TreeEstimator(max_features=1.5)\\n        assert_raises(ValueError, est.fit, X, y)\\n\\n        est = TreeEstimator(max_features=\"foobar\")\\n        assert_raises(ValueError, est.fit, X, y)\\n\\n\\ndef test_error():\\n    # Test that it gives proper exception on deficient input.\\n    for name, TreeEstimator in CLF_TREES.items():\\n        # predict before fit\\n        est = TreeEstimator()\\n        assert_raises(NotFittedError, est.predict_proba, X)\\n\\n        est.fit(X, y)\\n        X2 = [[-2, -1, 1]]  # wrong feature shape for sample\\n        assert_raises(ValueError, est.predict_proba, X2)\\n\\n    for name, TreeEstimator in ALL_TREES.items():\\n        # Invalid values for parameters\\n        assert_raises(ValueError, TreeEstimator(min_samples_leaf=-1).fit, X, y)\\n        assert_raises(ValueError,\\n                      TreeEstimator(min_weight_fraction_leaf=-1).fit,\\n                      X, y)\\n        assert_raises(ValueError,\\n                      TreeEstimator(min_weight_fraction_leaf=0.51).fit,\\n                      X, y)\\n        assert_raises(ValueError, TreeEstimator(min_samples_split=-1).fit,\\n                      X, y)\\n        assert_raises(ValueError, TreeEstimator(max_depth=-1).fit, X, y)\\n        assert_raises(ValueError, TreeEstimator(max_features=42).fit, X, y)\\n\\n        # Wrong dimensions\\n        est = TreeEstimator()\\n        y2 = y[:-1]\\n        assert_raises(ValueError, est.fit, X, y2)\\n\\n        # Test with arrays that are non-contiguous.\\n        Xf = np.asfortranarray(X)\\n        est = TreeEstimator()\\n        est.fit(Xf, y)\\n        assert_almost_equal(est.predict(T), true_result)\\n\\n        # predict before fitting\\n        est = TreeEstimator()\\n        assert_raises(NotFittedError, est.predict, T)\\n\\n        # predict on vector with different dims\\n        est.fit(X, y)\\n        t = np.asarray(T)\\n        assert_raises(ValueError, est.predict, t[:, 1:])\\n\\n        # wrong sample shape\\n        Xt = np.array(X).T\\n\\n        est = TreeEstimator()\\n        est.fit(np.dot(X, Xt), y)\\n        assert_raises(ValueError, est.predict, X)\\n        assert_raises(ValueError, est.apply, X)\\n\\n        clf = TreeEstimator()\\n        clf.fit(X, y)\\n        assert_raises(ValueError, clf.predict, Xt)\\n        assert_raises(ValueError, clf.apply, Xt)\\n\\n        # apply before fitting\\n        est = TreeEstimator()\\n        assert_raises(NotFittedError, est.apply, T)\\n\\n\\ndef test_min_samples_leaf():\\n    # Test if leaves contain more than leaf_count training examples\\n    X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE))\\n    y = iris.target\\n\\n    # test both DepthFirstTreeBuilder and BestFirstTreeBuilder\\n    # by setting max_leaf_nodes\\n    for max_leaf_nodes in (None, 1000):\\n        for name, TreeEstimator in ALL_TREES.items():\\n            est = TreeEstimator(min_samples_leaf=5,\\n                                max_leaf_nodes=max_leaf_nodes,\\n                                random_state=0)\\n            est.fit(X, y)\\n            out = est.tree_.apply(X)\\n            node_counts = np.bincount(out)\\n            # drop inner nodes\\n            leaf_count = node_counts[node_counts != 0]\\n            assert_greater(np.min(leaf_count), 4,\\n                           \"Failed with {0}\".format(name))\\n\\n\\ndef check_min_weight_fraction_leaf(name, datasets, sparse=False):\\n    \"\"\"Test if leaves contain at least min_weight_fraction_leaf of the\\n    training set\"\"\"\\n    if sparse:\\n        X = DATASETS[datasets][\"X_sparse\"].astype(np.float32)\\n    else:\\n        X = DATASETS[datasets][\"X\"].astype(np.float32)\\n    y = DATASETS[datasets][\"y\"]\\n\\n    weights = rng.rand(X.shape[0])\\n    total_weight = np.sum(weights)\\n\\n    TreeEstimator = ALL_TREES[name]\\n\\n    # test both DepthFirstTreeBuilder and BestFirstTreeBuilder\\n    # by setting max_leaf_nodes\\n    for max_leaf_nodes, frac in product((None, 1000), np.linspace(0, 0.5, 6)):\\n        est = TreeEstimator(min_weight_fraction_leaf=frac,\\n                            max_leaf_nodes=max_leaf_nodes,\\n                            random_state=0)\\n        est.fit(X, y, sample_weight=weights)\\n\\n        if sparse:\\n            out = est.tree_.apply(X.tocsr())\\n\\n        else:\\n            out = est.tree_.apply(X)\\n\\n        node_weights = np.bincount(out, weights=weights)\\n        # drop inner nodes\\n        leaf_weights = node_weights[node_weights != 0]\\n        assert_greater_equal(\\n            np.min(leaf_weights),\\n            total_weight * est.min_weight_fraction_leaf,\\n            \"Failed with {0} \"\\n            \"min_weight_fraction_leaf={1}\".format(\\n                name, est.min_weight_fraction_leaf))\\n\\n\\ndef test_min_weight_fraction_leaf():\\n    # Check on dense input\\n    for name in ALL_TREES:\\n        yield check_min_weight_fraction_leaf, name, \"iris\"\\n\\n    # Check on sparse input\\n    for name in SPARSE_TREES:\\n        yield check_min_weight_fraction_leaf, name, \"multilabel\", True\\n\\n\\ndef test_pickle():\\n    # Check that tree estimator are pickable\\n    for name, TreeClassifier in CLF_TREES.items():\\n        clf = TreeClassifier(random_state=0)\\n        clf.fit(iris.data, iris.target)\\n        score = clf.score(iris.data, iris.target)\\n\\n        serialized_object = pickle.dumps(clf)\\n        clf2 = pickle.loads(serialized_object)\\n        assert_equal(type(clf2), clf.__class__)\\n        score2 = clf2.score(iris.data, iris.target)\\n        assert_equal(score, score2, \"Failed to generate same score \"\\n                                    \"after pickling (classification) \"\\n                                    \"with {0}\".format(name))\\n\\n    for name, TreeRegressor in REG_TREES.items():\\n        reg = TreeRegressor(random_state=0)\\n        reg.fit(boston.data, boston.target)\\n        score = reg.score(boston.data, boston.target)\\n\\n        serialized_object = pickle.dumps(reg)\\n        reg2 = pickle.loads(serialized_object)\\n        assert_equal(type(reg2), reg.__class__)\\n        score2 = reg2.score(boston.data, boston.target)\\n        assert_equal(score, score2, \"Failed to generate same score \"\\n                                    \"after pickling (regression) \"\\n                                    \"with {0}\".format(name))\\n\\n\\ndef test_multioutput():\\n    # Check estimators on multi-output problems.\\n    X = [[-2, -1],\\n         [-1, -1],\\n         [-1, -2],\\n         [1, 1],\\n         [1, 2],\\n         [2, 1],\\n         [-2, 1],\\n         [-1, 1],\\n         [-1, 2],\\n         [2, -1],\\n         [1, -1],\\n         [1, -2]]\\n\\n    y = [[-1, 0],\\n         [-1, 0],\\n         [-1, 0],\\n         [1, 1],\\n         [1, 1],\\n         [1, 1],\\n         [-1, 2],\\n         [-1, 2],\\n         [-1, 2],\\n         [1, 3],\\n         [1, 3],\\n         [1, 3]]\\n\\n    T = [[-1, -1], [1, 1], [-1, 1], [1, -1]]\\n    y_true = [[-1, 0], [1, 1], [-1, 2], [1, 3]]\\n\\n    # toy classification problem\\n    for name, TreeClassifier in CLF_TREES.items():\\n        clf = TreeClassifier(random_state=0)\\n        y_hat = clf.fit(X, y).predict(T)\\n        assert_array_equal(y_hat, y_true)\\n        assert_equal(y_hat.shape, (4, 2))\\n\\n        proba = clf.predict_proba(T)\\n        assert_equal(len(proba), 2)\\n        assert_equal(proba[0].shape, (4, 2))\\n        assert_equal(proba[1].shape, (4, 4))\\n\\n        log_proba = clf.predict_log_proba(T)\\n        assert_equal(len(log_proba), 2)\\n        assert_equal(log_proba[0].shape, (4, 2))\\n        assert_equal(log_proba[1].shape, (4, 4))\\n\\n    # toy regression problem\\n    for name, TreeRegressor in REG_TREES.items():\\n        reg = TreeRegressor(random_state=0)\\n        y_hat = reg.fit(X, y).predict(T)\\n        assert_almost_equal(y_hat, y_true)\\n        assert_equal(y_hat.shape, (4, 2))\\n\\n\\ndef test_classes_shape():\\n    # Test that n_classes_ and classes_ have proper shape.\\n    for name, TreeClassifier in CLF_TREES.items():\\n        # Classification, single output\\n        clf = TreeClassifier(random_state=0)\\n        clf.fit(X, y)\\n\\n        assert_equal(clf.n_classes_, 2)\\n        assert_array_equal(clf.classes_, [-1, 1])\\n\\n        # Classification, multi-output\\n        _y = np.vstack((y, np.array(y) * 2)).T\\n        clf = TreeClassifier(random_state=0)\\n        clf.fit(X, _y)\\n        assert_equal(len(clf.n_classes_), 2)\\n        assert_equal(len(clf.classes_), 2)\\n        assert_array_equal(clf.n_classes_, [2, 2])\\n        assert_array_equal(clf.classes_, [[-1, 1], [-2, 2]])\\n\\n\\ndef test_unbalanced_iris():\\n    # Check class rebalancing.\\n    unbalanced_X = iris.data[:125]\\n    unbalanced_y = iris.target[:125]\\n    sample_weight = _balance_weights(unbalanced_y)\\n\\n    for name, TreeClassifier in CLF_TREES.items():\\n        clf = TreeClassifier(random_state=0)\\n        clf.fit(unbalanced_X, unbalanced_y, sample_weight=sample_weight)\\n        assert_almost_equal(clf.predict(unbalanced_X), unbalanced_y)\\n\\n\\ndef test_memory_layout():\\n    # Check that it works no matter the memory layout\\n    for (name, TreeEstimator), dtype in product(ALL_TREES.items(),\\n                                                [np.float64, np.float32]):\\n        est = TreeEstimator(random_state=0)\\n\\n        # Nothing\\n        X = np.asarray(iris.data, dtype=dtype)\\n        y = iris.target\\n        assert_array_equal(est.fit(X, y).predict(X), y)\\n\\n        # C-order\\n        X = np.asarray(iris.data, order=\"C\", dtype=dtype)\\n        y = iris.target\\n        assert_array_equal(est.fit(X, y).predict(X), y)\\n\\n        # F-order\\n        X = np.asarray(iris.data, order=\"F\", dtype=dtype)\\n        y = iris.target\\n        assert_array_equal(est.fit(X, y).predict(X), y)\\n\\n        # Contiguous\\n        X = np.ascontiguousarray(iris.data, dtype=dtype)\\n        y = iris.target\\n        assert_array_equal(est.fit(X, y).predict(X), y)\\n\\n        if est.splitter in SPARSE_SPLITTERS:\\n            # csr matrix\\n            X = csr_matrix(iris.data, dtype=dtype)\\n            y = iris.target\\n            assert_array_equal(est.fit(X, y).predict(X), y)\\n\\n            # csc_matrix\\n            X = csc_matrix(iris.data, dtype=dtype)\\n            y = iris.target\\n            assert_array_equal(est.fit(X, y).predict(X), y)\\n\\n        # Strided\\n        X = np.asarray(iris.data[::3], dtype=dtype)\\n        y = iris.target[::3]\\n        assert_array_equal(est.fit(X, y).predict(X), y)\\n\\n\\ndef test_sample_weight():\\n    # Check sample weighting.\\n    # Test that zero-weighted samples are not taken into account\\n    X = np.arange(100)[:, np.newaxis]\\n    y = np.ones(100)\\n    y[:50] = 0.0\\n\\n    sample_weight = np.ones(100)\\n    sample_weight[y == 0] = 0.0\\n\\n    clf = DecisionTreeClassifier(random_state=0)\\n    clf.fit(X, y, sample_weight=sample_weight)\\n    assert_array_equal(clf.predict(X), np.ones(100))\\n\\n    # Test that low weighted samples are not taken into account at low depth\\n    X = np.arange(200)[:, np.newaxis]\\n    y = np.zeros(200)\\n    y[50:100] = 1\\n    y[100:200] = 2\\n    X[100:200, 0] = 200\\n\\n    sample_weight = np.ones(200)\\n\\n    sample_weight[y == 2] = .51  # Samples of class \\'2\\' are still weightier\\n    clf = DecisionTreeClassifier(max_depth=1, random_state=0)\\n    clf.fit(X, y, sample_weight=sample_weight)\\n    assert_equal(clf.tree_.threshold[0], 149.5)\\n\\n    sample_weight[y == 2] = .5  # Samples of class \\'2\\' are no longer weightier\\n    clf = DecisionTreeClassifier(max_depth=1, random_state=0)\\n    clf.fit(X, y, sample_weight=sample_weight)\\n    assert_equal(clf.tree_.threshold[0], 49.5)  # Threshold should have moved\\n\\n    # Test that sample weighting is the same as having duplicates\\n    X = iris.data\\n    y = iris.target\\n\\n    duplicates = rng.randint(0, X.shape[0], 100)\\n\\n    clf = DecisionTreeClassifier(random_state=1)\\n    clf.fit(X[duplicates], y[duplicates])\\n\\n    sample_weight = np.bincount(duplicates, minlength=X.shape[0])\\n    clf2 = DecisionTreeClassifier(random_state=1)\\n    clf2.fit(X, y, sample_weight=sample_weight)\\n\\n    internal = clf.tree_.children_left != tree._tree.TREE_LEAF\\n    assert_array_almost_equal(clf.tree_.threshold[internal],\\n                              clf2.tree_.threshold[internal])\\n\\n\\ndef test_sample_weight_invalid():\\n    # Check sample weighting raises errors.\\n    X = np.arange(100)[:, np.newaxis]\\n    y = np.ones(100)\\n    y[:50] = 0.0\\n\\n    clf = DecisionTreeClassifier(random_state=0)\\n\\n    sample_weight = np.random.rand(100, 1)\\n    assert_raises(ValueError, clf.fit, X, y, sample_weight=sample_weight)\\n\\n    sample_weight = np.array(0)\\n    assert_raises(ValueError, clf.fit, X, y, sample_weight=sample_weight)\\n\\n    sample_weight = np.ones(101)\\n    assert_raises(ValueError, clf.fit, X, y, sample_weight=sample_weight)\\n\\n    sample_weight = np.ones(99)\\n    assert_raises(ValueError, clf.fit, X, y, sample_weight=sample_weight)\\n\\n\\ndef check_class_weights(name):\\n    \"\"\"Check class_weights resemble sample_weights behavior.\"\"\"\\n    TreeClassifier = CLF_TREES[name]\\n\\n    # Iris is balanced, so no effect expected for using \\'balanced\\' weights\\n    clf1 = TreeClassifier(random_state=0)\\n    clf1.fit(iris.data, iris.target)\\n    clf2 = TreeClassifier(class_weight=\\'balanced\\', random_state=0)\\n    clf2.fit(iris.data, iris.target)\\n    assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)\\n\\n    # Make a multi-output problem with three copies of Iris\\n    iris_multi = np.vstack((iris.target, iris.target, iris.target)).T\\n    # Create user-defined weights that should balance over the outputs\\n    clf3 = TreeClassifier(class_weight=[{0: 2., 1: 2., 2: 1.},\\n                                        {0: 2., 1: 1., 2: 2.},\\n                                        {0: 1., 1: 2., 2: 2.}],\\n                          random_state=0)\\n    clf3.fit(iris.data, iris_multi)\\n    assert_almost_equal(clf2.feature_importances_, clf3.feature_importances_)\\n    # Check against multi-output \"auto\" which should also have no effect\\n    clf4 = TreeClassifier(class_weight=\\'balanced\\', random_state=0)\\n    clf4.fit(iris.data, iris_multi)\\n    assert_almost_equal(clf3.feature_importances_, clf4.feature_importances_)\\n\\n    # Inflate importance of class 1, check against user-defined weights\\n    sample_weight = np.ones(iris.target.shape)\\n    sample_weight[iris.target == 1] *= 100\\n    class_weight = {0: 1., 1: 100., 2: 1.}\\n    clf1 = TreeClassifier(random_state=0)\\n    clf1.fit(iris.data, iris.target, sample_weight)\\n    clf2 = TreeClassifier(class_weight=class_weight, random_state=0)\\n    clf2.fit(iris.data, iris.target)\\n    assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)\\n\\n    # Check that sample_weight and class_weight are multiplicative\\n    clf1 = TreeClassifier(random_state=0)\\n    clf1.fit(iris.data, iris.target, sample_weight ** 2)\\n    clf2 = TreeClassifier(class_weight=class_weight, random_state=0)\\n    clf2.fit(iris.data, iris.target, sample_weight)\\n    assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)\\n\\n\\ndef test_class_weights():\\n    for name in CLF_TREES:\\n        yield check_class_weights, name\\n\\n\\ndef check_class_weight_errors(name):\\n    # Test if class_weight raises errors and warnings when expected.\\n    TreeClassifier = CLF_TREES[name]\\n    _y = np.vstack((y, np.array(y) * 2)).T\\n\\n    # Invalid preset string\\n    clf = TreeClassifier(class_weight=\\'the larch\\', random_state=0)\\n    assert_raises(ValueError, clf.fit, X, y)\\n    assert_raises(ValueError, clf.fit, X, _y)\\n\\n    # Not a list or preset for multi-output\\n    clf = TreeClassifier(class_weight=1, random_state=0)\\n    assert_raises(ValueError, clf.fit, X, _y)\\n\\n    # Incorrect length list for multi-output\\n    clf = TreeClassifier(class_weight=[{-1: 0.5, 1: 1.}], random_state=0)\\n    assert_raises(ValueError, clf.fit, X, _y)\\n\\n\\ndef test_class_weight_errors():\\n    for name in CLF_TREES:\\n        yield check_class_weight_errors, name\\n\\n\\ndef test_max_leaf_nodes():\\n    # Test greedy trees with max_depth + 1 leafs.\\n    from sklearn.tree._tree import TREE_LEAF\\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\\n    k = 4\\n    for name, TreeEstimator in ALL_TREES.items():\\n        est = TreeEstimator(max_depth=None, max_leaf_nodes=k + 1).fit(X, y)\\n        tree = est.tree_\\n        assert_equal((tree.children_left == TREE_LEAF).sum(), k + 1)\\n\\n        # max_leaf_nodes in (0, 1) should raise ValueError\\n        est = TreeEstimator(max_depth=None, max_leaf_nodes=0)\\n        assert_raises(ValueError, est.fit, X, y)\\n        est = TreeEstimator(max_depth=None, max_leaf_nodes=1)\\n        assert_raises(ValueError, est.fit, X, y)\\n        est = TreeEstimator(max_depth=None, max_leaf_nodes=0.1)\\n        assert_raises(ValueError, est.fit, X, y)\\n\\n\\ndef test_max_leaf_nodes_max_depth():\\n    # Test preceedence of max_leaf_nodes over max_depth.\\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\\n    k = 4\\n    for name, TreeEstimator in ALL_TREES.items():\\n        est = TreeEstimator(max_depth=1, max_leaf_nodes=k).fit(X, y)\\n        tree = est.tree_\\n        assert_greater(tree.max_depth, 1)\\n\\n\\ndef test_arrays_persist():\\n    # Ensure property arrays\\' memory stays alive when tree disappears\\n    # non-regression for #2726\\n    for attr in [\\'n_classes\\', \\'value\\', \\'children_left\\', \\'children_right\\',\\n                 \\'threshold\\', \\'impurity\\', \\'feature\\', \\'n_node_samples\\']:\\n        value = getattr(DecisionTreeClassifier().fit([[0]], [0]).tree_, attr)\\n        # if pointing to freed memory, contents may be arbitrary\\n        assert_true(-2 <= value.flat[0] < 2,\\n                    \\'Array points to arbitrary memory\\')\\n\\n\\ndef test_only_constant_features():\\n    random_state = check_random_state(0)\\n    X = np.zeros((10, 20))\\n    y = random_state.randint(0, 2, (10, ))\\n    for name, TreeEstimator in ALL_TREES.items():\\n        est = TreeEstimator(random_state=0)\\n        est.fit(X, y)\\n        assert_equal(est.tree_.max_depth, 0)\\n\\n\\ndef test_with_only_one_non_constant_features():\\n    X = np.hstack([np.array([[1.], [1.], [0.], [0.]]),\\n                   np.zeros((4, 1000))])\\n\\n    y = np.array([0., 1., 0., 1.0])\\n    for name, TreeEstimator in CLF_TREES.items():\\n        est = TreeEstimator(random_state=0, max_features=1)\\n        est.fit(X, y)\\n        assert_equal(est.tree_.max_depth, 1)\\n        assert_array_equal(est.predict_proba(X), 0.5 * np.ones((4, 2)))\\n\\n    for name, TreeEstimator in REG_TREES.items():\\n        est = TreeEstimator(random_state=0, max_features=1)\\n        est.fit(X, y)\\n        assert_equal(est.tree_.max_depth, 1)\\n        assert_array_equal(est.predict(X), 0.5 * np.ones((4, )))\\n\\n\\ndef test_big_input():\\n    # Test if the warning for too large inputs is appropriate.\\n    X = np.repeat(10 ** 40., 4).astype(np.float64).reshape(-1, 1)\\n    clf = DecisionTreeClassifier()\\n    try:\\n        clf.fit(X, [0, 1, 0, 1])\\n    except ValueError as e:\\n        assert_in(\"float32\", str(e))\\n\\n\\ndef test_realloc():\\n    from sklearn.tree._utils import _realloc_test\\n    assert_raises(MemoryError, _realloc_test)\\n\\n\\ndef test_huge_allocations():\\n    n_bits = int(platform.architecture()[0].rstrip(\\'bit\\'))\\n\\n    X = np.random.randn(10, 2)\\n    y = np.random.randint(0, 2, 10)\\n\\n    # Sanity check: we cannot request more memory than the size of the address\\n    # space. Currently raises OverflowError.\\n    huge = 2 ** (n_bits + 1)\\n    clf = DecisionTreeClassifier(splitter=\\'best\\', max_leaf_nodes=huge)\\n    assert_raises(Exception, clf.fit, X, y)\\n\\n    # Non-regression test: MemoryError used to be dropped by Cython\\n    # because of missing \"except *\".\\n    huge = 2 ** (n_bits - 1) - 1\\n    clf = DecisionTreeClassifier(splitter=\\'best\\', max_leaf_nodes=huge)\\n    assert_raises(MemoryError, clf.fit, X, y)\\n\\n\\ndef check_sparse_input(tree, dataset, max_depth=None):\\n    TreeEstimator = ALL_TREES[tree]\\n    X = DATASETS[dataset][\"X\"]\\n    X_sparse = DATASETS[dataset][\"X_sparse\"]\\n    y = DATASETS[dataset][\"y\"]\\n\\n    # Gain testing time\\n    if dataset in [\"digits\", \"boston\"]:\\n        n_samples = X.shape[0] // 5\\n        X = X[:n_samples]\\n        X_sparse = X_sparse[:n_samples]\\n        y = y[:n_samples]\\n\\n    for sparse_format in (csr_matrix, csc_matrix, coo_matrix):\\n        X_sparse = sparse_format(X_sparse)\\n\\n        # Check the default (depth first search)\\n        d = TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y)\\n        s = TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)\\n\\n        assert_tree_equal(d.tree_, s.tree_,\\n                          \"{0} with dense and sparse format gave different \"\\n                          \"trees\".format(tree))\\n\\n        y_pred = d.predict(X)\\n        if tree in CLF_TREES:\\n            y_proba = d.predict_proba(X)\\n            y_log_proba = d.predict_log_proba(X)\\n\\n        for sparse_matrix in (csr_matrix, csc_matrix, coo_matrix):\\n            X_sparse_test = sparse_matrix(X_sparse, dtype=np.float32)\\n\\n            assert_array_almost_equal(s.predict(X_sparse_test), y_pred)\\n\\n            if tree in CLF_TREES:\\n                assert_array_almost_equal(s.predict_proba(X_sparse_test),\\n                                          y_proba)\\n                assert_array_almost_equal(s.predict_log_proba(X_sparse_test),\\n                                          y_log_proba)\\n\\n\\ndef test_sparse_input():\\n    for tree, dataset in product(SPARSE_TREES,\\n                                 (\"clf_small\", \"toy\", \"digits\", \"multilabel\",\\n                                  \"sparse-pos\", \"sparse-neg\", \"sparse-mix\",\\n                                  \"zeros\")):\\n        max_depth = 3 if dataset == \"digits\" else None\\n        yield (check_sparse_input, tree, dataset, max_depth)\\n\\n    # Due to numerical instability of MSE and too strict test, we limit the\\n    # maximal depth\\n    for tree, dataset in product(REG_TREES, [\"boston\", \"reg_small\"]):\\n        if tree in SPARSE_TREES:\\n            yield (check_sparse_input, tree, dataset, 2)\\n\\n\\ndef check_sparse_parameters(tree, dataset):\\n    TreeEstimator = ALL_TREES[tree]\\n    X = DATASETS[dataset][\"X\"]\\n    X_sparse = DATASETS[dataset][\"X_sparse\"]\\n    y = DATASETS[dataset][\"y\"]\\n\\n    # Check max_features\\n    d = TreeEstimator(random_state=0, max_features=1, max_depth=2).fit(X, y)\\n    s = TreeEstimator(random_state=0, max_features=1,\\n                      max_depth=2).fit(X_sparse, y)\\n    assert_tree_equal(d.tree_, s.tree_,\\n                      \"{0} with dense and sparse format gave different \"\\n                      \"trees\".format(tree))\\n    assert_array_almost_equal(s.predict(X), d.predict(X))\\n\\n    # Check min_samples_split\\n    d = TreeEstimator(random_state=0, max_features=1,\\n                      min_samples_split=10).fit(X, y)\\n    s = TreeEstimator(random_state=0, max_features=1,\\n                      min_samples_split=10).fit(X_sparse, y)\\n    assert_tree_equal(d.tree_, s.tree_,\\n                      \"{0} with dense and sparse format gave different \"\\n                      \"trees\".format(tree))\\n    assert_array_almost_equal(s.predict(X), d.predict(X))\\n\\n    # Check min_samples_leaf\\n    d = TreeEstimator(random_state=0,\\n                      min_samples_leaf=X_sparse.shape[0] // 2).fit(X, y)\\n    s = TreeEstimator(random_state=0,\\n                      min_samples_leaf=X_sparse.shape[0] // 2).fit(X_sparse, y)\\n    assert_tree_equal(d.tree_, s.tree_,\\n                      \"{0} with dense and sparse format gave different \"\\n                      \"trees\".format(tree))\\n    assert_array_almost_equal(s.predict(X), d.predict(X))\\n\\n    # Check best-first search\\n    d = TreeEstimator(random_state=0, max_leaf_nodes=3).fit(X, y)\\n    s = TreeEstimator(random_state=0, max_leaf_nodes=3).fit(X_sparse, y)\\n    assert_tree_equal(d.tree_, s.tree_,\\n                      \"{0} with dense and sparse format gave different \"\\n                      \"trees\".format(tree))\\n    assert_array_almost_equal(s.predict(X), d.predict(X))\\n\\n\\ndef test_sparse_parameters():\\n    for tree, dataset in product(SPARSE_TREES,\\n                                 [\"sparse-pos\", \"sparse-neg\", \"sparse-mix\",\\n                                  \"zeros\"]):\\n        yield (check_sparse_parameters, tree, dataset)\\n\\n\\ndef check_sparse_criterion(tree, dataset):\\n    TreeEstimator = ALL_TREES[tree]\\n    X = DATASETS[dataset][\"X\"]\\n    X_sparse = DATASETS[dataset][\"X_sparse\"]\\n    y = DATASETS[dataset][\"y\"]\\n\\n    # Check various criterion\\n    CRITERIONS = REG_CRITERIONS if tree in REG_TREES else CLF_CRITERIONS\\n    for criterion in CRITERIONS:\\n        d = TreeEstimator(random_state=0, max_depth=3,\\n                          criterion=criterion).fit(X, y)\\n        s = TreeEstimator(random_state=0, max_depth=3,\\n                          criterion=criterion).fit(X_sparse, y)\\n\\n        assert_tree_equal(d.tree_, s.tree_,\\n                          \"{0} with dense and sparse format gave different \"\\n                          \"trees\".format(tree))\\n        assert_array_almost_equal(s.predict(X), d.predict(X))\\n\\n\\ndef test_sparse_criterion():\\n    for tree, dataset in product(SPARSE_TREES,\\n                                 [\"sparse-pos\", \"sparse-neg\", \"sparse-mix\",\\n                                  \"zeros\"]):\\n        yield (check_sparse_criterion, tree, dataset)\\n\\n\\ndef check_explicit_sparse_zeros(tree, max_depth=3,\\n                                n_features=10):\\n    TreeEstimator = ALL_TREES[tree]\\n\\n    # n_samples set n_feature to ease construction of a simultaneous\\n    # construction of a csr and csc matrix\\n    n_samples = n_features\\n    samples = np.arange(n_samples)\\n\\n    # Generate X, y\\n    random_state = check_random_state(0)\\n    indices = []\\n    data = []\\n    offset = 0\\n    indptr = [offset]\\n    for i in range(n_features):\\n        n_nonzero_i = random_state.binomial(n_samples, 0.5)\\n        indices_i = random_state.permutation(samples)[:n_nonzero_i]\\n        indices.append(indices_i)\\n        data_i = random_state.binomial(3, 0.5, size=(n_nonzero_i, )) - 1\\n        data.append(data_i)\\n        offset += n_nonzero_i\\n        indptr.append(offset)\\n\\n    indices = np.concatenate(indices)\\n    data = np.array(np.concatenate(data), dtype=np.float32)\\n    X_sparse = csc_matrix((data, indices, indptr),\\n                          shape=(n_samples, n_features))\\n    X = X_sparse.toarray()\\n    X_sparse_test = csr_matrix((data, indices, indptr),\\n                               shape=(n_samples, n_features))\\n    X_test = X_sparse_test.toarray()\\n    y = random_state.randint(0, 3, size=(n_samples, ))\\n\\n    # Ensure that X_sparse_test owns its data, indices and indptr array\\n    X_sparse_test = X_sparse_test.copy()\\n\\n    # Ensure that we have explicit zeros\\n    assert_greater((X_sparse.data == 0.).sum(), 0)\\n    assert_greater((X_sparse_test.data == 0.).sum(), 0)\\n\\n    # Perform the comparison\\n    d = TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y)\\n    s = TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)\\n\\n    assert_tree_equal(d.tree_, s.tree_,\\n                      \"{0} with dense and sparse format gave different \"\\n                      \"trees\".format(tree))\\n\\n    Xs = (X_test, X_sparse_test)\\n    for X1, X2 in product(Xs, Xs):\\n        assert_array_almost_equal(s.tree_.apply(X1), d.tree_.apply(X2))\\n        assert_array_almost_equal(s.apply(X1), d.apply(X2))\\n        assert_array_almost_equal(s.apply(X1), s.tree_.apply(X1))\\n        assert_array_almost_equal(s.predict(X1), d.predict(X2))\\n\\n        if tree in CLF_TREES:\\n            assert_array_almost_equal(s.predict_proba(X1),\\n                                      d.predict_proba(X2))\\n\\n\\ndef test_explicit_sparse_zeros():\\n    for tree in SPARSE_TREES:\\n        yield (check_explicit_sparse_zeros, tree)\\n\\n\\n@ignore_warnings\\ndef check_raise_error_on_1d_input(name):\\n    TreeEstimator = ALL_TREES[name]\\n\\n    X = iris.data[:, 0].ravel()\\n    X_2d = iris.data[:, 0].reshape((-1, 1))\\n    y = iris.target\\n\\n    assert_raises(ValueError, TreeEstimator(random_state=0).fit, X, y)\\n\\n    est = TreeEstimator(random_state=0)\\n    est.fit(X_2d, y)\\n    assert_raises(ValueError, est.predict, [X])\\n\\n\\n@ignore_warnings\\ndef test_1d_input():\\n    for name in ALL_TREES:\\n        yield check_raise_error_on_1d_input, name\\n\\n\\ndef _check_min_weight_leaf_split_level(TreeEstimator, X, y, sample_weight):\\n    # Private function to keep pretty printing in nose yielded tests\\n    est = TreeEstimator(random_state=0)\\n    est.fit(X, y, sample_weight=sample_weight)\\n    assert_equal(est.tree_.max_depth, 1)\\n\\n    est = TreeEstimator(random_state=0, min_weight_fraction_leaf=0.4)\\n    est.fit(X, y, sample_weight=sample_weight)\\n    assert_equal(est.tree_.max_depth, 0)\\n\\n\\ndef check_min_weight_leaf_split_level(name):\\n    TreeEstimator = ALL_TREES[name]\\n\\n    X = np.array([[0], [0], [0], [0], [1]])\\n    y = [0, 0, 0, 0, 1]\\n    sample_weight = [0.2, 0.2, 0.2, 0.2, 0.2]\\n    _check_min_weight_leaf_split_level(TreeEstimator, X, y, sample_weight)\\n\\n    if TreeEstimator().splitter in SPARSE_SPLITTERS:\\n        _check_min_weight_leaf_split_level(TreeEstimator, csc_matrix(X), y,\\n                                           sample_weight)\\n\\n\\ndef test_min_weight_leaf_split_level():\\n    for name in ALL_TREES:\\n        yield check_min_weight_leaf_split_level, name\\n\\n\\ndef check_public_apply(name):\\n    X_small32 = X_small.astype(tree._tree.DTYPE)\\n\\n    est = ALL_TREES[name]()\\n    est.fit(X_small, y_small)\\n    assert_array_equal(est.apply(X_small),\\n                       est.tree_.apply(X_small32))\\n\\n\\ndef check_public_apply_sparse(name):\\n    X_small32 = csr_matrix(X_small.astype(tree._tree.DTYPE))\\n\\n    est = ALL_TREES[name]()\\n    est.fit(X_small, y_small)\\n    assert_array_equal(est.apply(X_small),\\n                       est.tree_.apply(X_small32))\\n\\n\\ndef test_public_apply():\\n    for name in ALL_TREES:\\n        yield (check_public_apply, name)\\n\\n    for name in SPARSE_TREES:\\n        yield (check_public_apply_sparse, name)\\n', '# -*- coding: utf-8 -*-\\n# Copyright (c) 2015, Vispy Development Team.\\n# Distributed under the (new) BSD License. See LICENSE.txt for more info.\\n\\nfrom __future__ import division\\n\\nimport numpy as np\\n\\nfrom .line import LineVisual\\nfrom ..color import ColorArray\\nfrom ..color.colormap import _normalize, get_colormap\\nfrom ..geometry.isocurve import isocurve\\nfrom ..testing import has_matplotlib\\n\\n# checking for matplotlib\\n_HAS_MPL = has_matplotlib()\\nif _HAS_MPL:\\n    from matplotlib import _cntr as cntr\\n\\n\\nclass IsocurveVisual(LineVisual):\\n    \"\"\"Displays an isocurve of a 2D scalar array.\\n\\n    Parameters\\n    ----------\\n    data : ndarray | None\\n        2D scalar array.\\n    levels : ndarray, shape (Nlev,) | None\\n        The levels at which the isocurve is constructed from \"*data*\".\\n    color_lev : Color, colormap name, tuple, list or array\\n        The color to use when drawing the line. If a list is given, it\\n        must be of shape (Nlev), if an array is given, it must be of\\n        shape (Nlev, ...). and provide one color per level (rgba, colorname).\\n    clim : tuple\\n        (min, max) limits to apply when mapping level values through a\\n        colormap.\\n    **kwargs : dict\\n        Keyword arguments to pass to `LineVisual`.\\n\\n    Notes\\n    -----\\n    \"\"\"\\n    def __init__(self, data=None, levels=None, color_lev=None, clim=None,\\n                 **kwargs):\\n        self._data = None\\n        self._levels = levels\\n        self._color_lev = color_lev\\n        self._clim = clim\\n        self._need_color_update = True\\n        self._need_level_update = True\\n        self._need_recompute = True\\n        self._X = None\\n        self._Y = None\\n        self._iso = None\\n        self._level_min = None\\n        self._data_is_uniform = False\\n        self._lc = None\\n        self._cl = None\\n        self._li = None\\n        self._connect = None\\n        self._verts = None\\n        kwargs[\\'method\\'] = \\'gl\\'\\n        kwargs[\\'antialias\\'] = False\\n        LineVisual.__init__(self, **kwargs)\\n        if data is not None:\\n            self.set_data(data)\\n\\n    @property\\n    def levels(self):\\n        \"\"\" The threshold at which the isocurve is constructed from the\\n        2D data.\\n        \"\"\"\\n        return self._levels\\n\\n    @levels.setter\\n    def levels(self, levels):\\n        self._levels = levels\\n        self._need_level_update = True\\n        self._need_recompute = True\\n        self.update()\\n\\n    @property\\n    def color(self):\\n        return self._color_lev\\n\\n    @color.setter\\n    def color(self, color):\\n        self._color_lev = color\\n        self._need_level_update = True\\n        self._need_color_update = True\\n        self.update()\\n\\n    def set_data(self, data):\\n        \"\"\" Set the scalar array data\\n\\n        Parameters\\n        ----------\\n        data : ndarray\\n            A 2D array of scalar values. The isocurve is constructed to show\\n            all locations in the scalar field equal to ``self.levels``.\\n        \"\"\"\\n        self._data = data\\n\\n        # if using matplotlib isoline algorithm we have to check for meshgrid\\n        # and we can setup the tracer object here\\n        if _HAS_MPL:\\n            if self._X is None or self._X.T.shape != data.shape:\\n                self._X, self._Y = np.meshgrid(np.arange(data.shape[0]),\\n                                               np.arange(data.shape[1]))\\n            self._iso = cntr.Cntr(self._X, self._Y, self._data.astype(float))\\n\\n        if self._clim is None:\\n            self._clim = (data.min(), data.max())\\n\\n        # sanity check,\\n        # should we raise an error here, since no isolines can be drawn?\\n        # for now, _prepare_draw returns False if no isoline can be drawn\\n        if self._data.min() != self._data.max():\\n            self._data_is_uniform = False\\n        else:\\n            self._data_is_uniform = True\\n\\n        self._need_recompute = True\\n        self.update()\\n\\n    def _get_verts_and_connect(self, paths):\\n        \"\"\" retrieve vertices and connects from given paths-list\\n        \"\"\"\\n        verts = np.vstack(paths)\\n        gaps = np.add.accumulate(np.array([len(x) for x in paths])) - 1\\n        connect = np.ones(gaps[-1], dtype=bool)\\n        connect[gaps[:-1]] = False\\n        return verts, connect\\n\\n    def _compute_iso_line(self):\\n        \"\"\" compute LineVisual vertices, connects and color-index\\n        \"\"\"\\n        level_index = []\\n        connects = []\\n        verts = []\\n\\n        # calculate which level are within data range\\n        # this works for now and the existing examples, but should be tested\\n        # thoroughly also with the data-sanity check in set_data-function\\n        choice = np.nonzero((self.levels > self._data.min()) &\\n                            (self._levels < self._data.max()))\\n        levels_to_calc = np.array(self.levels)[choice]\\n\\n        # save minimum level index\\n        self._level_min = choice[0][0]\\n\\n        for level in levels_to_calc:\\n            # if we use matplotlib isoline algorithm we need to add half a\\n            # pixel in both (x,y) dimensions because isolines are aligned to\\n            # pixel centers\\n            if _HAS_MPL:\\n                nlist = self._iso.trace(level, level, 0)\\n                paths = nlist[:len(nlist)//2]\\n                v, c = self._get_verts_and_connect(paths)\\n                v += np.array([0.5, 0.5])\\n            else:\\n                paths = isocurve(self._data.astype(float).T, level,\\n                                 extend_to_edge=True, connected=True)\\n                v, c = self._get_verts_and_connect(paths)\\n\\n            level_index.append(v.shape[0])\\n            connects.append(np.hstack((c, [False])))\\n            verts.append(v)\\n\\n        self._li = np.hstack(level_index)\\n        self._connect = np.hstack(connects)\\n        self._verts = np.vstack(verts)\\n\\n    def _compute_iso_color(self):\\n        \"\"\" compute LineVisual color from level index and corresponding color\\n        \"\"\"\\n        level_color = []\\n        colors = self._lc\\n        for i, index in enumerate(self._li):\\n            level_color.append(np.zeros((index, 4)) +\\n                               colors[i+self._level_min])\\n        self._cl = np.vstack(level_color)\\n\\n    def _levels_to_colors(self):\\n        # computes ColorArrays for given levels\\n        # try _color_lev as colormap, except as everything else\\n        try:\\n            f_color_levs = get_colormap(self._color_lev)\\n        except:\\n            colors = ColorArray(self._color_lev).rgba\\n        else:\\n            lev = _normalize(self._levels, self._clim[0], self._clim[1])\\n            # map function expects (Nlev,1)!\\n            colors = f_color_levs.map(lev[:, np.newaxis])\\n\\n        # broadcast to (nlev, 4) array\\n        if len(colors) == 1:\\n            colors = colors * np.ones((len(self._levels), 1))\\n\\n        # detect color_lev/levels mismatch and raise error\\n        if (len(colors) != len(self._levels)):\\n            raise TypeError(\"Color/level mismatch. Color must be of shape \"\\n                            \"(Nlev, ...) and provide one color per level\")\\n\\n        self._lc = colors\\n\\n    def _prepare_draw(self, view):\\n        if (self._data is None or self._levels is None or\\n                self._color_lev is None or self._data_is_uniform):\\n            return False\\n\\n        if self._need_level_update:\\n            self._levels_to_colors()\\n            self._need_level_update = False\\n\\n        if self._need_recompute:\\n            self._compute_iso_line()\\n            self._compute_iso_color()\\n            LineVisual.set_data(self, pos=self._verts, connect=self._connect,\\n                                color=self._cl)\\n            self._need_recompute = False\\n\\n        if self._need_color_update:\\n            self._compute_iso_color()\\n            LineVisual.set_data(self, color=self._cl)\\n            self._need_color_update = False\\n\\n        return LineVisual._prepare_draw(self, view)\\n', 'from matplotlib.patches import Circle\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pytest\\n\\nfrom pysisyphus.calculators.AnaPot import AnaPot\\nfrom pysisyphus.dynamics.velocity_verlet import md\\n\\n\\ndef test_velocity_verlet():\\n    geom = AnaPot.get_geom((0.52, 1.80, 0))\\n    x0 = geom.coords.copy()\\n    v0 = .1 * np.random.rand(*geom.coords.shape)\\n    t = 3\\n    dts = (.005, .01, .02, .04, .08)\\n    all_xs = list()\\n    for dt in dts:\\n        geom.coords = x0.copy()\\n        md_kwargs = {\\n            \"v0\": v0.copy(),\\n            \"t\": t,\\n            \"dt\": dt,\\n        }\\n        md_result = md(geom, **md_kwargs)\\n        all_xs.append(md_result.coords)\\n    calc = geom.calculator\\n    calc.plot()\\n    ax = calc.ax\\n    for dt, xs in zip(dts, all_xs):\\n        ax.plot(*xs.T[:2], \"o-\", label=f\"dt={dt:.3f}\")\\n        # ax.plot(*xs.T[:2], \"-\", label=f\"dt={dt:.3f}\")\\n    ax.legend()\\n    plt.show()\\n\\n\\ndef ase_md_playground():\\n    geom = AnaPot.get_geom((0.52, 1.80, 0), atoms=(\"H\", ))\\n    atoms = geom.as_ase_atoms()\\n    # ase_calc = FakeASE(geom.calculator)\\n    # from ase.optimize import BFGS\\n    # dyn = BFGS(atoms)\\n    # dyn.run(fmax=0.05)\\n\\n    import ase\\n    from ase import units\\n    from ase.io.trajectory import Trajectory\\n    from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\\n    from ase.md.verlet import VelocityVerlet\\n\\n    MaxwellBoltzmannDistribution(atoms, 300 * units.kB)\\n    momenta = atoms.get_momenta()\\n    momenta[0, 2] = 0.\\n    # Zero 3rd dimension\\n    atoms.set_momenta(momenta)\\n\\n    dyn = VelocityVerlet(atoms, .005 * units.fs)  # 5 fs time step.\\n\\n\\n    def printenergy(a):\\n        \"\"\"Function to print the potential, kinetic and total energy\"\"\"\\n        epot = a.get_potential_energy() / len(a)\\n        ekin = a.get_kinetic_energy() / len(a)\\n        print(\\'Energy per atom: Epot = %.3feV  Ekin = %.3feV (T=%3.0fK)  \\'\\n              \\'Etot = %.3feV\\' % (epot, ekin, ekin / (1.5 * units.kB), epot + ekin))\\n\\n    # Now run the dynamics\\n    printenergy(atoms)\\n    traj_fn = \\'asemd.traj\\'\\n    traj = Trajectory(traj_fn, \\'w\\', atoms)\\n    dyn.attach(traj.write, interval=5)\\n    # dyn.attach(bumms().bimms, interval=1)\\n\\n    dyn.run(10000)\\n    printenergy(atoms)\\n    traj.close()\\n\\n    traj = ase.io.read(traj_fn+\"@:\")#, \"r\")\\n    pos = [a.get_positions() for a in traj]\\n    from pysisyphus.constants import BOHR2ANG\\n    pos = np.array(pos) / BOHR2ANG\\n\\n    calc = geom.calculator\\n    calc.plot()\\n\\n    ax = calc.ax\\n    ax.plot(*pos[:,0,:2].T)\\n\\n    plt.show()\\n\\n\\nif __name__ == \"__main__\":\\n    ase_md_playground()\\n', '\"\"\"\\n\\neta^3 polynomials planner\\n\\nauthor: Joe Dinius, Ph.D (https://jwdinius.github.io)\\n        Atsushi Sakai (@Atsushi_twi)\\n\\nRef:\\n- [eta^3-Splines for the Smooth Path Generation of Wheeled Mobile Robots]\\n(https://ieeexplore.ieee.org/document/4339545/)\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom scipy.integrate import quad\\n\\n# NOTE: *_pose is a 3-array:\\n# 0 - x coord, 1 - y coord, 2 - orientation angle \\\\theta\\n\\nshow_animation = True\\n\\n\\nclass Eta3Path(object):\\n    \"\"\"\\n    Eta3Path\\n\\n    input\\n        segments: a list of `Eta3PathSegment` instances\\n        defining a continuous path\\n    \"\"\"\\n\\n    def __init__(self, segments):\\n        # ensure input has the correct form\\n        assert(isinstance(segments, list) and isinstance(\\n            segments[0], Eta3PathSegment))\\n        # ensure that each segment begins from the previous segment\\'s end (continuity)\\n        for r, s in zip(segments[:-1], segments[1:]):\\n            assert(np.array_equal(r.end_pose, s.start_pose))\\n        self.segments = segments\\n\\n    def calc_path_point(self, u):\\n        \"\"\"\\n        Eta3Path::calc_path_point\\n\\n        input\\n            normalized interpolation point along path object, 0 <= u <= len(self.segments)\\n        returns\\n            2d (x,y) position vector\\n        \"\"\"\\n\\n        assert(0 <= u <= len(self.segments))\\n        if np.isclose(u, len(self.segments)):\\n            segment_idx = len(self.segments) - 1\\n            u = 1.\\n        else:\\n            segment_idx = int(np.floor(u))\\n            u -= segment_idx\\n        return self.segments[segment_idx].calc_point(u)\\n\\n\\nclass Eta3PathSegment(object):\\n    \"\"\"\\n    Eta3PathSegment - constructs an eta^3 path segment based on desired\\n    shaping, eta, and curvature vector, kappa. If either, or both,\\n    of eta and kappa are not set during initialization,\\n    they will default to zeros.\\n\\n    input\\n        start_pose - starting pose array  (x, y, \\\\theta)\\n        end_pose - ending pose array (x, y, \\\\theta)\\n        eta - shaping parameters, default=None\\n        kappa - curvature parameters, default=None\\n    \"\"\"\\n\\n    def __init__(self, start_pose, end_pose, eta=None, kappa=None):\\n        # make sure inputs are of the correct size\\n        assert(len(start_pose) == 3 and len(start_pose) == len(end_pose))\\n        self.start_pose = start_pose\\n        self.end_pose = end_pose\\n        # if no eta is passed, initialize it to array of zeros\\n        if not eta:\\n            eta = np.zeros((6,))\\n        else:\\n            # make sure that eta has correct size\\n            assert(len(eta) == 6)\\n        # if no kappa is passed, initialize to array of zeros\\n        if not kappa:\\n            kappa = np.zeros((4,))\\n        else:\\n            assert(len(kappa) == 4)\\n        # set up angle cosines and sines for simpler computations below\\n        ca = np.cos(start_pose[2])\\n        sa = np.sin(start_pose[2])\\n        cb = np.cos(end_pose[2])\\n        sb = np.sin(end_pose[2])\\n        # 2 dimensions (x,y) x 8 coefficients per dimension\\n        self.coeffs = np.empty((2, 8))\\n        # constant terms (u^0)\\n        self.coeffs[0, 0] = start_pose[0]\\n        self.coeffs[1, 0] = start_pose[1]\\n        # linear (u^1)\\n        self.coeffs[0, 1] = eta[0] * ca\\n        self.coeffs[1, 1] = eta[0] * sa\\n        # quadratic (u^2)\\n        self.coeffs[0, 2] = 1. / 2 * eta[2] * \\\\\\n            ca - 1. / 2 * eta[0]**2 * kappa[0] * sa\\n        self.coeffs[1, 2] = 1. / 2 * eta[2] * \\\\\\n            sa + 1. / 2 * eta[0]**2 * kappa[0] * ca\\n        # cubic (u^3)\\n        self.coeffs[0, 3] = 1. / 6 * eta[4] * ca - 1. / 6 * \\\\\\n            (eta[0]**3 * kappa[1] + 3. * eta[0] * eta[2] * kappa[0]) * sa\\n        self.coeffs[1, 3] = 1. / 6 * eta[4] * sa + 1. / 6 * \\\\\\n            (eta[0]**3 * kappa[1] + 3. * eta[0] * eta[2] * kappa[0]) * ca\\n        # quartic (u^4)\\n        tmp1 = 35. * (end_pose[0] - start_pose[0])\\n        tmp2 = (20. * eta[0] + 5 * eta[2] + 2. / 3 * eta[4]) * ca\\n        tmp3 = (5. * eta[0] ** 2 * kappa[0] + 2. / 3 * eta[0] ** 3 * kappa[1]\\n                + 2. * eta[0] * eta[2] * kappa[0]) * sa\\n        tmp4 = (15. * eta[1] - 5. / 2 * eta[3] + 1. / 6 * eta[5]) * cb\\n        tmp5 = (5. / 2 * eta[1] ** 2 * kappa[2] - 1. / 6 * eta[1] ** 3 *\\n                kappa[3] - 1. / 2 * eta[1] * eta[3] * kappa[2]) * sb\\n        self.coeffs[0, 4] = tmp1 - tmp2 + tmp3 - tmp4 - tmp5\\n        tmp1 = 35. * (end_pose[1] - start_pose[1])\\n        tmp2 = (20. * eta[0] + 5. * eta[2] + 2. / 3 * eta[4]) * sa\\n        tmp3 = (5. * eta[0] ** 2 * kappa[0] + 2. / 3 * eta[0] ** 3 * kappa[1]\\n                + 2. * eta[0] * eta[2] * kappa[0]) * ca\\n        tmp4 = (15. * eta[1] - 5. / 2 * eta[3] + 1. / 6 * eta[5]) * sb\\n        tmp5 = (5. / 2 * eta[1] ** 2 * kappa[2] - 1. / 6 * eta[1] ** 3 *\\n                kappa[3] - 1. / 2 * eta[1] * eta[3] * kappa[2]) * cb\\n        self.coeffs[1, 4] = tmp1 - tmp2 - tmp3 - tmp4 + tmp5\\n        # quintic (u^5)\\n        tmp1 = -84. * (end_pose[0] - start_pose[0])\\n        tmp2 = (45. * eta[0] + 10. * eta[2] + eta[4]) * ca\\n        tmp3 = (10. * eta[0] ** 2 * kappa[0] + eta[0] ** 3 * kappa[1] + 3. *\\n                eta[0] * eta[2] * kappa[0]) * sa\\n        tmp4 = (39. * eta[1] - 7. * eta[3] + 1. / 2 * eta[5]) * cb\\n        tmp5 = + (7. * eta[1] ** 2 * kappa[2] - 1. / 2 * eta[1] ** 3 * kappa[3]\\n                  - 3. / 2 * eta[1] * eta[3] * kappa[2]) * sb\\n        self.coeffs[0, 5] = tmp1 + tmp2 - tmp3 + tmp4 + tmp5\\n        tmp1 = -84. * (end_pose[1] - start_pose[1])\\n        tmp2 = (45. * eta[0] + 10. * eta[2] + eta[4]) * sa\\n        tmp3 = (10. * eta[0] ** 2 * kappa[0] + eta[0] ** 3 * kappa[1] + 3. *\\n                eta[0] * eta[2] * kappa[0]) * ca\\n        tmp4 = (39. * eta[1] - 7. * eta[3] + 1. / 2 * eta[5]) * sb\\n        tmp5 = - (7. * eta[1] ** 2 * kappa[2] - 1. / 2 * eta[1] ** 3 * kappa[3]\\n                  - 3. / 2 * eta[1] * eta[3] * kappa[2]) * cb\\n        self.coeffs[1, 5] = tmp1 + tmp2 + tmp3 + tmp4 + tmp5\\n        # sextic (u^6)\\n        tmp1 = 70. * (end_pose[0] - start_pose[0])\\n        tmp2 = (36. * eta[0] + 15. / 2 * eta[2] + 2. / 3 * eta[4]) * ca\\n        tmp3 = + (15. / 2 * eta[0] ** 2 * kappa[0] + 2. / 3 * eta[0] ** 3 *\\n                  kappa[1] + 2. * eta[0] * eta[2] * kappa[0]) * sa\\n        tmp4 = (34. * eta[1] - 13. / 2 * eta[3] + 1. / 2 * eta[5]) * cb\\n        tmp5 = - (13. / 2 * eta[1] ** 2 * kappa[2] - 1. / 2 * eta[1] ** 3 *\\n                  kappa[3] - 3. / 2 * eta[1] * eta[3] * kappa[2]) * sb\\n        self.coeffs[0, 6] = tmp1 - tmp2 + tmp3 - tmp4 + tmp5\\n        tmp1 = 70. * (end_pose[1] - start_pose[1])\\n        tmp2 = - (36. * eta[0] + 15. / 2 * eta[2] + 2. / 3 * eta[4]) * sa\\n        tmp3 = - (15. / 2 * eta[0] ** 2 * kappa[0] + 2. / 3 * eta[0] ** 3 *\\n                  kappa[1] + 2. * eta[0] * eta[2] * kappa[0]) * ca\\n        tmp4 = - (34. * eta[1] - 13. / 2 * eta[3] + 1. / 2 * eta[5]) * sb\\n        tmp5 = + (13. / 2 * eta[1] ** 2 * kappa[2] - 1. / 2 * eta[1] ** 3 *\\n                  kappa[3] - 3. / 2 * eta[1] * eta[3] * kappa[2]) * cb\\n        self.coeffs[1, 6] = tmp1 + tmp2 + tmp3 + tmp4 + tmp5\\n        # septic (u^7)\\n        tmp1 = -20. * (end_pose[0] - start_pose[0])\\n        tmp2 = (10. * eta[0] + 2. * eta[2] + 1. / 6 * eta[4]) * ca\\n        tmp3 = - (2. * eta[0] ** 2 * kappa[0] + 1. / 6 * eta[0] ** 3 * kappa[1]\\n                  + 1. / 2 * eta[0] * eta[2] * kappa[0]) * sa\\n        tmp4 = (10. * eta[1] - 2. * eta[3] + 1. / 6 * eta[5]) * cb\\n        tmp5 = (2. * eta[1] ** 2 * kappa[2] - 1. / 6 * eta[1] ** 3 * kappa[3]\\n                - 1. / 2 * eta[1] * eta[3] * kappa[2]) * sb\\n        self.coeffs[0, 7] = tmp1 + tmp2 + tmp3 + tmp4 + tmp5\\n\\n        tmp1 = -20. * (end_pose[1] - start_pose[1])\\n        tmp2 = (10. * eta[0] + 2. * eta[2] + 1. / 6 * eta[4]) * sa\\n        tmp3 = (2. * eta[0] ** 2 * kappa[0] + 1. / 6 * eta[0] ** 3 * kappa[1]\\n                + 1. / 2 * eta[0] * eta[2] * kappa[0]) * ca\\n        tmp4 = (10. * eta[1] - 2. * eta[3] + 1. / 6 * eta[5]) * sb\\n        tmp5 = - (2. * eta[1] ** 2 * kappa[2] - 1. / 6 * eta[1] ** 3 * kappa[3]\\n                  - 1. / 2 * eta[1] * eta[3] * kappa[2]) * cb\\n        self.coeffs[1, 7] = tmp1 + tmp2 + tmp3 + tmp4 + tmp5\\n        self.s_dot = lambda u: max(np.linalg.norm(\\n            self.coeffs[:, 1:].dot(np.array(\\n                [1, 2. * u, 3. * u**2, 4. * u**3,\\n                 5. * u**4, 6. * u**5, 7. * u**6]))), 1e-6)\\n        self.f_length = lambda ue: quad(lambda u: self.s_dot(u), 0, ue)\\n        self.segment_length = self.f_length(1)[0]\\n\\n    def calc_point(self, u):\\n        \"\"\"\\n        Eta3PathSegment::calc_point\\n\\n        input\\n            u - parametric representation of a point along the segment, 0 <= u <= 1\\n        returns\\n            (x,y) of point along the segment\\n        \"\"\"\\n        assert(0 <= u <= 1)\\n        return self.coeffs.dot(np.array([1, u, u**2, u**3, u**4, u**5, u**6, u**7]))\\n\\n    def calc_deriv(self, u, order=1):\\n        \"\"\"\\n        Eta3PathSegment::calc_deriv\\n\\n        input\\n            u - parametric representation of a point along the segment, 0 <= u <= 1\\n        returns\\n            (d^nx/du^n,d^ny/du^n) of point along the segment, for 0 < n <= 2\\n        \"\"\"\\n\\n        assert(0 <= u <= 1)\\n        assert(0 < order <= 2)\\n        if order == 1:\\n            return self.coeffs[:, 1:].dot(np.array([1, 2. * u, 3. * u**2, 4. * u**3, 5. * u**4, 6. * u**5, 7. * u**6]))\\n\\n        return self.coeffs[:, 2:].dot(np.array([2, 6. * u, 12. * u**2, 20. * u**3, 30. * u**4, 42. * u**5]))\\n\\n\\ndef test1():\\n\\n    for i in range(10):\\n        path_segments = []\\n        # segment 1: lane-change curve\\n        start_pose = [0, 0, 0]\\n        end_pose = [4, 3.0, 0]\\n        # NOTE: The ordering on kappa is [kappa_A, kappad_A, kappa_B, kappad_B], with kappad_* being the curvature derivative\\n        kappa = [0, 0, 0, 0]\\n        eta = [i, i, 0, 0, 0, 0]\\n        path_segments.append(Eta3PathSegment(\\n            start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa))\\n\\n        path = Eta3Path(path_segments)\\n\\n        # interpolate at several points along the path\\n        ui = np.linspace(0, len(path_segments), 1001)\\n        pos = np.empty((2, ui.size))\\n        for j, u in enumerate(ui):\\n            pos[:, j] = path.calc_path_point(u)\\n\\n        if show_animation:\\n            # plot the path\\n            plt.plot(pos[0, :], pos[1, :])\\n            # for stopping simulation with the esc key.\\n            plt.gcf().canvas.mpl_connect(\\n                \\'key_release_event\\',\\n                lambda event: [exit(0) if event.key == \\'escape\\' else None])\\n            plt.pause(1.0)\\n\\n    if show_animation:\\n        plt.close(\"all\")\\n\\n\\ndef test2():\\n\\n    for i in range(10):\\n        path_segments = []\\n        # segment 1: lane-change curve\\n        start_pose = [0, 0, 0]\\n        end_pose = [4, 3.0, 0]\\n        # NOTE: The ordering on kappa is [kappa_A, kappad_A, kappa_B, kappad_B], with kappad_* being the curvature derivative\\n        kappa = [0, 0, 0, 0]\\n        eta = [0, 0, (i - 5) * 20, (5 - i) * 20, 0, 0]\\n        path_segments.append(Eta3PathSegment(\\n            start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa))\\n\\n        path = Eta3Path(path_segments)\\n\\n        # interpolate at several points along the path\\n        ui = np.linspace(0, len(path_segments), 1001)\\n        pos = np.empty((2, ui.size))\\n        for j, u in enumerate(ui):\\n            pos[:, j] = path.calc_path_point(u)\\n\\n        if show_animation:\\n            # plot the path\\n            plt.plot(pos[0, :], pos[1, :])\\n            plt.pause(1.0)\\n\\n    if show_animation:\\n        plt.close(\"all\")\\n\\n\\ndef test3():\\n    path_segments = []\\n\\n    # segment 1: lane-change curve\\n    start_pose = [0, 0, 0]\\n    end_pose = [4, 1.5, 0]\\n    # NOTE: The ordering on kappa is [kappa_A, kappad_A, kappa_B, kappad_B], with kappad_* being the curvature derivative\\n    kappa = [0, 0, 0, 0]\\n    eta = [4.27, 4.27, 0, 0, 0, 0]\\n    path_segments.append(Eta3PathSegment(\\n        start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa))\\n\\n    # segment 2: line segment\\n    start_pose = [4, 1.5, 0]\\n    end_pose = [5.5, 1.5, 0]\\n    kappa = [0, 0, 0, 0]\\n    eta = [0, 0, 0, 0, 0, 0]\\n    path_segments.append(Eta3PathSegment(\\n        start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa))\\n\\n    # segment 3: cubic spiral\\n    start_pose = [5.5, 1.5, 0]\\n    end_pose = [7.4377, 1.8235, 0.6667]\\n    kappa = [0, 0, 1, 1]\\n    eta = [1.88, 1.88, 0, 0, 0, 0]\\n    path_segments.append(Eta3PathSegment(\\n        start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa))\\n\\n    # segment 4: generic twirl arc\\n    start_pose = [7.4377, 1.8235, 0.6667]\\n    end_pose = [7.8, 4.3, 1.8]\\n    kappa = [1, 1, 0.5, 0]\\n    eta = [7, 10, 10, -10, 4, 4]\\n    path_segments.append(Eta3PathSegment(\\n        start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa))\\n\\n    # segment 5: circular arc\\n    start_pose = [7.8, 4.3, 1.8]\\n    end_pose = [5.4581, 5.8064, 3.3416]\\n    kappa = [0.5, 0, 0.5, 0]\\n    eta = [2.98, 2.98, 0, 0, 0, 0]\\n    path_segments.append(Eta3PathSegment(\\n        start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa))\\n\\n    # construct the whole path\\n    path = Eta3Path(path_segments)\\n\\n    # interpolate at several points along the path\\n    ui = np.linspace(0, len(path_segments), 1001)\\n    pos = np.empty((2, ui.size))\\n    for i, u in enumerate(ui):\\n        pos[:, i] = path.calc_path_point(u)\\n\\n    # plot the path\\n\\n    if show_animation:\\n        plt.figure(\\'Path from Reference\\')\\n        plt.plot(pos[0, :], pos[1, :])\\n        plt.xlabel(\\'x\\')\\n        plt.ylabel(\\'y\\')\\n        plt.title(\\'Path\\')\\n        plt.pause(1.0)\\n\\n        plt.show()\\n\\n\\ndef main():\\n    \"\"\"\\n    recreate path from reference (see Table 1)\\n    \"\"\"\\n    test1()\\n    test2()\\n    test3()\\n\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n', '#!/usr/bin/python2\\n# -*- coding: utf-8 -*-\\n\\nimport sys\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom sklearn import datasets\\nfrom sklearn.cross_validation import cross_val_predict\\nfrom sklearn import linear_model\\nfrom sklearn import datasets\\n\\nX = []\\nY = []\\nfor line in sys.stdin:\\n    line = line.rstrip()\\n    X.append([len(line.split())])\\n    Y.append(line.count(\",\"))\\n\\nlr = linear_model.LinearRegression()\\n\\npredicted = cross_val_predict(lr, X, Y)\\n\\nFILE = open(sys.argv[1], \"r\")\\nX_TEST = []\\nY_TEST = []\\nfor line in FILE:\\n    line = line.rstrip()\\n    Y_TEST.append(line.count(\",\"))\\n    line = line.replace(\",\", \"\")\\n    X_TEST.append([len(line.split())])\\nregr = linear_model.LinearRegression()\\nregr.fit(X, Y)\\nprint \"Coefficients: \", regr.coef_\\nprint \"Residual sum of squares: %.2f\" % np.mean((regr.predict(X_TEST) - Y_TEST) ** 2)\\nprint \"Variance score: %.2f\" % regr.score(X_TEST, Y_TEST)\\nplt.scatter(X_TEST, Y_TEST,  color=\\'black\\')\\nplt.plot(X_TEST, regr.predict(X_TEST), color=\\'green\\', linewidth=2)\\nplt.xticks(())\\nplt.yticks(())\\nplt.show()\\n', '# Fluorescence Analysis\\n\\nimport os\\nimport cv2\\nimport numpy as np\\nimport pandas as pd\\nfrom plotnine import ggplot, geom_label, aes, geom_line\\nfrom plantcv.plantcv import print_image\\nfrom plantcv.plantcv import plot_image\\nfrom plantcv.plantcv import fatal_error\\nfrom plantcv.plantcv import params\\nfrom plantcv.plantcv import outputs\\n\\n\\ndef analyze_fvfm(fdark, fmin, fmax, mask, bins=256, label=\"default\"):\\n    \"\"\"Analyze PSII camera images.\\n    Inputs:\\n    fdark       = grayscale fdark image\\n    fmin        = grayscale fmin image\\n    fmax        = grayscale fmax image\\n    mask        = mask of plant (binary, single channel)\\n    bins        = number of bins (1 to 256 for 8-bit; 1 to 65,536 for 16-bit; default is 256)\\n    label       = optional label parameter, modifies the variable name of observations recorded\\n\\n    Returns:\\n    analysis_images = list of images (fv image and fvfm histogram image)\\n    :param fdark: numpy.ndarray\\n    :param fmin: numpy.ndarray\\n    :param fmax: numpy.ndarray\\n    :param mask: numpy.ndarray\\n    :param bins: int\\n    :param label: str\\n    :return analysis_images: numpy.ndarray\\n    \"\"\"\\n\\n    # Auto-increment the device counter\\n    params.device += 1\\n    # Check that fdark, fmin, and fmax are grayscale (single channel)\\n    if not all(len(np.shape(i)) == 2 for i in [fdark, fmin, fmax]):\\n        fatal_error(\"The fdark, fmin, and fmax images must be grayscale images.\")\\n\\n    # QC Fdark Image\\n    fdark_mask = cv2.bitwise_and(fdark, fdark, mask=mask)\\n    if np.amax(fdark_mask) > 2000:\\n        qc_fdark = False\\n    else:\\n        qc_fdark = True\\n\\n    # Mask Fmin and Fmax Image\\n    fmin_mask = cv2.bitwise_and(fmin, fmin, mask=mask)\\n    fmax_mask = cv2.bitwise_and(fmax, fmax, mask=mask)\\n\\n    # Calculate Fvariable, where Fv = Fmax - Fmin (masked)\\n    fv = np.subtract(fmax_mask, fmin_mask)\\n\\n    # When Fmin is greater than Fmax, a negative value is returned.\\n    # Because the data type is unsigned integers, negative values roll over, resulting in nonsensical values\\n    # Wherever Fmin is greater than Fmax, set Fv to zero\\n    fv[np.where(fmax_mask < fmin_mask)] = 0\\n    analysis_images = []\\n\\n    # Calculate Fv/Fm (Fvariable / Fmax) where Fmax is greater than zero\\n    # By definition above, wherever Fmax is zero, Fvariable will also be zero\\n    # To calculate the divisions properly we need to change from unit16 to float64 data types\\n    fvfm = fv.astype(np.float64)\\n    analysis_images.append(fvfm)\\n    fmax_flt = fmax_mask.astype(np.float64)\\n    fvfm[np.where(fmax_mask > 0)] /= fmax_flt[np.where(fmax_mask > 0)]\\n\\n    # Calculate the median Fv/Fm value for non-zero pixels\\n    fvfm_median = np.median(fvfm[np.where(fvfm > 0)])\\n\\n    # Calculate the histogram of Fv/Fm non-zero values\\n    fvfm_hist, fvfm_bins = np.histogram(fvfm[np.where(fvfm > 0)], bins, range=(0, 1))\\n    # fvfm_bins is a bins + 1 length list of bin endpoints, so we need to calculate bin midpoints so that\\n    # the we have a one-to-one list of x (FvFm) and y (frequency) values.\\n    # To do this we add half the bin width to each lower bin edge x-value\\n    midpoints = fvfm_bins[:-1] + 0.5 * np.diff(fvfm_bins)\\n\\n    # Calculate which non-zero bin has the maximum Fv/Fm value\\n    max_bin = midpoints[np.argmax(fvfm_hist)]\\n\\n    # Create a dataframe\\n    dataset = pd.DataFrame({\\'Plant Pixels\\': fvfm_hist, \\'Fv/Fm\\': midpoints})\\n    # Make the histogram figure using plotnine\\n    fvfm_hist_fig = (ggplot(data=dataset, mapping=aes(x=\\'Fv/Fm\\', y=\\'Plant Pixels\\'))\\n                     + geom_line(color=\\'green\\', show_legend=True)\\n                     + geom_label(label=\\'Peak Bin Value: \\' + str(max_bin),\\n                                  x=.15, y=205, size=8, color=\\'green\\'))\\n    analysis_images.append(fvfm_hist_fig)\\n\\n    if params.debug == \\'print\\':\\n        print_image(fmin_mask, os.path.join(params.debug_outdir, str(params.device) + \\'_fmin_mask.png\\'))\\n        print_image(fmax_mask, os.path.join(params.debug_outdir, str(params.device) + \\'_fmax_mask.png\\'))\\n        print_image(fv, os.path.join(params.debug_outdir, str(params.device) + \\'_fv_convert.png\\'))\\n        fvfm_hist_fig.save(os.path.join(params.debug_outdir, str(params.device) + \\'_fv_hist.png\\'), verbose=False)\\n    elif params.debug == \\'plot\\':\\n        plot_image(fmin_mask, cmap=\\'gray\\')\\n        plot_image(fmax_mask, cmap=\\'gray\\')\\n        plot_image(fv, cmap=\\'gray\\')\\n        print(fvfm_hist_fig)\\n\\n    outputs.add_observation(sample=label, variable=\\'fvfm_hist\\', trait=\\'Fv/Fm frequencies\\',\\n                            method=\\'plantcv.plantcv.fluor_fvfm\\', scale=\\'none\\', datatype=list,\\n                            value=fvfm_hist.tolist(), label=np.around(midpoints, decimals=len(str(bins))).tolist())\\n    outputs.add_observation(sample=label, variable=\\'fvfm_hist_peak\\', trait=\\'peak Fv/Fm value\\',\\n                            method=\\'plantcv.plantcv.fluor_fvfm\\', scale=\\'none\\', datatype=float,\\n                            value=float(max_bin), label=\\'none\\')\\n    outputs.add_observation(sample=label, variable=\\'fvfm_median\\', trait=\\'Fv/Fm median\\',\\n                            method=\\'plantcv.plantcv.fluor_fvfm\\', scale=\\'none\\', datatype=float,\\n                            value=float(np.around(fvfm_median, decimals=4)), label=\\'none\\')\\n    outputs.add_observation(sample=label, variable=\\'fdark_passed_qc\\', trait=\\'Fdark passed QC\\',\\n                            method=\\'plantcv.plantcv.fluor_fvfm\\', scale=\\'none\\', datatype=bool,\\n                            value=qc_fdark, label=\\'none\\')\\n\\n    # Store images\\n    outputs.images.append(analysis_images)\\n\\n    return analysis_images\\n', '\"\"\"\\nInteractive D3 rendering of matplotlib images\\n=============================================\\n\\nFunctions: General Use\\n----------------------\\n:func:`fig_to_html`\\n    convert a figure to an html string\\n\\n:func:`fig_to_dict`\\n    convert a figure to a dictionary representation\\n\\n:func:`show`\\n    launch a web server to view an d3/html figure representation\\n\\n:func:`save_html`\\n    save a figure to an html file\\n\\n:func:`save_json`\\n    save a JSON representation of a figure to file\\n\\n\\nFunctions: IPython Notebook\\n---------------------------\\n:func:`display`\\n    display a figure in an IPython notebook\\n\\n:func:`enable_notebook`\\n    enable automatic D3 display of figures in the IPython notebook.\\n\\n:func:`disable_notebook`\\n    disable automatic D3 display of figures in the IPython\\n\"\"\"\\n\\n__all__ = [\"__version__\",\\n           \"fig_to_html\", \"fig_to_dict\", \"fig_to_d3\", \"display_d3\",\\n           \"display\", \"show_d3\", \"show\", \"save_html\", \"save_json\",\\n           \"enable_notebook\", \"disable_notebook\", \"plugins\", \"urls\"]\\n\\nfrom .__about__ import __version__\\nfrom . import plugins\\nfrom . import urls\\nfrom ._display import *\\n'], 'license': ['bsd-3-clause', 'bsd-3-clause', 'mit', 'bsd-3-clause', 'mit', 'apache-2.0', 'bsd-3-clause', 'gpl-3.0', 'mit', 'mit', 'gpl-2.0', 'mit', 'apache-2.0', 'gpl-3.0', 'bsd-3-clause', 'bsd-3-clause', 'bsd-3-clause', 'bsd-3-clause', 'bsd-3-clause', 'bsd-3-clause', 'bsd-3-clause', 'mit', 'bsd-3-clause', 'mit', 'apache-2.0', 'bsd-3-clause', 'bsd-3-clause', 'gpl-3.0', 'mit', 'gpl-2.0', 'mit', 'bsd-3-clause']}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     19\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     20\u001b[0m     callbacks\u001b[39m=\u001b[39m[pl\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m)],\n\u001b[1;32m     21\u001b[0m     logger\u001b[39m=\u001b[39mlogger,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_loader, valid_loader)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:213\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    212\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 213\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    217\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     85\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\n\u001b[1;32m     86\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(optimizers, kwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:202\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, optimizers: List[Tuple[\u001b[39mint\u001b[39m, Optimizer]], kwargs: OrderedDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_kwargs(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hiddens)\n\u001b[0;32m--> 202\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position])\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    206\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:249\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[1;32m    241\u001b[0m         closure()\n\u001b[1;32m    243\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    251\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:370\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    362\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m return True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39musing_native_amp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprecision_plugin, MixedPrecisionPlugin)\n\u001b[0;32m--> 370\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    371\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    372\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    373\u001b[0m     batch_idx,\n\u001b[1;32m    374\u001b[0m     optimizer,\n\u001b[1;32m    375\u001b[0m     opt_idx,\n\u001b[1;32m    376\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    377\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[1;32m    378\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    379\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    383\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1356\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m   1355\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1356\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1358\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/core/module.py:1742\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_lbfgs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1664\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1665\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1672\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1673\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1675\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \n\u001b[1;32m   1741\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:169\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:234\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[1;32m    235\u001b[0m     optimizer, model\u001b[39m=\u001b[39;49mmodel, optimizer_idx\u001b[39m=\u001b[39;49mopt_idx, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    236\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:119\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/torch/optim/adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 183\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    185\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    186\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:105\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     93\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     98\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     99\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:149\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:135\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 135\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:419\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \n\u001b[1;32m    412\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m    A ``ClosureResult`` containing the training step output.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m training_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    422\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mtraining_step_end\u001b[39m\u001b[39m\"\u001b[39m, training_step_output)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1496\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/exp/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:378\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[1;32m    377\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Code/learning/MiniGPT/notebooks/../core/tiny.py:78\u001b[0m, in \u001b[0;36mGPTSimple.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     76\u001b[0m     \u001b[39m# Get the inputs from the batch.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39mprint\u001b[39m(batch)\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mprint\u001b[39m(batch\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m     79\u001b[0m     inputs \u001b[39m=\u001b[39m batch\n\u001b[1;32m     81\u001b[0m     \u001b[39m# Compute the logits.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Load the CodeParrot dataset\n",
    "# dataset = load_dataset(\"codeparrot/github-code\", streaming=True, languages=[\"Python\"])\n",
    "\n",
    "# Set up the DataLoader for the training data\n",
    "train_dataset = load_dataset(\"huggingface-course/codeparrot-ds-train\", streaming=True, split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "# Set up the DataLoader for the validation data\n",
    "valid_dataset = load_dataset(\"huggingface-course/codeparrot-ds-valid\", streaming=True, split=\"validation\")\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "\n",
    "# Define the GPTSimple model and the optimizer\n",
    "config = GPT1Config(vocab_size=50257, max_len=512)\n",
    "model = GPTSimple(config)\n",
    "\n",
    "# Set up the PyTorch Lightning trainer\n",
    "logger = TensorBoardLogger(\"logs\", name=\"gpt-simple\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[pl.callbacks.EarlyStopping(monitor=\"val_loss\")],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e83ec8dcf0b5679deb55b7d5613794b95abb2df0860752b19e6f08ac0cb49c75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
